To study the performance of compressed word embeddings in downstream tasks, we begin by empirically comparing methods with different underlying machineries. We observe a simple fast uniform quantization approach can compete with state-of-the-art deep compositional code learning (DCCL) \citep{dccl17} and k-means based compression \citep{andrews16} across various NLP tasks at different compression rates. In an attempt to understand what determines the downstream performance, we investigate the correlation between existing compression quality metrics and the performance of compressed embeddings. Though there is substantial progress in connecting matrix reconstruction error and generalization performance recently \citep{yin18,avron17,lprff18}, we observe these reconstruction errors, as a compression quality metrics, can poorly correlate with downstream performance across the simple uniform quantization and state-of-the-art methods. These observations impose challenges to understand the performance of compressed embeddings in a principle way. In this section, we first expose preliminaries in the compression methods and the existing compression quality metrics; this is followed by our performance comparison and empirical correlation analysis. These empirical observations motivates a new compression quality metric in Section~\ref{sec:new_metric}, leading to a more principled understanding on how compression impacts generalization.

\subsection{Preliminary: Embedding Compression Methods}
\label{subsec:prel}
%\paragraph{Word Embedding Compression Methods.} 
\todo{(we should pull prelim as a separate paragraph)}To deploy word embedding to memory constraint settings in data center and edge devices, many methods has been proposed to compress the word embedding. \todo{(we should consider whether we can say kmeans is stoa.)}. Among these methods, state-of-the-art \emph{DCCL approach} \citep{dccl17}\footnote{This idea is also independently adopted by \citet{kway18}.} adopts dictionary learning to represent a large number of word vectors with fewer basis vectors. These basis vectors are organized into multiple dictionaries, and each word is represented as a hash code to discretely combine basis from the dictionaries. In particular, the discrete combination hash code is attained by training an autoencoder with Gumbel-softmax reparameterization \citep{maddison2016concrete,jang2016categorical} to minimize the embedding vector reconstruction error. 

Built on a significantly different machinery, embeddings compressed by a \emph{k-mean method} also demonstrates strong empirical performance in downstream tasks \citep{andrews16}. This k-means method assigns embedding matrix entry values into clusters, and each embedding entries is the approximated by the corresponding cluster center. In this way, one can achieve compression by only storing the cluster id instead of the original value for each embedding matrix entry. 

In contrast to the DCCL and k-mean approach, \emph{uniform quantization} is a simple method requiring no computationally expensive training. This method, as shown in at a high-level in Algorithm~\ref{alg:smallfry}, first clips the embedding matrix into the range $[-r,r]$, and then uniformly quantizes the entries of the clipped matrix; we choose the value of $r$ which minimizes the reconstruction error of the quantized matrix.
Because our algorithm uses uniform quantization, we first review uniform quantization, and then describe in more detail how our algorithm uses it to compress word embeddings.


%\begin{itemize}
%	\item Briefly describe DCCL and k-means approach
%	\item Present uniform quantization with Pseudo-code
%\end{itemize}
%At a high-level, our algorithm clips the embedding matrix into the range $[-r,r]$, and then uniformly quantizes the entries of the clipped matrix;
%we choose the value of $r$ which minimizes the reconstruction error of the quantized matrix.
%Because our algorithm uses uniform quantization, we first review uniform quantization, and then describe in more detail how our algorithm uses it to compress word embeddings.

A \textit{$b$-bit uniform quantization} $Q_{b,r}(x)$ of a real number $x \in [-r,r]$ is computed as follows:
First, the interval $[-r,r]$ is divided into $2^b - 1$ sub-intervals of equal size.
Then, $x$ is rounded to either the top or bottom of the sub-interval $[\ulx,\olx]$ containing $x$, where $\ulx = r + j\frac{2r}{2^b-1}$ and $\olx = r + (j+1)\frac{2r}{2^b-1}$, for $j\in\{0,1,\ldots,2^b-2\}$.
Given this rounded value, one can simply store the $b$-bit integer $j$ or $j+1$ in place of the real-valued $x$, depending on whether $x$ was rounded to $\ulx$ and $\olx$ respectively.
In this work, we will consider a deterministic rounding scheme which rounds $x$ to the nearest value, and a stochastic rounding scheme which rounds $x$ up or down in such a way that the expected value is equal to $x$.\footnote{
	This stochastic scheme rounds $x$ to $\ulx$ with probability $\frac{\olx-x}{\olx-\ulx}$ and to $\olx$ with probability $\frac{x-\ulx}{\olx-\ulx}$.
}
In particular, our analysis will focus on the stochastic rounding scheme, while our experiments will include results with both schemes.
%Note that we can upper bound the variance of these rounding schemes using the fact that a bounded random variable in an interval of length $c$ has variance at most $c^2/4$;
%using this fact, we can see that the variances of these rounding schemes are at most $\frac{1}{4} \cdot \Big(\frac{2r}{(2^b-1)}\Big)^2 = \frac{4r^2}{(2^b-1)^2}$.


We are now ready to present the uniform quantization compression algorithm, which we express in pseudo-code in Algorithm~\ref{alg:smallfry}.
The input to the algorithm is an embedding matrix $X \in \RR^{n\times d}$, where $n$ is the size of the vocabulary, and $d$ is the dimension of the embeddings.
We define the function $\clip_r(x) = \max(\min(x,r),-r)$ for any non-negative $r$; when matrices are passed in as inputs to this function, it clips the entries in an element-wise fashion.
The first step in our algorithm is to find the value of $r \in [0,\max(|X|)]$ which minimizes the reconstruction error of the quantized embeddings after $X$ is clipped to $[-r,r]$.
More formally, we let $r^* \defeq \argmin_{r \in [0,\max(|X|)} \|Q_{b,r}(\clip_r(X))-X\|_F$.
We then use this value $r^*$ to clip $X$, and then quantize the clipped embeddings to $b$ bits per entry.

In our experiments, we find $r^*$ to within a specified tolerance $\eps > 0$ using the golden-section search algorithm \citep{golden53}.
To avoid stochasticity impacting the search process, we always use deterministic rounding in the search for $r^*$, even if we use stochastic quantization in the final quantization.
%this choice also allows us to more cleanly compare deterministic vs.\ stochastic rounding, since they will always use the same value of $r^*$.

\begin{algorithm}[tb]
   \caption{Uniform quantization for word embeddings}
   \label{alg:smallfry}
\begin{algorithmic}[1]
	\STATE {\bfseries Input:}  Embedding matrix $X \in \RR^{n \times d}$, quantization function $Q_{b,r}$, clipping function $\clip_r\colon\RR\rightarrow[-r,r]$.
	\STATE {\bfseries Output:} Quantized embeddings $\hat{X}$.
	\STATE $r^* \defeq \argmin_{r \in [0,\max(|X|)} \|Q_{b,r}(\clip_r(X))-X\|_F$.
	%Search for $r^* \in [0,\max(|X|)]$ minimizing $\|Q_{b,r}(\clip_r(X))-X\|_F$.
	\STATE {\bfseries Return:} $Q_{b,r^*}(\clip_{r^*}(X))$.
\end{algorithmic}
\end{algorithm}


\subsection{Matrix Approximation Error and Generalization}
Many main stream word embedding generation problem can be casted matrix decomposition \citep{levy2014neural}. For example, Skip-gram Word2Vec \citep{word2vec13} implicitly factorizes the Pointwise Mutual Information (PMI) matrix while GloVe \citep{glove14} decomposes the word co-occurrence matrix. In this paper, given an uncompressed embedding $X\in\mathbb{R}^{n\times d}$ and its compressed version $\tilde{X}\in\mathbb{R}^{n\times \tilde{d}}$, we consider the reconstruction error between Gram matrices $G = XX^T$ and $\tilde{G} = \tilde{X}\tilde{X}^T$ as the proxy metrics of compression quality. Recently, there has been substantial progress in understanding the matrix reconstruction error and generalization for models derived from matrix factorization. Particularly in our empirical evaluation, we consider the \emph{Pointwise Inner Product (PIP)} loss \citep{yin18} and \emph{spectral approximation error} \citep{avron17,lprff18} as the metrics for word embedding compression quality. 

\paragraph{Pointwise Inner Product Loss}
Given $G$ and $\tilde{G}$, the Gram matrix of the uncompressed and compressed embeddings, Pointwise Inner Product (PIP) Loss is defined as $\|G - \tilde{G}\|_F^2$. It is first proposed to reveal the bias-variance trade-off when selecting the optimal embedding dimensionality. Though PIP loss does not explicitly consider the generalization performance of downstream tasks,  \citet{yin18} show that select embedding dimensionality by PIP loss minimization can attain strong downstream NLP task performance.



\paragraph{Spectral Approximation Error}
Built on recent theoretical work on kernel approximation \citep{avron17}, \citet{lprff18} propose the notion of \textit{$(\Delta_1,\Delta_2)$-spectral approximation} to understand the generalization performance of kernel method on supervised learning problems. In the context of uncompressed embedding $X$ and compressed embeddings $\tilde{X}$, the Gram matrix $\tilde{G} = \tilde{X}\tilde{X}^T$ is a $(\Delta_1,\Delta_2)$-spectral approximation to $G = XX^T$ if it satisfies 
\[(1-\Delta_1) G \preceq \tilde{G} \preceq (1-\Delta_2) G.\]
Though PIP loss and $(\Delta_1,\Delta_2)$-spectral approximation can roughly imply the generalization performance by examining the matrix reconstruction error in previous works \citep{avron17,yin18,lprff18}, we observe in Section\ref{subsec:hard_explain} these two metrics, as a measure of compression quality, can poorly correlate with the downstream task performance across different compressed embedding types.
%\paragraph{Matrix Approximation Error and Generalization}
%\label{subsec:error_gen}
%	\begin{itemize}
%		\item PIP Loss: introduce pip loss
%		\item Delta 1 and Delta 2: introduce Deltas
%	\end{itemize}
	
\subsection{Hardness in Explaining Generalization}
\label{subsec:hard_explain}
	\begin{itemize}
		\item Results and claim 1: uniform quantization method can compete well with the state-of-the-art, and it can match uncompressed embedding with high compression rate.
		\begin{itemize}
			\item show the generalization performance for quantized embedding and other baselines at different bit rate. (DCCL, kmeans, dimension reduction and uniform quantization)
			\item Figures on 4 tasks: DrQa, one sentiment, one word analogy and one word similarity
			\item Tables on tasks performance and time
		\end{itemize}
		\item Results and claim 2: Existing matrix reconstruction errors (PIP, spectral approximation error--delta1, delta2) do not correlate well with the generalization performance of compressed embeddings.
		\begin{itemize}
			\item Show correlation between performance and Frob norm (PIP) / Deltas (maybe the combination of delta1 and delta2) across a few tasks.
			\item We can optionally do R2 measurements reliably
		\end{itemize}
	\end{itemize}