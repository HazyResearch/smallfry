\subsection{Preliminary}
\label{subsec:prel}
\paragraph{Word Embedding Compression Methods}
\begin{itemize}
	\item Briefly describe DCCL and k-means approach
	\item Present uniform quantization with Pseudo-code
\end{itemize}

\paragraph{Matrix Approximation Error and Generalization}
%\label{subsec:error_gen}
	\begin{itemize}
		\item PIP Loss: introduce pip loss
		\item Delta 1 and Delta 2: introduce Deltas
	\end{itemize}
	
\subsection{Hardness in Explaining Generalization}
\label{subsec:hard_explain}
	\begin{itemize}
		\item Results and claim 1: uniform quantization method can compete well with the state-of-the-art, and it can match uncompressed embedding with high compression rate.
		\begin{itemize}
			\item show the generalization performance for quantized embedding and other baselines at different bit rate. (DCCL, kmeans, dimension reduction and uniform quantization)
			\item Figures on 4 tasks: DrQa, one sentiment, one word analogy and one word similarity
			\item Tables on tasks performance and time
		\end{itemize}
		\item Results and claim 2: Existing matrix reconstruction errors (PIP, spectral approximation error--delta1, delta2) do not correlate well with the generalization performance of compressed embeddings.
		\begin{itemize}
			\item Show correlation between performance and Frob norm (PIP) / Deltas (maybe the combination of delta1 and delta2) across a few tasks.
			\item We can optionally do R2 measurements reliably
		\end{itemize}
	\end{itemize}