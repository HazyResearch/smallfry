As a first step toward better understanding the downstream performance of compressed word embeddings, we perform an empirical comparison of the different compression methods in terms of their downstream performance on several tasks, as well as in terms of the metrics of compression quality discussed above.
We make two surprising observations in this empirical study:
First, we observe that the simple uniform quantization method performs very similarly to the deep dictionary learning (DCCL) and k-means compression methods across a number of tasks, embedding types, and compression ratios.
Second, we show that this cannot be satisfactorily explained in terms of any of the compression quality metrics described in Section~\ref{subsec:existing_metrics}, because the downstream performances of the compressed embeddings do not align well with any of these metrics.
For example, the uniform quantization method can have significantly worse PIP loss than DCCL while attaining similar downstream performance.

In this section, we present the results of our empirical analysis:
In Section~\ref{sec:perf_comp} we compare the various compression methods in terms of their downstream performance on a number of tasks as a function of the memory occupied by the embeddings.
We then present our results on the lack of correlation between existing metrics and downstream performance in Section~\ref{subsec:hard_explain}.
These observations motivate us to propose a new compression quality metric in Section~\ref{sec:new_metric} which better captures the properties of the compressed embeddings which determine its downstream performance.
%and leads to a more principled understanding on how compression impacts generalization. 

%To study the performance of compressed word embeddings in downstream tasks, we begin by empirically comparing methods with different underlying machineries. In our attempt to understand what intrinsic characteristic determines the downstream performance across different compressed embedding types, we observe that PIP loss and $(\Delta_1, \Delta_2)$-spectral approximation error, as compression quality metrics, can poorly correlate with downstream performance across the simple uniform quantization and state-of-the-art methods. This impose challenges to understand what intrinsic characteristic determines the downstream performance of compressed embeddings in a principle.
%In particular, we observe the simple fast uniform quantization approach can compete with state-of-the-art deep compositional code learning (DCCL) \citep{dccl17} and k-means based compression \citep{andrews16} across various NLP tasks at different compression rates. However, the simple quantization method can have significantly worse compression quality metric while matching the downstream performance of the state-of-the-art methods. In this section, we discuss our empirical comparison of compression methods, and demonstrate the lack of correlation between existing metrics and downstream task performance. These observations motivates our new compression quality metric in Section~\ref{sec:new_metric} which correlates well with downstream performance; it leads to a more principled understanding on how compression impacts generalization. 
\begin{figure*}
	\centering
	%	\begin{tabular}{c c c c}
	\begin{tabular}{@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}}
		\includegraphics[width=.245\linewidth]{figures/glove400k_qa_best-f1_vs_compression_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove400k_sentiment_sst_test-acc_vs_compression_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove400k_intrinsics_analogy-avg-score_vs_compression_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove400k_intrinsics_similarity-avg-score_vs_compression_linx.pdf} \\
		\includegraphics[width=.245\linewidth]{figures/fasttext1m_qa_best-f1_vs_compression_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/fasttext1m_sentiment_sst_test-acc_vs_compression_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/fasttext1m_intrinsics_analogy-avg-score_vs_compression_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/fasttext1m_intrinsics_similarity-avg-score_vs_compression_linx.pdf} \\
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_qa_best-f1_vs_compression_linx.pdf} &
\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_sentiment_sst_test-acc_vs_compression_linx.pdf} &
\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_intrinsics_analogy-avg-score_vs_compression_linx.pdf} &
\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_intrinsics_similarity-avg-score_vs_compression_linx.pdf} \\
		\;\;\;\;\;(a) & \;\;\;\;\;\;(b) & \;\;\;\;\;\;(c) & \;\;\;\;\;\;(d)
	\end{tabular}
	\caption{
		\textbf{Downstream Performance vs. Compression Rate.}
		Performance of compressed publically available GloVe (Wiki'14) (top) and fastText (bottom) embeddings on question answering (a), sentiment analysis (TREC dataset) (b), word analogy (c), and word similarity (d) tasks.
		We generally observe that the uniform quantization, k-means, and DCCL compression methods perform similarly across compression rates, whereas the dimensionality reduction method performs significantly worse.
%		Uniform quantization performs similarly to k-means and DCCL on question answering and sentiment analysis;
%		on the word analogy and similarity tasks, uniform quantization once again performs similarly to k-means, but outperforms the DCCL method.
%		For the GloVe (Wiki'14) experiments, we are also able to compare with publicly available embeddings of smaller dimensions ($d\in\{50,100,200,300\}$), and observe that compressing the 300-dimensional embeddings is significantly better than using lower-dimensional full-precision embeddings (which we ``Dim. reduction'').
		\todo{Add Glove-wiki400k-am results to Appendix.}
	}
	\label{fig:perf_comp}
\end{figure*}


\subsection{Downstream Performance vs. Compression Rate}
\label{sec:perf_comp}

We now present our empirical results comparing the downstream performance of the various compression methods as a function of the memory occupied by the compressed embeddings.

\paragraph{Experiment Details}
For these experiments, we use three different types of embeddings,
compress them using four different compression methods,
and evaluate them on four types of tasks.
For the embedding types, we use publicly available fastText embeddings (vocabulary size $\approx 10^6$, dimension $d=300$), publicly available GloVe embeddings trained on Wikipedia 2014 and Gigaword 5 corpora (vocabulary size $= 4\times 10^5$, dimension $d \in \{50,100,200,300\}$), and GloVe embeddings which we trained on a Wikipedia 2017 corpus ($d \in \{25,50,100,200,400\}$).
We will denote these embeddings throughout the paper by ``fastText'', ``GloVe (Wiki'14)'', and ``GloVe (Wiki'17)'', respectively.
We use the four embedding compression methods discuss in Section~\ref{sec:prelim}: DCCL, k-means, uniform quantization, and dimensionality reduction.
For the tasks, we consider question answering using the DrQA model \citep{drqa17} on the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, sentiment analysis tasks trained using a CNN model \citep{kim14}, and word analogy and similarity tasks \citep{levy15}.
For more details on the exact embeddings we use, and on the details of the various tasks, please see Appendix~\ref{app:experiments}

\paragraph{Experiment Results}
In Figure~\ref{fig:perf_comp} we present the results of our performance comparison experiments.
In these plots, we plot the performance of the publically available GloVe (Wiki'14) (top) and fastText (bottom) embeddings across the four tasks: question answering (left-most), sentiment analysis, word analogy, word similarity (right-most).
For the word analogy and similarity plots, we plot the average performance of each embedding across a number of analogy and similarity tasks, respectively;
we use the analogy and similarity tasks from \citet{levy15}.
In each individual plot, we plot the performance per task of each compression method, at various compression rates;
higher $y$-values are better across all plots.
For k-means, uniform quantization, and DCCL we use compression rates 8, 16, and 32.
For the GloVe embeddings, the compression rates in the plots range from 1 to 32 because we also evaluate the performance of publicly available GloVe (Wiki'14) embeddings at dimensions $d\in\{50,100,200,300\}$, which attain compression rates of 6, 3, 1.5, 1, respectively;
this is the dimensionality reduction method.
The fastText embeddings are not released at various dimensions, so we were unable to compute the dimensionality reduction method there.
Across all the plots we use five random seed per compression setting, and plot the average, with the standard deviation visualized with error bars.

Our primary conclusions from Figure~\ref{fig:perf_comp}:
\begin{itemize}
\item Dimensionality reduction methods perform very poorly relative to the other compression methods.
For example, the GloVe (Wiki'14) dimensionality reduction method attains an average question answering F1 score of approximately 71 at a compression rate of 6, while the uniform quantization method attains an average F1 score of approximately 74 at the higher compression rate of 8.
\item The simpler compression methods (k-means, uniform quantization) perform on par with the more elaborate DCCL compression method.
For example, on the question answering task uniform quantization attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the uncompressed GloVe embeddings, while the DCCL method of \citet{dccl17} is 0.43\% below.
\end{itemize}
To attempt to explain these surprising results, we turn to the various metrics we described in Section~\ref{subsec:existing_metrics}.
Surprisingly, as we show in the next section, none of these metrics aligns well with the downstream performance of the various compression methods, leading us to propose a new metric and corresponding theoretical analysis and experiments in Section~\ref{sec:new_metric}.

%\todo{(dataset info might be need to be added)} In our empirical performance comparison of the different compression method, we evaluate the embeddings compressed by each method on a variety of tasks. We demonstrate that the compressed embeddings based on simple uniform quantization can match the the state-of-the-art DCCL and k-means method. To compare these methods on different uncompressed embedding types, we consider compressing both 300 dimensional GloVe publically available embedding and \todo{bla dim} FastText pretrained embeddings. In the experiment with GloVe uncompress embedding, we also include different dimensional GloVe publically available embeddings, and use it to compare the performance of compressing high dimensional embeddings and directly using lower dimensional publically available embeddings with the same memory footprint. In Figure~\ref{fig:perf_comp}, we can observe across the four tasks using both GloVe and FastText embeddings, the simple uniform quantization method can closely match the DCCL and k-means approach in downstream performance at high compression rate. For example, using the DrQA model \citep{drqa17} on the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, uniform quantization attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the uncompressed GloVe embeddings, while the DCCL method of \citet{dccl17} is 0.43\% below. Additionally, we observe in Figure~\ref{fig:perf_comp} (top) that compressing higher dimensional word embeddings can perform significantly better than using lower dimensional publically available embeddings with the same memory consumption.

\subsection{Downstream Performance vs. Compression Quality Metrics}
\label{subsec:hard_explain}
We now show that the surprising empirical results from the previous section, including the strong downstream performance of the simple compression methods, cannot be explained in terms of the various compression quality metrics we discussed in Section~\ref{subsec:existing_metrics}.

\paragraph{Experiment Details}
We use the same experimental setup as described in Section~\ref{sec:perf_comp}.
The metrics we consider are reconstruction error, PIP loss, and $(\Delta_1,\Delta_2)$-spectral approximation.
To measure these metrics, we compare the various compressed embeddings with the corresponding \textit{full-dimensional} uncompressed embeddings ($d=300$ for GloVe (Wiki'14) and fastText, $d=400$ for GloVe (Wiki'17)).
For the reconstruction error metric, we can only compute this metric when the dimension of the compressed embeddings matches that of the full-dimensional uncompressed embedding.
For the PIP loss, we compare the Gram matrix of the full-dimensional embeddings to that of the compressed embeddings.
To measure $\Delta_1$ and $\Delta_2$, we present results with $\lambda$ equal to the smallest eigenvalue of the the full-dimensional embedding Gram matrix; our results are robust to the choice of $\lambda$, as we show in Appendix \ref{app:experiments}.

\paragraph{Experiment Results}
In Figure~\ref{fig:bad_correlation} we present the results for downstream performance vs.\ compression quality metrics.
We plot question answering performance (left two columns) and sentiment analysis performance on the ``sst1'' task from \citet{kim14} (right two columns).\footnote{For results on the other sentiment tasks from \citet{kim14} please see Appendix~\ref{app:experiments}.}
Each point in these plots represents a compressed embedding;
the $x$-value represents one of the compression quality metrics measured on this compressed embedding relative to the corresponding uncompressed full-dimensional embedding, while the $y$-value represents the performance of this compressed embedding on one of the downstream tasks.

At a high level, what we hope to understand from the plots in Figure~\ref{fig:bad_correlation} is the following:
When a compression quality metric ranks one compressed embedding higher than another, how likely is it that the higher-ranked embedding will also perform better on a downstream task?
If the ranking based on the quality metric were identical to the ranking based on the downstream task performance, we would see a monotonically decreasing sequence of dots (since the ``quality'' metrics are actually a measure of compression error, with larger values representing lower quality).
As can be seen in these plots, none of the compression quality metrics align consistently with the downstream performance of the corresponding embeddings.
For example, the uniformly quantized GloVe (Wiki'14) embeddings attain a much larger PIP loss than the DCCL embeddings, but attain similar performance.

To quantify the above intuition regarding whether the ranking based on the compression quality metric correlates strongly with the ranking based on the downstream performance, we measure Spearman's rank correlation coefficient, which we denote as $\rho$.
We present these results in Table~\ref{TODO}.
As we can see, on all the tasks these compression quality metrics attain $\rho$ values below \todo{XX}, with most of the compression quality metrics actually attaining values around \todo{YY} or below.

The fact that these sophisticated metrics all fail to explain the downstream performance of the compressed embeddings is troubling, as it suggests that our current understanding of what leads to strong downstream performance is flawed.
To overcome this problem, we propose a new metric in Section~\ref{sec:new_metric} which correlates well with downstream performance. In the context of linear regression, we prove a generalization bound in terms of this metric for compressed embeddings in Section~\ref{sec:new_metric}.
Leveraging this metric, we can then explain why simple uniformly quantized embeddings performs well in a range of downstream tasks at low-precisions.

\begin{figure*}
	\centering
	\begin{tabular}{@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}}
		\includegraphics[width=.245\linewidth]{figures/glove400k_qa_best-f1_vs_embed-frob-error_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_qa_best-f1_vs_embed-frob-error_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove400k_sentiment_sst_test-acc_vs_embed-frob-error_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_sentiment_sst_test-acc_vs_embed-frob-error_linx.pdf} \\
		\includegraphics[width=.245\linewidth]{figures/glove400k_qa_best-f1_vs_gram-large-dim-frob-error_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_qa_best-f1_vs_gram-large-dim-frob-error_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove400k_sentiment_sst_test-acc_vs_gram-large-dim-frob-error_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_sentiment_sst_test-acc_vs_gram-large-dim-frob-error_linx.pdf} \\
		\includegraphics[width=.245\linewidth]{figures/glove400k_qa_best-f1_vs_gram-large-dim-delta1-2-trans_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_qa_best-f1_vs_gram-large-dim-delta1-2-trans_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove400k_sentiment_sst_test-acc_vs_gram-large-dim-delta1-2-trans_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_sentiment_sst_test-acc_vs_gram-large-dim-delta1-2-trans_linx.pdf} \\
		\includegraphics[width=.245\linewidth]{figures/glove400k_qa_best-f1_vs_gram-large-dim-delta2-2_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_qa_best-f1_vs_gram-large-dim-delta2-2_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove400k_sentiment_sst_test-acc_vs_gram-large-dim-delta2-2_linx.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_sentiment_sst_test-acc_vs_gram-large-dim-delta2-2_linx.pdf} \\
		(a) GloVe, QA & (b) GloVe Wiki, QA  & (c) GloVe, sentiment & (d) GloVe Wiki, sentiment
	\end{tabular}
	\caption{
		\textbf{Downstream Performance vs. Compression Quality Metrics.}
		For the compressed embeddings based on the GloVe (Wiki'14) (columns a,c) and GloVe (Wiki'17) (columns b,d) uncompressed embeddings, we show scatter plots of the downstream performance ($y$ axis) vs.\ compression quality metrics ($x$ axis).
		For the compression quality metrics, we consider embedding reconstruction error (first row), PIP loss (second row), $\Delta_1$ (third row), and $\Delta_2$ (fourth row).
		We can observe in both settings, for example, higher dimensional 1-bit uniform quantization can perform better than lower dimensional uncompressed embeddings (``Dim. reduction''). As another example, uniformly quantized embeddings with different precision can results in similar downstream performance while the compression quality metrics are drastically different.}
	\label{fig:bad_correlation}
\end{figure*}
%To understand why simple uniform quantization can perform as well as state-of-the-art in downstream tasks, we ask the question what intrinsic property of a compressed embedding determines its downstream performance. In particular, we analyze the correlation between existing compression quality metrics and the downstream performance. However, we observe that the metrics we considered in our experiments correlates poorly with the performance of downstream.
%Specifically, we consider the Pointwise Inner Product (PIP) loss and the $(\Delta_1, \Delta_2)$-spectral approximation error. This is because many embedding generation methods are implicit matrix decomposition; these two metrics are previously used to understand questions regarding generalization performance for matrix decomposition based supervised learning models \citep{avron17,yin18,lprff18}. 

%In Figure~\ref{fig:bad_correlation}, we demonstrate the correlation between downstream task performance and quality metric PIP loss, $\Delta_1$, $\Delta_2$ on both the question answering and sentiment analysis tasks. In Figure~\ref{fig:bad_correlation} (a)(c) we can observe the metrics does not correlate well with generalization performance across different type of compressed embeddings. In this setting, we compress GloVe embeddings with vocabulary size 71,290 on \todo{could you double check here?} and we use the 300 dimensional embedding as the reference embedding to compute the quality metrics. Particularly on both question answering and sentiment analysis, DCCL method and uniform quantization can demonstrate similar generalization performance, but up to 2x different compression quality metrics on both question answering and sentiment analysis. Strikingly higher dimensional 1-bit embeddings outperforms lower dimensional uncompressed embeddings at the same memory footprint with large margin but demonstrate significantly larger PIP loss and $\Delta_2$ values. Furthermore, we also demonstrate the correlation for uniformly quantized embeddings across different quantization precisions, where we compress GloVe with a vocabulary size of 3.8 million and use the 400 dimensional embedding to compute compression quality metrics \todo{could you double check here?}. In Figure \ref{fig:bad_correlation}(a)(c), though 1-bit quantized embeddings show significantly worse compression quality metric than 16-bit quantized embeddings, they can demonstrate similar performance on the downstream tasks. 

%An intrinsic characteristic which correlates well with downstream performance is critical to quantitatively understand the generalization of compressed embeddings. However the existing compression quality metrics fail to play this critical role due to poor correlations, which makes understanding the generalization performance challenging without an anchor characteristic. To overcome this problem, we propose a new metric in Section~\ref{sec:new_metric} which correlates well with downstream performance. In the context of linear regression, we prove a generalization bound in terms of this metric for compressed embeddings in Section~\ref{sec:new_metric}. Leverage these understanding built on the new metric, we can then explain why simple uniformly quantized embeddings performs well in a range of downstream tasks with high compression rate.

 
%%%%%%%%%%% info on these embeddings %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%We generate embeddings on two corpora: Text8 with 17 million tokens (vocab size of 71,290) and Wiki with 4.5 billion tokens (vocab size of 3.8 million). The Text8 dataset is the first 100MB of a 2006 cleaned-up dump of Wikipedia where all words are lowercase, numbers are spelled out, and any non-English characters are replaced by whitespace. This data is public available online\footnote{http://mattmahoney.net/dc/text8.zip}. The Wiki dataset is the full English Wikimedia dump on Dec. 4, 2017 which was pre-processed by a fastText script~\footnote{https://github.com/facebookresearch/fastText/blob/master/get-wikimedia.sh} while keeping the letter cases and digits.



%	\begin{itemize}
%		\item Results and claim 1: uniform quantization method can compete well with the state-of-the-art, and it can match uncompressed embedding with high compression rate.
%		\begin{itemize}
%			\item show the generalization performance for quantized embedding and other baselines at different bit rate. (DCCL, kmeans, dimension reduction and uniform quantization)
%			\item Figures on 4 tasks: DrQa, one sentiment, one word analogy and one word similarity
%			\item Tables on tasks performance and time
%		\end{itemize}
%		\item Results and claim 2: Existing matrix reconstruction errors (PIP, spectral approximation error--delta1, delta2) do not correlate well with the generalization performance of compressed embeddings.
%		\begin{itemize}
%			\item Show correlation between performance and Frob norm (PIP) / Deltas (maybe the combination of delta1 and delta2) across a few tasks.
%			\item We can optionally do R2 measurements reliably
%		\end{itemize}
%	\end{itemize}

%\subsection{Preliminary: Embedding Compression Methods}
%\label{subsec:prel}
%%\paragraph{Word Embedding Compression Methods.} 
%\todo{(we should pull prelim as a separate paragraph)}To deploy word embedding to memory constraint settings in data center and edge devices, many methods has been proposed to compress the word embedding. \todo{(we should consider whether we can say kmeans is stoa.)}. Among these methods, state-of-the-art \emph{DCCL approach} \citep{dccl17}\footnote{This idea is also independently adopted by \citet{kway18}.} adopts dictionary learning to represent a large number of word vectors with fewer basis vectors. These basis vectors are organized into multiple dictionaries, and each word is represented as a hash code to discretely combine basis from the dictionaries. In particular, the discrete combination hash code is attained by training an autoencoder with Gumbel-softmax reparameterization \citep{maddison2016concrete,jang2016categorical} to minimize the embedding vector reconstruction error. 
%
%Built on a significantly different machinery, embeddings compressed by a \emph{k-mean method} also demonstrates strong empirical performance in downstream tasks \citep{andrews16}. This k-means method assigns embedding matrix entry values into clusters, and each embedding entries is the approximated by the corresponding cluster center. In this way, one can achieve compression by only storing the cluster id instead of the original value for each embedding matrix entry. 
%
%In contrast to the DCCL and k-mean approach, \emph{uniform quantization} is a simple method requiring no computationally expensive training. This method, as shown in at a high-level in Algorithm~\ref{alg:smallfry}, first clips the embedding matrix into the range $[-r,r]$, and then uniformly quantizes the entries of the clipped matrix; we choose the value of $r$ which minimizes the reconstruction error of the quantized matrix.
%Because our algorithm uses uniform quantization, we first review uniform quantization, and then describe in more detail how our algorithm uses it to compress word embeddings.
%
%
%%\begin{itemize}
%%	\item Briefly describe DCCL and k-means approach
%%	\item Present uniform quantization with Pseudo-code
%%\end{itemize}
%%At a high-level, our algorithm clips the embedding matrix into the range $[-r,r]$, and then uniformly quantizes the entries of the clipped matrix;
%%we choose the value of $r$ which minimizes the reconstruction error of the quantized matrix.
%%Because our algorithm uses uniform quantization, we first review uniform quantization, and then describe in more detail how our algorithm uses it to compress word embeddings.
%
%A \textit{$b$-bit uniform quantization} $Q_{b,r}(x)$ of a real number $x \in [-r,r]$ is computed as follows:
%First, the interval $[-r,r]$ is divided into $2^b - 1$ sub-intervals of equal size.
%Then, $x$ is rounded to either the top or bottom of the sub-interval $[\ulx,\olx]$ containing $x$, where $\ulx = r + j\frac{2r}{2^b-1}$ and $\olx = r + (j+1)\frac{2r}{2^b-1}$, for $j\in\{0,1,\ldots,2^b-2\}$.
%Given this rounded value, one can simply store the $b$-bit integer $j$ or $j+1$ in place of the real-valued $x$, depending on whether $x$ was rounded to $\ulx$ and $\olx$ respectively.
%In this work, we will consider a deterministic rounding scheme which rounds $x$ to the nearest value, and a stochastic rounding scheme which rounds $x$ up or down in such a way that the expected value is equal to $x$.\footnote{
%	This stochastic scheme rounds $x$ to $\ulx$ with probability $\frac{\olx-x}{\olx-\ulx}$ and to $\olx$ with probability $\frac{x-\ulx}{\olx-\ulx}$.
%}
%In particular, our analysis will focus on the stochastic rounding scheme, while our experiments will include results with both schemes.
%%Note that we can upper bound the variance of these rounding schemes using the fact that a bounded random variable in an interval of length $c$ has variance at most $c^2/4$;
%%using this fact, we can see that the variances of these rounding schemes are at most $\frac{1}{4} \cdot \Big(\frac{2r}{(2^b-1)}\Big)^2 = \frac{4r^2}{(2^b-1)^2}$.
%
%
%We are now ready to present the uniform quantization compression algorithm, which we express in pseudo-code in Algorithm~\ref{alg:smallfry}.
%The input to the algorithm is an embedding matrix $X \in \RR^{n\times d}$, where $n$ is the size of the vocabulary, and $d$ is the dimension of the embeddings.
%We define the function $\clip_r(x) = \max(\min(x,r),-r)$ for any non-negative $r$; when matrices are passed in as inputs to this function, it clips the entries in an element-wise fashion.
%The first step in our algorithm is to find the value of $r \in [0,\max(|X|)]$ which minimizes the reconstruction error of the quantized embeddings after $X$ is clipped to $[-r,r]$.
%More formally, we let $r^* \defeq \argmin_{r \in [0,\max(|X|)} \|Q_{b,r}(\clip_r(X))-X\|_F$.
%We then use this value $r^*$ to clip $X$, and then quantize the clipped embeddings to $b$ bits per entry.
%
%In our experiments, we find $r^*$ to within a specified tolerance $\eps > 0$ using the golden-section search algorithm \citep{golden53}.
%To avoid stochasticity impacting the search process, we always use deterministic rounding in the search for $r^*$, even if we use stochastic quantization in the final quantization.
%%this choice also allows us to more cleanly compare deterministic vs.\ stochastic rounding, since they will always use the same value of $r^*$.
%
%\begin{algorithm}[tb]
%   \caption{Uniform quantization for word embeddings}
%   \label{alg:smallfry}
%\begin{algorithmic}[1]
%	\STATE {\bfseries Input:}  Embedding matrix $X \in \RR^{n \times d}$, quantization function $Q_{b,r}$, clipping function $\clip_r\colon\RR\rightarrow[-r,r]$.
%	\STATE {\bfseries Output:} Quantized embeddings $\hat{X}$.
%	\STATE $r^* \defeq \argmin_{r \in [0,\max(|X|)} \|Q_{b,r}(\clip_r(X))-X\|_F$.
%	%Search for $r^* \in [0,\max(|X|)]$ minimizing $\|Q_{b,r}(\clip_r(X))-X\|_F$.
%	\STATE {\bfseries Return:} $Q_{b,r^*}(\clip_{r^*}(X))$.
%\end{algorithmic}
%\end{algorithm}
%
%
%\subsection{Matrix Approximation Error and Generalization}
%Many main stream word embedding generation problem can be casted matrix decomposition \citep{levy2014neural}. For example, Skip-gram Word2Vec \citep{word2vec13} implicitly factorizes the Pointwise Mutual Information (PMI) matrix while GloVe \citep{glove14} decomposes the word co-occurrence matrix. In this paper, given an uncompressed embedding $X\in\mathbb{R}^{n\times d}$ and its compressed version $\tilde{X}\in\mathbb{R}^{n\times \tilde{d}}$, we consider the reconstruction error between Gram matrices $G = XX^T$ and $\tilde{G} = \tilde{X}\tilde{X}^T$ as the proxy metrics of compression quality. Recently, there has been substantial progress in understanding the matrix reconstruction error and generalization for models derived from matrix factorization. Particularly in our empirical evaluation, we consider the \emph{Pointwise Inner Product (PIP)} loss \citep{yin18} and \emph{spectral approximation error} \citep{avron17,lprff18} as the metrics for word embedding compression quality. 
%
%\paragraph{Pointwise Inner Product Loss}
%Given $G$ and $\tilde{G}$, the Gram matrix of the uncompressed and compressed embeddings, Pointwise Inner Product (PIP) Loss is defined as $\|G - \tilde{G}\|_F^2$. It is first proposed to reveal the bias-variance trade-off when selecting the optimal embedding dimensionality. Though PIP loss does not explicitly consider the generalization performance of downstream tasks,  \citet{yin18} show that select embedding dimensionality by PIP loss minimization can attain strong downstream NLP task performance.
%
%
%
%\paragraph{Spectral Approximation Error}
%Built on recent theoretical work on kernel approximation \citep{avron17}, \citet{lprff18} propose the notion of \textit{$(\Delta_1,\Delta_2)$-spectral approximation} to understand the generalization performance of kernel method on supervised learning problems. In the context of uncompressed embedding $X$ and compressed embeddings $\tilde{X}$, the Gram matrix $\tilde{G} = \tilde{X}\tilde{X}^T$ is a $(\Delta_1,\Delta_2)$-spectral approximation to $G = XX^T$ if it satisfies 
%\[(1-\Delta_1) G \preceq \tilde{G} \preceq (1-\Delta_2) G.\]
%Though PIP loss and $(\Delta_1,\Delta_2)$-spectral approximation can roughly imply the generalization performance by examining the matrix reconstruction error in previous works \citep{avron17,yin18,lprff18}, we observe in Section\ref{subsec:hard_explain} these two metrics, as a measure of compression quality, can poorly correlate with the downstream task performance across different compressed embedding types.
%%\paragraph{Matrix Approximation Error and Generalization}
%%\label{subsec:error_gen}
%%	\begin{itemize}
%%		\item PIP Loss: introduce pip loss
%%		\item Delta 1 and Delta 2: introduce Deltas
%%	\end{itemize}
%	
%\subsection{Hardness in Explaining Generalization}
%\label{subsec:hard_explain}
%	\begin{itemize}
%		\item Results and claim 1: uniform quantization method can compete well with the state-of-the-art, and it can match uncompressed embedding with high compression rate.
%		\begin{itemize}
%			\item show the generalization performance for quantized embedding and other baselines at different bit rate. (DCCL, kmeans, dimension reduction and uniform quantization)
%			\item Figures on 4 tasks: DrQa, one sentiment, one word analogy and one word similarity
%			\item Tables on tasks performance and time
%		\end{itemize}
%		\item Results and claim 2: Existing matrix reconstruction errors (PIP, spectral approximation error--delta1, delta2) do not correlate well with the generalization performance of compressed embeddings.
%		\begin{itemize}
%			\item Show correlation between performance and Frob norm (PIP) / Deltas (maybe the combination of delta1 and delta2) across a few tasks.
%			\item We can optionally do R2 measurements reliably
%		\end{itemize}
%	\end{itemize}