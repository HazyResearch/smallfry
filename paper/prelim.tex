In this paper, we focus on gaining a deeper understanding of what intrinsic characteristics of compressed embeddings determine their downstream performance.
We begin by reviewing existing compression methods (\S\ref{subsec:existing_methods}) as well as existing metrics (\S\ref{subsec:existing_metrics}) which measure the quality of a compressed embedding relative to the uncompressed embedding.
% NOTE: I DON'T THINK WE NEED TO REPEAT PAPER OUTLINE, SINCE WE JUST DID THIS IN PARAGRAPH ABOVE (END OF INTRO)
%We then show in Section~\ref{sec:challenge} that these metrics do not align well with downstream performance, leading us to propose a new metric in Section~\ref{sec:new_metric} which aligns much better.

%In this paper, we focus on gaining a deeper understanding on what intrinsic characteristic of a compressed embedding determines its downstream task performance. To setup this connection, we first discuss preliminaries in different word embedding compression methods. We then present existing compression quality metrics, which are intrinsic characteristics of a compressed embedding matrix. In Section~\ref{sec:challenge}, we demonstrate that these existing metrics can poorly correlates with the downstream task performance of embeddings compressed by different methods; this leads to a new metric we propose in Section~\ref{sec:new_metric} which correlates well with downstream performance.

\subsection{Embedding Compression Methods}
\label{subsec:existing_methods}
We now review a number of existing compression methods for word embeddings;
these are the compression methods which we will be considering in the remainder of the paper.

\paragraph{Deep Compositional Code Learning (DCCL)} 
The DCCL method \citep{dccl17}\footnote{This idea is also independently adopted by \citet{kway18}.} uses a dictionary learning approach to represent a large number of word vectors using a much smaller number of basis vectors.
These basis vectors are organized into multiple dictionaries, and each word is represented as a sum which includes one basis vector per dictionary.
These dictionaries are trained using an autoencoder-style architecture to minimize the embedding vector reconstruction error.
%\todo{(we should pull prelim as a separate paragraph)}To deploy word embedding to memory constraint settings in data center and edge devices, many methods has been proposed to compress the word embedding. \todo{(we should consider whether we can say kmeans is stoa.)}. Among these methods, state-of-the-art \emph{DCCL approach} \citep{dccl17}\footnote{This idea is also independently adopted by \citet{kway18}.} adopts dictionary learning to represent a large number of word vectors with fewer basis vectors. These basis vectors are organized into multiple dictionaries, and each word is represented as a hash code to discretely combine basis from the dictionaries. In particular, the discrete combination hash code is attained by training an autoencoder with Gumbel-softmax reparameterization \citep{maddison2016concrete,jang2016categorical} to minimize the embedding vector reconstruction error. 

\paragraph{K-means Compression}
The k-means clustering algorithm can be used to compress word embeddings by first running 1-dimensional clustering on the full list of values in the word embedding matrix, and then replacing each value with the index of the closest centroid \citep{andrews16}.
Using $k=2^b$ allows for storing each matrix entry using only $b$ bits.

%Built on a significantly different machinery, embeddings compressed by a \emph{k-mean method} also demonstrates strong empirical performance in downstream tasks \citep{andrews16}. This k-means method assigns embedding matrix entry values into clusters, and each embedding entries is the approximated by the corresponding cluster center. In this way, one can achieve compression by only storing the cluster id instead of the original value for each embedding matrix entry.

\paragraph{Uniform Quantization}
Uniform quantization is a classic compression method which divides an interval into a fixed number of sub-intervals of equal size, and then rounds the values in each sub-interval to one of the boundaries of the sub-interval \citep{quant77} (for a review of uniform quantization, see Appendix \todo{XX}).
We apply this method to compress word embeddings as follows:
We first determine the optimal threshold at which to clip the extreme values in the word embedding matrix, and we then uniformly quantize the clipped embeddings within the clipped interval.
This algorithm is summarized in Algorithm~\ref{alg:smallfry}, where we denote by $Q_{b,r}$ the function which quantizes the interval $[-r,r]$ using $b$-bits per value.
Note that in our experiments, we find the optimal clipping threshold $r^*$ to within a specified tolerance $\eps > 0$ using the golden-section search algorithm \citep{golden53}.


%In contrast to the DCCL and k-mean approach, \emph{uniform quantization} is a simple method requiring no computationally expensive training. This method, as shown in at a high-level in Algorithm~\ref{alg:smallfry}, first clips the embedding matrix into the range $[-r,r]$, and then uniformly quantizes the entries of the clipped matrix; we choose the value of $r$ which minimizes the reconstruction error of the quantized matrix.
%Because our algorithm uses uniform quantization, we first review uniform quantization, and then describe in more detail how our algorithm uses it to compress word embeddings.


%\begin{itemize}
%	\item Briefly describe DCCL and k-means approach
%	\item Present uniform quantization with Pseudo-code
%\end{itemize}
%At a high-level, our algorithm clips the embedding matrix into the range $[-r,r]$, and then uniformly quantizes the entries of the clipped matrix;
%we choose the value of $r$ which minimizes the reconstruction error of the quantized matrix.
%Because our algorithm uses uniform quantization, we first review uniform quantization, and then describe in more detail how our algorithm uses it to compress word embeddings.

\begin{algorithm}[tb]
   \caption{Uniform quantization for word embeddings}
   \label{alg:smallfry}
\begin{algorithmic}[1]
	\STATE {\bfseries Input:}  Embedding matrix $X \in \RR^{n \times d}$, quantization function $Q_{b,r}$, clipping function $\clip_r\colon\RR\rightarrow[-r,r]$.
	\STATE {\bfseries Output:} Quantized embeddings $\hat{X}$.
	\STATE $r^* \defeq \argmin_{r \in [0,\max(|X|)} \|Q_{b,r}(\clip_r(X))-X\|_F$.
	%Search for $r^* \in [0,\max(|X|)]$ minimizing $\|Q_{b,r}(\clip_r(X))-X\|_F$.
	\STATE {\bfseries Return:} $Q_{b,r^*}(\clip_{r^*}(X))$.
\end{algorithmic}
\end{algorithm}


\subsection{Matrix Approximation Error and Generalization}
\label{subsec:existing_metrics}
\todo{Do we need more top-down here? What to say?}
Here we review a number of existing metrics which we use in order to evaluate the quality of a compressed word embedding relative to the uncompressed embedding.
Several of these metrics are based on comparing the pairwise inner product (Gram) matrices of the compressed vs.\ uncompressed embeddings.
The Gram matrices of embeddings are natural to consider for a couple reasons:
First, the loss function for training word embeddings typically only considers dot-products between embedding vectors \citep{word2vec13,glove14}.
Second, one can view word embedding training as implicit matrix factorization \citep{levy2014neural}, and thus comparing the Gram matrices of two embedding matrices is similar to comparing the matrices these embeddings are implicitly factoring.
\todo{Do we need this much detail?}

%Recently, there has been substantial progress in understanding the matrix reconstruction error and generalization for models derived from matrix factorization. Particularly in our empirical evaluation, we consider the \emph{Pointwise Inner Product (PIP)} loss \citep{yin18} and \emph{spectral approximation error} \citep{avron17,lprff18} as the metrics for word embedding compression quality.

When defining the metrics below, we will denote by $X \in \RR^{n\times d}$ the uncompressed embedding, and $\tX \in \RR^{n \times k}$ the compressed embedding, where $n$ is the vocabulary size, and $d$, $k$ are the compressed and uncompressed word embedding dimensions, respectively.
We now review the metrics:


%Many main stream word embedding generation problem can be casted matrix decomposition . For example, Skip-gram Word2Vec \citep{word2vec13} implicitly factorizes the Pointwise Mutual Information (PMI) matrix while GloVe \citep{glove14} decomposes the word co-occurrence matrix. In this paper, given an uncompressed embedding $X\in\mathbb{R}^{n\times d}$ and its compressed version $\tilde{X}\in\mathbb{R}^{n\times \tilde{d}}$, we consider the reconstruction error between Gram matrices $G = XX^T$ and $\tilde{G} = \tilde{X}\tilde{X}^T$ as the proxy metrics of compression quality. Recently, there has been substantial progress in understanding the matrix reconstruction error and generalization for models derived from matrix factorization. Particularly in our empirical evaluation, we consider the \emph{Pointwise Inner Product (PIP)} loss \citep{yin18} and \emph{spectral approximation error} \citep{avron17,lprff18} as the metrics for word embedding compression quality.

\paragraph{Word Embedding Reconstruction Error}
The first and simplest way of comparing two embeddings $X$ and $\tX$ is simply to measure the reconstruction error $\|X-\tX\|_F^2$.
In fact, many embedding compression methods \citep{andrews16,dccl17} use this as the loss function for compression.
Note that in order to be able to use this metric, $X$ and $\tX$ must have the same dimension.
This limits the settings in which this metric can be used, relative to the metrics which depend only on the Gram matrices.

\paragraph{Pairwise Inner Product (PIP) Loss}
Given $XX^T$ and $\tX\tX^T$, the Gram matrices of the uncompressed and compressed embeddings, the \textit{Pairwise Inner Product (PIP} Loss) is defined as $\|XX^T -\tX\tX^T\|_F^2$ \citep{yin18}.
This metric was recently proposed in order to explain the existence of an optimal dimension for word embeddings;
specifically, this work shows that increasing the embedding dimension decreases a bias term but increases a variance term in the PIP loss.
Though the PIP loss does not explicitly consider the generalization performance of the embeddings on downstream tasks, \citep{yin18} show that selecting the embedding dimension to minimize the PIP loss can help attain strong downstream NLP task performance.

%It is first proposed to reveal the bias-variance trade-off when selecting the optimal embedding dimensionality. Though PIP loss does not explicitly consider the generalization performance of downstream tasks,  \citet{yin18} show that select embedding dimensionality by PIP loss minimization can attain strong downstream NLP task performance.

\paragraph{Spectral Approximation Error}
Recent work on understanding the generalization performance of kernel approximation methods has proposed a way of comparing Gram matrices in terms of their spectral properties \citep{avron17,lprff18}.
Specifically, a gram matrix $\tX\tX^T$ is said to be \textit{$(\Delta_1,\Delta_2)$-spectral approximation} of another Gram matrix $XX^T$ if it satisfies $(1-\Delta_1) XX^T \preceq \tX\tX^T \preceq (1+\Delta_2)XX^T$.
In order to make this metric more robust, $\lambda I$ is typically added to each of the Gram matrices before computing the minimum $\Delta_1$ and $\Delta_2$ values satisfying the above equation;
in the supervised learning setting, the $\lambda$ value corresponds to the regularization parameter used during training.
\citet{lprff18} show that if $\tX\tX^T+\lambda I$ is a $(\Delta_1,\Delta_2)$-spectral approximation of $XX^T + \lambda$, then the linear model trained using $\tX$ will attain similar generalization performance to the linear model trained using $X$.

Although all of these metrics have strong theoretical foundations, in Section~\ref{subsec:hard_explain} we show that when these metrics are measured on compressed word embeddings, the resulting values align poorly with the downstream performance of the compressed embeddings.
This suggests that these metrics are poor indicators of the quality of the compressed embeddings.

%Building on recent theoretical work \citep{avron17}, \citet{lprff18} propose the notion of \textit{$(\Delta_1,\Delta_2)$-spectral approximation} between Gram matrices to understand the generalization performance of kernel approximation methods on supervised learning problems.
%This metric is defined as 
%A Gram matrix $\tX\tX^T$
%In the context of uncompressed embedding $X$ and compressed embeddings $\tilde{X}$, the Gram matrix $\tilde{G} = \tilde{X}\tilde{X}^T$ is a $(\Delta_1,\Delta_2)$-spectral approximation to $G = XX^T$ if it satisfies 
%\[(1-\Delta_1) G \preceq \tilde{G} \preceq (1-\Delta_2) G.\]
%Though PIP loss and $(\Delta_1,\Delta_2)$-spectral approximation can roughly imply the generalization performance by examining the matrix reconstruction error in previous works \citep{avron17,yin18,lprff18}, we observe in Section\ref{subsec:hard_explain} these two metrics, as a measure of compression quality, can poorly correlate with the downstream task performance across different compressed embedding types.
%\paragraph{Matrix Approximation Error and Generalization}
%\label{subsec:error_gen}
%	\begin{itemize}
%		\item PIP Loss: introduce pip loss
%		\item Delta 1 and Delta 2: introduce Deltas
%	\end{itemize}
	
%\subsection{Hardness in Explaining Generalization}
%\label{subsec:hard_explain}
%	\begin{itemize}
%		\item Results and claim 1: uniform quantization method can compete well with the state-of-the-art, and it can match uncompressed embedding with high compression rate.
%		\begin{itemize}
%			\item show the generalization performance for quantized embedding and other baselines at different bit rate. (DCCL, kmeans, dimension reduction and uniform quantization)
%			\item Figures on 4 tasks: DrQa, one sentiment, one word analogy and one word similarity
%			\item Tables on tasks performance and time
%		\end{itemize}
%		\item Results and claim 2: Existing matrix reconstruction errors (PIP, spectral approximation error--delta1, delta2) do not correlate well with the generalization performance of compressed embeddings.
%		\begin{itemize}
%			\item Show correlation between performance and Frob norm (PIP) / Deltas (maybe the combination of delta1 and delta2) across a few tasks.
%			\item We can optionally do R2 measurements reliably
%		\end{itemize}
%	\end{itemize}