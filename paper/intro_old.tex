In recent years, \textit{word embeddings} \citep{word2vec13,glove14,fasttext18} have brought large improvements to a wide range of applications in natural language processing (NLP) \citep{collins16,drqa17}.
However, these word embeddings can occupy a very large amount of memory, making it expensive to deploy them in data centers, and impractical to use them in memory-constrained environments like smart phones. %\citep{reference?}.
Thus, compressing these embeddings is an important problem.

While there have been numerous successful methods proposed recently for compressing embeddings \citep{sparse16,andrews16,dccl17,kway18}, these methods share a couple of important shortcomings.
First, these methods are generally quite computationally expensive and training them requires tuning many hyperparameters, costing valuable computation and developer time to use.
The current state-of-the-art methods, for example, train neural network models to represent each word as a sum of vectors from learned dictionaries \citep{dccl17,kway18}.
Second, these methods lack theoretical guarantees for determining when compression is likely to produce embeddings which perform well.
This makes it difficult for practitioners to confidently use these methods without extensive downstream verification.

In this work, we address both of these shortcomings.
First, we show that a simple and fast compression method based on uniform quantization can empirically compete with the state-of-the-art methods, while being orders of magnitude faster and requiring no hyperparameter tuning.
This method can compress a large embedding matrix in less than a minute on a single CPU core.
Second, we present generalization bounds for our compressed embeddings, revealing settings where strong generalization performance is guaranteed.

%Second, we present theoretical guarantees for our method, building on recent work in kernel ridge regression to understand how compression affects generalization performance in the context of linear ridge regression \citep{avron17,lprff18}.
%This analysis reveals settings where the uniformly quantized embeddings 
%In particular, we leverage a recent notion of spectral distance between matrices \citep{avron17,lprff18}
%In particular, we leverage the proposed notion of approximation between matrices, $(\Delta_1,\Delta_2)$-spectral approximation, to prove that when the compressed embedding is a close spectral approximation of the uncompressed embedding, strong generalization performance is guaranteed.

%This analysis reveals important settings under which compression can give large memory savings while provably retaining strong generalization performance.
%Together, these results show that compression can 
%analysis for how compression impacts generalization performance in the context of linear ridge regression.

%Second, we present generalization bounds for our compressed embeddings in the context of linear ridge regression.
%We leverage \citet{lprff18}'s notion of approximation between matrices called $(\Delta_1,\Delta_2)$-spectral approximation, and show that when the compressed embedding is close to the uncompressed embedding in this sense they are guaranteed to attain similar generalization performance.


%Second, we present generalization bounds for our compressed embeddings in the context of linear ridge regression, leveraging \citet{lprff18}'s notion of approximation between matrices called $(\Delta_1,\Delta_2)$-spectral approximation.
%We show that when the compressed embedding is close to the uncompressed embedding in this sense, they are guaranteed to attain similar generalization performance.


%Second, we present theoretical guarantees for our method, building on recent work in kernel ridge regression to understand how compression affects generalization performance in the context of linear ridge regression.
%\citet{lprff18} define a notion of approximation between matrices called $(\Delta_1,\Delta_2)$-spectral approximation, and we show that when the compressed embedding is close to the uncompressed embedding in this sense, they are guaranteed to attain similar generalization performance.

Empirically, we demonstrate that our compression method can match the performance of the state-of-the-art compression schemes on a variety of tasks and compression ratios.
We evaluate performance on question answering, sentiment analysis, and word analogy and similarity tasks, for both GloVe \citep{glove14} and fastText \citep{fasttext18} embeddings.
We observe that across tasks, our compression method can attain 32x compression within \todo{XX\%}-\todo{YY\%} performance relative to those attained by the state-of-the-art deep composition code learning (DCCL) approach~\citep{dccl17}.
%with \todo{XX\%}-\todo{YY\%} absolute performance loss, while DCCL attains \todo{XX\%}-\todo{YY\%}.
As an example, using the DrQA model \citep{drqa17} on the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, uniform quantization attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the uncompressed GloVe embeddings, while the DCCL performance is 0.43\% below at the same compression rate.

Theoretically, we analyze the generalization performance for our compressed embeddings in the context of linear ridge regression, revealing settings where the compressed embeddings are guaranteed to match the performance of the uncompressed ones.
Specifically, we leverage a recent notion of spectral approximation between matrices \citep{avron17,lprff18}, and show that when the compressed embeddings are a close approximation to the uncompressed ones, strong generalization performance is guaranteed.
We then show that our method will with high probability produce embeddings which are a close spectral approximation of the uncompressed embeddings, if the number of bits per matrix entry is chosen appropriately.
This analysis provides a sufficient condition---close spectral approximation---for guaranteeing strong performance.
Though our analysis is specific to linear ridge regression, we believe this is an important first step toward understanding compression's impact on generalization, and hope this work inspires further analysis.


%
%We then show that our embeddings will be a close spectral approximation with high probability when the quantization noise is small relative to the regularization parameter.
%
%
%
%We prove that our compression method produces th
%
%- First, if close approximation -> strong performance
%- Importantly, when 
%
%
%Noticeably, when pl
%
%This requires two steps: First, we leverage a recent notion of spectral approximation between matrices
%
%
%Toward this end, we leverage a recent notion of spectral approximation between matrices ,
%and prove that when this approximation is close strong generalization peformance is guaranteed.
%
%
%
%Theoretically, we 
%
%%As a first step toward understanding the generalization performance of our compressed embeddings, 
%
%
%Theoretically, we present generalization bounds for our compressed embeddings in the context of linear ridge regression.
%Specifically, we show that our method will with high probability produce embeddings which are a close $(\Delta_1,\Delta_2)$-spectral approximation of the uncompressed embeddings, if the number of bits per matrix entry is chosen appropriately.
%This analysis provides a sufficient condition---small $\Delta_1$ and $\Delta_2$ values---which practitioners can use to certify the quality of their compressed embeddings.
%%Furthermore, it reveals settings under which one can attain large memory savings while provably retaining strong generalization performance.
%Though our analysis is specific to linear ridge regression, we believe this is an important first step toward understanding compression's impact on generalization, and hope this work inspires further analysis.
%Though linear regression is admittedly a simplified setting, we believe our analysis is an important first step toward understanding compression's impact on generalization, and hope this work inspires further analysis.

%We hope this work inspires further analysis on how compression impacts generalization performance in more complex settings.

%
%Leveraging \citet{lprff18}'s notion of approximation between matrices called $(\Delta_1,\Delta_2)$-spectral approximation, we show that when the compressed embedding is close to the uncompressed embedding in this sense they are guaranteed to attain similar generalization performance.
%Then, we show that our compression method will with high probability produce embeddings which are a close spectral approximation of the uncompressed embeddings, if the number of bits per matrix entry is chosen appropriately.

The rest of this paper is organized as follows:
In Section~\ref{sec:relwork} we review related work, including existing methods for compressing word embeddings.
In Section~\ref{sec:uniform} we present our uniform quantization method for compressing embeddings.
We discuss our large-scale experiments in Section~\ref{sec:experiments}, and the theoretical analysis for our method in Section~\ref{sec:theory}.
We conclude in Section~\ref{sec:conclusion}.

%we expose to practitioners settings under which our compressed embeddings attain similar generalization performance to the the uncompressed embeddings.
%Leveraging \citep{lprff18}'s notion of $(\Delta_1,\Delta_2)$-spectral approximation between matrices, we show that when the compressed embedding are a close spectral approximation of the uncompressed embeddings, they will attain similar generalization performance.
%We then show that this condition holds with high probability for our compressed embeddings when the quantization noise is small relative to the generalization parameter.
%
%
%
%Theretically 
%
%
%Theoretically, we present novel analysis on the impact of compression on generalization ,
%
%Theoretically, we study the impact of compression on generalization performance for our method.
%In the context of linear ridge regression,
%
%we present an in-depth analysis of the way compression affects generalization performance, 
%
%
%
%
%Second, we present theoretical guarantees for when our method will succeed
%Second, we propose using a recent notion of spectral distance between matrices to determine when a compressed embedding is ``close'' to the uncompressed embedding.
%We present generalization bounds guaranteeing that the 
%
%
%Second, leverage recent generalization bounds from the context of ridge regression 
%
%Second, we propose a theoretical framework for understanding the generalization performance of compressed embeddings based on recent work in kernel ridge regression \citep{lprff18}.
%
%
%
%
%There are a couple shortcomings with the existing methods for compressing embeddings.
%First, these compression ds proposed for compressing embeddings \citep{sparse16,andrews16,dccl17,kway18} 
%
%First, these compression methods are generally quite computationally expensive and require tuning many hyperparameters.
%The current state-of-the-art methods, for example, train neural network models to represent each word as a sum of vectors from learned dictionaries \citep{dccl17,kway18}.
%Second, there is currently no theoretical framework to understand whether a compressed embedding will perform similarly to the uncompressed embedding on a downstream task.
%This absence of a framework makes it difficult to predict how a compressed embedding will perform without running a full downstream evaluation.
%%In addition to being expensive to train and tune, these compression schemes are difficult to analyze; for example, no guarantees exist for when the algorithms will produce embeddings with strong downstream performance.
%
%
%
%
%
%
%Furthermore, we present a theoretical framework 
%%This compression scheme is also amenable to theoretical analysis;
%%we present conditions under which the quantized embeddings will perform on par with the uncompressed embeddings in the context of linear ridge regression.
%%we show that the spectrum of the word embedding matrix plays an important role in determining whether its compressed representation will perform well, with slow spectral decay being more amenable to quantization.
%%The compression method has two parts: It first determines the optimal threshold at which to clip the extreme values in the word embedding matrix, and then uniformly quantizes the embeddings within the clipped interval.
%%This simple approach can compress large embeddings in under a minute on a single CPU core.
%
%Empirically, we demonstrate that uniform quantization can match the performance of the state-of-the-art compression schemes on a variety of tasks and compression ratios.
%We evaluate performance on question answering, sentiment analysis, and word analogy and similarity tasks, for both GloVe \citep{glove14} and fastText \citep{fasttext18} embeddings.
%Using the DrQA model \citep{drqa17} on the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, for example, uniform quantization attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the uncompressed GloVe embeddings, while the deep dictionary learning method of \citet{dccl17} is 0.43\% below.
%%For the fixed memory budget experiments, we show that 400-dimensional GloVe embeddings quantized to 1 bit per entry attain an average DrQA F1 score of 77.2\%, while 25-dimensional embeddings quantized to 16 bits per entry get 70.1\%.
%
%Theoretically, we show that quantization has a negligible effect on generalization performance when the quantization noise is small relative to the regularization parameter, in the context of linear ridge regression.
%Furthermore, we show that when the spectrum of the word embedding matrix decays slowly, strong regularization has minimal impact on generalization performance.
%Combined, these two results imply that when a word embedding matrix has a slow spectral decay (which we empirically observe to be true), it's quantized representation will perform similarly to the uncompressed embeddings at high compression rates.
%These results provide a novel framework for understanding when compression is likely to succeed.
%Though this is a simplified setting, it is an important first step toward understanding how compression impacts generalization performance, and provides important insights on when compression is possible.
%
%
%Theoretically, we show that when the spectral distance between the compressed and uncompressed embeddings is small, they will attain similar generalization performance.
%
%leverage a recent notion of spectral distance between matrices to show that when this distance is small, 
%
%
%
%Theoretically, we expose to practitioners settings under which our compressed embeddings attain similar generalization performance to the the uncompressed embeddings.
%Leveraging \citep{lprff18}'s notion of $(\Delta_1,\Delta_2)$-spectral approximation between matrices, we show that when the compressed embedding are a close spectral approximation of the uncompressed embeddings, they will attain similar generalization performance.
%We then show that this condition holds with high probability for our compressed embeddings when the quantization noise is small relative to the generalization parameter.
%Though linear regression is a simplified setting, it is an important first step toward understanding how compression impacts generalization performance, and provides important insights on when compression will succeed.
%
%
%
%
%
%
%The rest of this paper is organized as follows:
%In Section~\ref{sec:relwork} we review related work, including existing methods for compressing word embeddings.
%In Section~\ref{sec:uniform} we present our uniform quantization method for compressing embeddings.
%We discuss our large-scale experiments in Section~\ref{sec:experiments}, and the theoretical analysis for our method in Section~\ref{sec:theory}.
%We conclude in Section~\ref{sec:conclusion}.

%Designing simple and fast embedding compression methods with strong empirical performance is challenging.
%There have been numerous important works proposing compression schemes for word embeddings and studying their empirical performance \citep{sparse16,andrews16,dccl17,kway18};
%for example, current state-of-the-art methods train neural network models to represent each word as a sum of vectors from learned dictionaries \citep{dccl17,kway18}.
%Although these methods can attain strong performance on a range of NLP tasks such as language modeling \citep{mikolov10} and machine translation \citep{bahdanau15}, 
%designing and training the neural networks for compression can be labor-intensive and computationally expensive.
%Furthermore, the complexity of these compression methods makes it difficult to analyze the impact that compression has on the downstream performance of the embeddings.

%In this work, we show that a simple compression method based on uniform quantization can empirically compete with the state-of-the-art methods, and we present generalization bounds for the quantized embeddings for linear regression models.
%The compression method has two parts: first, it determines the optimal threshold at which to clip the extreme values in the word embedding matrix using golden section search \citep{golden53}.
%It then uniformly quantizes the clipped embeddings within the clipped interval.
%This simple approach can compress large embeddings in seconds on a single CPU core, without computationally expensive training or hyperparameter tuning.

%Empirically, we demonstrate that uniform quantization can match the performance of the state-of-the-art compression schemes on a variety of tasks.
%In our comparisons to other compression schemes, we evaluate performance on question answering, sentiment analysis, and word analogy and similarity tasks, for both GloVe \citep{glove14} and fastText \citep{fasttext18} embeddings.
%Using the DrQA model \citep{drqa17} on the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, for example, uniform quantization attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the uncompressed GloVe embeddings, while the deep dictionary learning method of \citet{dccl17} is 0.43\% below.
%, and that low-precision high-dimensional quantized embeddings can attain significantly better performance than full-precision lower-dimensional embeddings of equal size.
%In our comparisons to other compression schemes, we evaluate performance on question answering, sentiment analysis, and word analogy and similarity tasks, for both GloVe \citep{glove14} and fastText \citep{fasttext18} embeddings.
%Using the DrQA model \citep{drqa17} on the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, for example, uniform quantization attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the uncompressed GloVe embeddings, while the deep dictionary learning method of \citet{dccl17} is 0.43\% below.
%For the fixed memory budget experiments, we show that 400-dimensional GloVe embeddings quantized to 1 bit per entry attain an average DrQA F1 score of 77.2\%, while 25-dimensional embeddings quantized to 16 bits per entry get 70.1\%.

%Theoretically, we analyze in the context of linear ridge regression how the dimension and precision of uniformly quantized word embeddings jointly impact generalization performance.
%We demonstrate that when choosing the dimension of word embeddings there is a bias-variance trade-off, and that there can thus an optimal dimension; although this phenomenon has been observed \citep{landauer97} and analyzed \citep{yin18} previously, this explanation provides a novel perspective.
%We then analyze how quantization impacts generalization performance; we show that quantized embeddings can attain comparable generalization performance to full-precision embeddings at high compression rates when the spectrum of the embedding matrix decays slowly (which we empirically observe to be true).
%Together, these insights suggest that one can attain large improvements in generalization performance in the memory constrained setting by using low-precision embeddings whose dimension approaches but does not exceed the optimal dimensionality.

%The rest of this paper is organized as follows:
%In Section~\ref{sec:relwork} we review related work, including existing methods for compressing word embeddings.
%In Section~\ref{sec:uniform} we present our uniform quantization method for compressing embeddings.
%We discuss our large-scale experiments in Section~\ref{sec:experiments}, and the theoretical analysis for our method in Section~\ref{sec:theory}.
%We conclude in Section~\ref{sec:conclusion}.

%In this paper, we show that a simple compression method based on uniform quantization can empirically compete with the state-of-the-art methods, while compressing large embeddings in seconds on a single CPU core without any training or hyperparameter tuning.
%%This method has two components: first, it determines the optimal threshold at which to clip the extreme values in the word embedding matrix using golden section search \citep{golden53}.
%%It then uniformly quantizes the clipped embeddings to fixed-point representations within the clipped interval.
%%This simple approach can compress large embeddings in seconds on a single CPU core, without computationally expensive training or hyperparameter tuning.
%Specifically, uniform quantization attains competitive performance across question answering, sentiment analysis, and word analogy and similarity tasks, for both GloVe \citep{glove14} and fastText \citep{fasttext18} embeddings.
%Using the DrQA model \citep{drqa17} on the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, for example, it attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the uncompressed GloVe embeddings, while the state-of-the-art deep compositional code learning method \cite{dccl17} is 0.43\% below.
%%\todo{Discuss other tasks}
%
%The simplicity of our compression scheme also makes it amenable to theoretical analysis.
%In the context of linear ridge regression, we analyze the 




%Designing embedding compression methods with both strong empirical performance and provable generalization bounds for downstream tasks is challenging.
%There have been numerous important works proposing compression schemes for word embeddings and studying their empirical performance \citep{sparse16,andrews16,dccl17,kway18};
%for example, current state-of-the-art methods train neural network models to represent each word as a sum of vectors from learned dictionaries \citep{dccl17}.
%Although these methods can attain large reductions in memory while performing well on tasks such as language modeling \citep{mikolov10} and machine translation \citep{bahdanau15}, to the best of our knowledge there exists no analysis describing the effect compression has on generalization performance for these methods.

%In this work, we show that a simple compression method based on uniform quantization can empirically compete with the state-of-the-art methods, and we present generalization bounds for the quantized embeddings for linear regression models.
%The compression method has two parts: first, it determines the optimal threshold at which to clip the extreme values in the word embedding matrix using golden section search \citep{golden53}.
%It then uniformly quantizes the clipped embeddings within the clipped interval.
%This simple approach can compress large embeddings in seconds on a single CPU core, without computationally expensive training or hyperparameter tuning.


\subsection{OLD VERSION}
In recent years, \textit{word embeddings} \citep{word2vec13,glove14,fasttext18} have brought large improvements to a wide range of applications in natural language processing (NLP) \citep{collins16,drqa17}.
By encoding words as low-dimensional dense vectors, word embeddings allow NLP tasks to be framed as pattern recognition problems over dense continuous spaces, which can then be tackled using the powerful machinery of neural networks.
However, these word embeddings can occupy a very large amount of memory, making it expensive to deploy them in data centers, and impractical to use them in memory-constrained environments like smart phones.
Thus, compressing these embeddings is an important problem.

Designing simple and fast embedding compression methods with strong empirical performance is challenging.
There have been numerous important works proposing compression schemes for word embeddings and studying their empirical performance \citep{sparse16,andrews16,dccl17,kway18};
for example, current state-of-the-art methods train neural network models to represent each word as a sum of vectors from learned dictionaries \citep{dccl17,kway18}.
Although these methods can attain strong performance on a range of natural language processing tasks such as language modeling \citep{mikolov10} and machine translation \citep{bahdanau15}, designing and training the neural network for compression can be time-consuming.

In this paper, we work on a simple compression method based on uniform quantization. We show that it can empirically compete with the state-of-the-art methods, and we analysis theoretically to understand how to optimize down steam task performance under a fixed memory budget for the quantized embedding.
Our simple compression method has two components: first, it determines the optimal threshold at which to clip the extreme values in the word embedding matrix using golden section search \citep{golden53}.
It then uniformly quantizes the clipped embeddings to fixed-point representations within the clipped interval.
This simple approach can compress large embeddings in seconds on a single CPU core, without computationally expensive training or hyperparameter tuning.


Empirically, we demonstrate that this uniform quantization method can match the performance of the state-of-the-art compression schemes on question answering, sentiment analysis, and word analogy and similarity tasks, for both GloVe \citep{glove14} and fastText \citep{fasttext18} embeddings.
Using the DrQA model \citep{drqa17} on the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, for example, it attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the uncompressed GloVe embeddings, while the state-of-the-art deep compositional code learning method \cite{dccl17} is 0.43\% below.
%\todo{Discuss other tasks}

Theoretically, we work on understanding a critical question regarding embedding quantization given a fixed memory budget to store embedding matrices --- {to optimize downstream task performance, should we quantize a higher dimensional uncompressed embedding to lower precision, or should we quantize a lower dimensional uncompressed embedding to higher precision.} Specifically, we analyze generalization bounds in the case of linear ridge regression using word embeddings.
Our analysis reveals that one can optimize down stream performance under the fixed memory budget following a two-steps principle. In the first step, we show an optimal dimensionality exists as a consequence of the bias-variance trade-off in generalization performance. In the second step, we show quantization can have negligible impact on the generalization bound when the uncompressed embedding has a slow-decaying spectrum (empirically observed to be true). Thus under limit memory budgets, one should use the lowest quantization precision until the quantized embedding reaches the optimal dimensionality for the uncompressed counter-part. 

The rest of this paper is organized as follows:
In Section~\ref{sec:relwork} we review related work, including existing methods for compressing word embeddings.
In Section~\ref{sec:uniform} we present our uniform quantization method for compressing embeddings.
We discuss our large-scale experiments in Section~\ref{sec:experiments}, and the theoretical analysis for our method in Section~\ref{sec:theory}.
We conclude in Section~\ref{sec:conclusion}.



%Theoretically, we show in the context of linear ridge regression that quantization has a negligible effect on generalization performance when the quantization noise is small relative to the regularization parameter.
%Furthermore, we show that when the spectrum of the word embedding matrix decays slowly (which we empirically observe to be true), strong regularization has minimal impact on generalization performance.
%Combined, these two results imply that the uniformly quantized embeddings can have similar generalization performance to the uncompressed embeddings at high compression rates.
%
%1. Theoretically, we analyze the trade-off between dimensionality and precision. From this analysis, we extract guidelines how to optimize for generalization performance given a fixed memory budget: there exists an optimal dimension without 
%2. Specifically, we analyze in the linear regression setting.
%3. We first go in renosante, show there is an optimal without memory constraint.
%3. We then show quantization has minimal effect in generalizaiton
%These two understanding combines and lead to our stuff.
%
%
%
%Theoretically, we show in the context of linear ridge regression that quantization has a negligible effect on generalization performance when the quantization noise is small relative to the regularization parameter.
%Furthermore, we show that when the spectrum of the word embedding matrix decays slowly (which we empirically observe to be true), strong regularization has minimal impact on generalization performance.
%Combined, these two results imply that the uniformly quantized embeddings can have similar generalization performance to the uncompressed embeddings at high compression rates.




%\subsection{Jian Version}
%In recent years, \textit{word embeddings} \citep{word2vec13,glove14,fasttext18} have brought large improvements to a wide range of applications in natural language processing (NLP) \citep{collins16,drqa17}.
%By encoding words as low-dimensional dense vectors, word embeddings allow NLP tasks to be framed as pattern recognition problems over dense continuous spaces, which can then be tackled using the powerful machinery of neural networks.
%However, these word embeddings can occupy a very large amount of memory, making it expensive to deploy in the data center, and impractical to deploy them to memory-constrained environments like smart phones.
%Thus, it is important to decrease the amount of memory occupied by the embeddings for massive deployment.
%
%Designing embedding compression methods with both strong empirical performance and provable bounds on generalization performance is challenging.
%For example, the current state-of-the-art methods for compressing word embeddings are deep learning-based approaches, such as deep compositional code \citep{dccl17} and K-way discrete code \citep{chen2018learning}. In these methods, the compression are achieved by training a neural network architectures to represent each word as a sum of vectors from learned dictionaries.
%
%
%In this work, we revisit a compression method based on simple uniform quantization, and analyze the impact of quantization on generalization performance in the case of linear regression models.
%The quantization-based approach has two parts: By using golden section search, we find the optimal threshold at which to clip the extreme values in the word embedding matrix. We then uniformly quantize the clipped embeddings within the clipped interval. Based on these two steps, this simple approach can achieve fast compression without computationally expensive training or extensive hyperparameter tuning.
%
%Empirically, we demonstrate that uniform quantization can match the performance of the more complex baselines across a variety of tasks, embedding types, and compression rates.
%On the machine reading comprehension task, we evaluate different embeddings with DrQA model \cite{drqa17} using the Stanford Question Answering Dataset (SQuAD) \citep{squad16}. Uniform quantization can attain 32x compression for GloVe and fastText embeddings, while on average attaining F1 scores within 0.45\% and \todo{XX\%} absolute of the full-precision embeddings, respectively. To evaluate on more NLP applications with various model architecture, we also show that quantized embeddings matche the generalization performance of embeddings compressed with state-of-the-art deep learning-based methods, across 7 sentiment analysis, 2 word similarity and 2 analogy tasks. 
%
%We theoretically show that the quantization has negligible effect on generalization in important schemes when quantization error is relatively small comparing to regularization strength. Specifically we work in the case of linear regression, which is a simplified model of regression and ranking-based NLP tasks such as sentiment analysis. Our analysis reveals that when the spectrum of the word embedding matrix decays slowly (which we empirically observe to be true), strong regularization has minimally impact on generalization performance. Thus uniformly quantized embeddings can have similar generalization performance to that of uncompressed full precision embeddings.
%\todo{Discuss how clipping fits into theory.?}
%
%
%
%\subsection{Avner Version}
%In recent years, \textit{word embeddings} \citep{word2vec13,glove14,fasttext18} have brought large improvements to a wide range of applications in natural language processing (NLP) \citep{collins16,drqa17}.
%By encoding words as low-dimensional dense vectors, word embeddings allow NLP tasks to be framed as pattern recognition problems over dense continuous spaces, which can then be tackled using the powerful machinery of neural networks.
%However, these word embeddings can occupy a very large amount of memory, making it impractical to deploy them to memory-constrained environments like smart phones.
%Our goal in this work is to dramatically decrease the amount of memory occupied by the embeddings, while provably retaining strong performance on downstream tasks.
%
%Designing a compression scheme which is capable of retaining strong performance at low memory budgets, while also being amenable to theoretical analysis, is a challenging problem.
%There have been numerous important works proposing compression schemes for word embeddings and studying their empirical performance, for example using deep dictionary learning or sparse representations \citep{sparse16,andrews16,dccl17}.
%Although these methods can attain large reductions in memory usage while performing well on tasks such as language modeling \citep{mikolov10} and machine translation \citep{bahdanau15}, to the best of our knowledge there exists no analysis describing the effect compression has on generalization performance for these methods.
%
%%Designing powerful and principled compression schemes is challenging because it is unclear a priori how to model how compression affects performance.
%%This is particularly true when the compression schemes being considered are complex, and when large non-convex models are employed for the downstream tasks.
%%For example, the current state-of-the-art method for compressing word embeddings, called deep compositional code learning (DCCL), uses a deep architecture to represent each word as a sum of vectors from learned dictionaries \citep{dccl17}.
%
%In this work, we show that a simple compression method based on uniform quantization can compete with the above-mentioned methods, and we bound this method's impact on the generalization performance of linear regression models.
%This method has two parts: first, it determines the optimal threshold at which to clip the extreme values in the word embedding matrix.
%It then uniformly quantizes the clipped embeddings within the clipped interval.
%This algorithm is easy to implement, has no hyperparameters, and runs in seconds on a single CPU core.
%
%Empirically, we demonstrate that this method can match the performance of the state-of-the-art compression schemes on question answering, sentiment analysis, and word analogy and similarity tasks, for both GloVe \citep{glove14} and fastText \citep{fasttext18} embeddings.
%On the Stanford Question Answering Dataset (SQuAD) \citep{squad16}, for example, it attains a compression ratio of 32x with an average F1 score only 0.47\% absolute below the full-precision GloVe embeddings, while the deep dictionary learning method of \citet{dccl17} is 0.43\% below.
%
%Theoretically, we show in the context of linear ridge regression that quantization has a negligible effect on generalization performance when the quantization noise is small relative to the regularization parameter.
%Furthermore, we show that when the spectrum of the word embedding matrix decays slowly (which we empirically observe to be true), a large regularizer, and thus low-precision, will perform similarly to the full-precision embeddings.
%Importantly, our analysis is not specific to word embeddings; it directly extends to any setting in which a linear model is being learned over a uniformly quantized representation.
%%\todo{Discuss how clipping fits into theory. Motivate why regression matters (as opposed to classification) for NLP.}
%%Our theoretical analysis builds on recent work analyzing the generalization performance of low-precision representations in the context of kernel ridge regression \citep{lprff18}.
%
%Our main contributions:
%\begin{itemize}
%	\item We show that a simple compression scheme based on uniform quantization can compete with state-of-the-art word embedding compression methods on a variety of tasks.
%	\item We prove that this compression method's impact on the generalization performance of linear ridge regression models is minimal when the quantization noise is small relative to the regularization parameter.
%\end{itemize}



