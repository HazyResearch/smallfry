\subsection{NEW THEORY}
Assume $K = USU^T \in \RR^{n \times n}$ (rank $d$), $\tK = VRV^T\in \RR^{n \times n}$ (rank $k$).
Let $\{U_1,\ldots,U_n\}$ be the eigenvectors of $K$ in decreasing eigenvalue order, with the first $d$ having non-zero eigenvalue.
Similarly, let $\{V_1,\ldots,V_n\}$ be the eigenvectors of $\tK$ in decreasing eigenvalue order, with the first $k$ having non-zero eigenvalue.
Let $U = [U_1,\ldots U_d]$, $U_{\perp} = [U_{d+1},\ldots,U_n]$, $V = [V_1,\ldots V_k]$, $V_{\perp} = [V_{k+1},\ldots,V_n]$.

\begin{eqnarray*}
	R(K) &=& \frac{\lambda^2}{n}y^T (K+\lambda I)^{-2} y + \frac{\sigma^2}{n} \tr\bigg(K^2(K+\lambda I)^{-2}\bigg) \\
	n\cdot R(K) &=& \sum_{i=1}^d \Big(\frac{\lambda}{\sigma_i + \lambda}\Big)^2(U_i^T y)^2 + \sum_{i=d+1}^n (U_i^T y)^2 + \sigma^2 \sum_{i=1}^d \Big(\frac{\sigma_i}{\sigma_i + \lambda}\Big)^2 \\
\end{eqnarray*}
If we assume that we are in the noiseless linear regression setting ($\sigma=0$, $\lambda=0$), instead of the noisy linear ridge regression setting, this simplifies:
\begin{eqnarray*}
	n\cdot R(K) &=& \sum_{i=d+1}^n (U_i^T y)^2 \\
	&=& y^T U_{\perp} U_{\perp}^T y \\
	&=& y^T (I - UU^T) y \\
	&=& \|y\|^2 - y^T UU^T y \\
	n\cdot R(\tK) &=& \|y\|^2 - y^T VV^T y \\
	n(R(\tK) - R(K)) &=& \Big(\|y\|^2 - y^T VV^T y\Big) - \Big(\|y\|^2 - y^T UU^T y\Big)\\
	&=& y^T (UU^T - VV^T) y \\
	&\leq& \|y\|^2 \|UU^T - VV^T\|_2 \\
	&\leq& \|y\|^2 \|UU^T - VV^T\|_F \\
	&\leq& \|y\|^2 \sqrt{d + k - 2 \|U^T V\|_F^2} \quad \text{(by Lemma\ref{lemma1})}\\
\end{eqnarray*}

ANOTHER VERSION\\
Here we consider a random $y \in \RR^n$ vector with identity covariance.
\begin{eqnarray*}
	\expect{y}{n(R(\tK) - R(K))} &=& \expect{y}{\Big(\|y\|^2 - y^T VV^T y\Big) - \Big(\|y\|^2 - y^T UU^T y\Big)}\\
	&=& \expect{y}{y^T (UU^T - VV^T) y} \\
	&=& \expect{y}{y^T UU^T y - y^TVV^T y} \\
	&=& \|U\|_F^2  - \expect{y}{y^TVV^T y} \\
	&\leq& d - \|V^T U\|_F^2 \quad \text{by derivation below.} \\
	\expect{y}{n(R(K) - R(\tK))} &\leq& k - \|V^T U\|_F^2 \\
	\expect{y}{y^T VV^T y} &=& \expect{y}{\|V^T y\|_2^2} \\
	&=& \expect{y}{\|VV^T y\|_2^2} \\
	&\geq&  \expect{y}{\|VV^T UU^T y\|_2^2}  \quad\text{(projecting onto $U$ first makes $y$ magnitude smaller)}\\
	&=& \|VV^T UU^T\|_F^2 \quad \text{(by Lemma\ref{lemma2})}\\
	&=& \tr\Big(UU^TVV^TVV^TUU^T \Big) \\
	&=& \tr\Big((U^TV) V^TV (V^TU) U^TU \Big) \\
	&=& \tr \Big( (V^T U)^T (V^T U)\Big) \\
	&=& \|V^T U\|_F^2
\end{eqnarray*}

ONE MORE VERSION
\begin{eqnarray*}
	n(R(\tK) - R(K)) &=& \Big(\|y\|^2 - y^T VV^T y\Big) - \Big(\|y\|^2 - y^T UU^T y\Big)\\
	&=& y^T (UU^T - VV^T) y \\
	&=& y^T UU^T y - y^TVV^T y \\
	&=& \text{add line here.} \\
	y^T VV^T y &=& \|V^T y\|_2^2 \\
	&=& \|VV^T y\|_2^2 \\
	&\geq&  \|VV^T UU^T y\|_2^2  \quad\text{(projecting onto $U$ first makes $y$ magnitude smaller)}\\
	&=&  \|(V^T U) (U^T y)\|_2^2 \\
	&\geq& \sigma_{min}(V^T U)^2 \|U^T y\|^2 \\
%	&=& \tr\Big(y^T UU^TVV^TVV^TUU^T y \Big) \\
%	&=& \tr\Big(UU^TVV^TVV^TUU^T yy^T \Big) \\
%	&=& \tr\Big(U (U^TV) (V^TU) U^T yy^T \Big) \\
%	&=& \tr\Big((U^TV) (V^TU) (U^T y)(U^T y)^T \Big) \\
%	&\geq& \text{NOT SURE HOW TO LOWER BOUND}
%	&=& \tr\Big((U^TV) V^TV (V^TU) U^TU \Big) \\
%	&=& \tr \Big( (V^T U)^T (V^T U)\Big) \\
%	&=& \|V^T U\|_F^2
\end{eqnarray*}
So the worst case analysis cares about the minimum singular value of $V^T U$. \todo{But this will be 0 if rank of $V$ is smaller than rank of $U$, right??}

ONE MORE:\\
Tri says we can use the fact that the singular values of $UU^T-VV^T$ are the same as those of $U^{\perp} U^{\perp T} VV^T$ (but repeated twice).  And then use sin(theta) theorem to argue that $\|U^{\perp} U^{\perp T} VV^T\|$ is small


\begin{lemma}
\label{lemma1}
\begin{eqnarray*}
	\|UU^T - VV^T \|_F^2 &=& \tr\Big( (UU^T - VV^T)^T (UU^T - VV^T)  \Big)\\
	&=& \tr(UU^TUU^T) + \tr(VV^T VV^T) - \tr(VV^T UU^T) - \tr(UU^T VV^T) \\
	&=& d + k - 2 \tr((U^T V)^T (U^T V)) \\
	&=& d + k - 2 \|U^T V\|_F^2
\end{eqnarray*}
\end{lemma}

\begin{lemma}
\label{lemma2}
Let $A \in \RR^{n\times d}$ be an arbitrary matrix. If a random vector $x \in \RR^d$ is drawn from a distribution with identiy covariance matrix, then $\|A\|_F^2 = \expect{x}{\|Ax\|_2^2}$.
\end{lemma}
\begin{proof}
\begin{eqnarray*}
\expect{x}{\|Ax\|_2^2} &=& \expect{x}{(Ax)^T (Ax)} \\
&=& \expect{x}{x^T A^T A x} \\
&=& \expect{x}{\tr \Big(x^T A^T A x\Big)} \\
&=& \expect{x}{\tr \Big(A^T A x x^T\Big)} \\
&=& \tr \Big(A^T A \cdot \expect{x}{x x^T}\Big) \\
&=& \tr \Big(A^T A\Big) \\
&=& \|A\|_F^2 \\
\end{eqnarray*}
\end{proof}

Let $\{a_i\}_{i=1}^r$ be an orthonormal basis of $U \backslash V$, $\{b_j\}_{i=1}^s$ be an orthonormal basis of $V / U$, and $\{c_k\}_{i=1}^t$ be an orthonormal basis of $U \cap V$.
\begin{eqnarray*}
	\|UU^T - VV^T \|_F^2 &=& \sum_{i=1}^r \|(UU^T - VV^T)a_i\|^2 + \sum_{j=1}^s \|(UU^T - VV^T)b_j\|^2 + \sum_{k=1}^t \|(UU^T - VV^T)c_k\|^2 \\
	&=& r + s
\end{eqnarray*}

\subsection{Theory Summary}
We have observed that word embedding matrices have pretty flat spectra, with large smallest non-zero singular values.  Here, we show that this implies that one can use a large regularizer $\lambda$ while attaining similar generalization performance to smaller $\lambda$.  We then show that large $\lambda$ implies small relative spectral distance between the full-precision embedding matrix and the low-precision embedding matrix with high probability.  We know from prior work that small relative spectral distance gives tight generalization bounds.

%High-level logic:
%\begin{enumerate}
%	\item If the optimal regularizer $\lambda^* \in [0,a\sm]$ for some $a \in [0,1]$, we show that we can actually attain similar generalization performance if we use $\lambda = a\sm$ instead of $\lambda^*$.  Using a large regularizer $\lambda = a\sm$ allows us to with high probability have small relative spectral distance between $ZZ^T$ and $(Z+C)(Z+C)^T$ (with respect to this $\lambda$).  This gives us a bound on the generalization performance of $(Z+C)(Z+C)^T$ using this $\lambda$, relative to (an upper bound on) the generalization performance of $ZZ^T$ with this $\lambda$.  Because we know that training with $a\sm$ on $ZZ^T$ gives performance close to optimal, this bounds the performance of the quantized embeddings with $\lambda = a\sm$ in terms of the best possible generalization performance of the full-precision embeddings.
%	\item If the optimal $\lambda^*$ is larger than $\sm$, then we have small relative spectral distance between $ZZ^T$ and $(Z+C)(Z+C)^T$ (with respect to this $\lambda^*$), and this directly bounds the generalization performance of the quantized features in terms of the best possible full-precision performance.
%\end{enumerate}
	
%Here, we show that this implies small relative spectral distance between the Gram matrices of the quantized vs.\ non-quantized embeddings.  This then implies that learning a linear regression model over the quantized embeddings should perform similarly to the model training on the full-precision embeddings (by the generalization bounds in LP-RFF paper), for very low-precision $b$.

%\textbf{Open Issue}: Using Tropp Style bounds to show that this result holds with high-probability for a random quantization of the embeddings (as opposed to showing it holds for the \textit{expectation} of the Gram matrix).

%$$w^* = (X^T X + \lambda I)^{-1}X^T (y+\eps)$$
%
%$$\expect{\eps}{\|y - Xw^*\|^2}$$


\subsection{Notation and Definitions}
We consider the setting of fixed design ridge regression over a fixed data matrix $X \in \RR^{n\times d}$ (in our case, a word embedding matrix). Let $y_i = \by_i + \eps_i \in \RR$ be the observed noisy label for the $i^{th}$ word $z_i$, where the noise satisfies $\expect{}{\eps_i} = 0$ and $\var{}{\eps_i} = \sigma^2$.  Let $K = XX^T$ be the Gram matrix corresponding to this data.  Let $X+C$ be a random quantization of $Z$ to $b$ bits, and let $\tK = (X+C)(X+C)^T$. Assume $\expect{}{C_{ij}} = 0$, and let $\delta_b^2/d \geq \expect{}{C_{ij}^2}$ be an upper bound on the variance of the entry-wise random quantization noise (and thus $0 \preceq \expect{}{CC^T} \preceq \delta_b^2 I_n$).  $\|X\|$ will denote spectral norm of $X$, $\|X\|_F$ will denote Frobenius norm.

We will assume $X_{ij} \in [-\frac{1}{\sqrt{d}},\frac{1}{\sqrt{d}}]$.  If we uniformly quantize this interval using $b$ bits, the size of each sub-interval is $\frac{2}{\sqrt{d}(2^b-1)}$.  Thus, using the fact that a bounded random variable in an interval of length $r$ has variance at most $r^2/4$, we get that $\expect{}{C_{ij}^2} \leq \Big(\frac{2}{\sqrt{d}(2^b-1)}\Big)^2\cdot \frac{1}{4} = \frac{1}{d(2^b-1)^2} = \delta_b^2/d$, for $\delta_b^2 \defeq 1/(2^b-1)^2$.

 
%
%From the LP-RFF work, recall the definition of relative spectral distance:
%$$D_{\lambda}(K,\tK) = \min\bigg\{\Delta \in \RR^+ \;\bigg|\; \frac{1}{1+\Delta}(K+\lambda I) \preceq \tK+\lambda I \preceq (1+\Delta)(K+\lambda I)\bigg\}$$

%It is easy to see that $\expect{}{(Z+C)(Z+C)^T} = K + D$, where $D$ is a diagonal matrix satisfying $0\preceq D\preceq d \cdot \delta_b^2 I$ (we also show this in the LP-RFF work).  

\subsection{Results}
\begin{enumerate}
	\item \textbf{Theorem 1}: Large regularizer doesn't impact generalization much if spectrum decays slowly.
	\item \textbf{Theorem 2}: Large regularizer + low-precision = small ($\Delta_1,\Delta_2$) between full-precision and low-precision Gram matrices.  We know from prior work that this implies good generalization bounds.
%	\item \textbf{Theorem 3}: We directly bound the generalization performance of the models trained on the low-precision embeddings relative to the full-precision embeddings, showing that when the product $2^b(\sigma_d  + \lambda)$ is large, the degradataion in generalization performance is small.  Here, $\sigma_d$ denotes the smallest eigenvalue of the covariance matrix $X^T X$, $\lambda$ is the regularization parameter, and $b$ is the number of bits per entry of the low-precision embeddings.
\end{enumerate}

\subsubsection{Theorem 1}
\begin{theorem}
Let $X$ be a data matrix, and $\by$ be the corresponding vector of labels. Let $\sm$ be the smallest eigenvalue of $X^T X$, and let $\lambda_1, \lambda_2$ be two scalars such that $0 \leq \lambda_1 \leq \lambda_2 \leq a\cdot \sm$, for some $a \in [0,1]$. Letting $\cR_{\lambda}(K)$ denote the expected loss when training with regularizer $\lambda$, Gram matrix $K = XX^T$, and label noise $\sigma^2$, we get that:
\begin{equation}
\frac{R_{\lambda_2}(XX^T) - R_{\lambda_1}(XX^T)}{\|y\|^2/n} \leq a^2
\label{eq1}
\end{equation}
\end{theorem}
\begin{proof}
Let $K = U\Sigma U^T$ be the eigendecomposition of $K = XX^T$. Because $K$ is a rank $d$ matrix, we know the eigenvalues $\sigma_i=0$ for $i > d$.  We know from previous results [Avron '17, Aloui '15] that:
\begin{eqnarray*}
\cR_{\lambda}(K) &=& \frac{1}{n}\lambda^2\by^T(K+\lambda I)^{-2} \by + \frac{1}{n}\sigma^2 \tr\bigg(K^2(K+\lambda I)^{-2}\bigg) \\
&=& \frac{1}{n}\sum_{i=1}^n (U_i^T \by)^2\frac{\lambda^2}{(\sigma_i + \lambda)^2} + \frac{\sigma^2}{n}\sum_{i=1}^n \frac{\sigma_i^2}{(\sigma_i + \lambda)^2} \\
&=& \frac{1}{n}\sum_{i=1}^d (U_i^T \by)^2\frac{\lambda^2}{(\sigma_i + \lambda)^2} +
\frac{1}{n}\sum_{i=d+1}^n (U_i^T \by)^2 +
 \frac{\sigma^2}{n}\sum_{i=1}^d \frac{\sigma_i^2}{(\sigma_i + \lambda)^2} \\
\cR_{\lambda_2}(K) - \cR_{\lambda_1}(K) 
&=& \frac{1}{n}\sum_{i=1}^d (U_i^T \by)^2\bigg(\frac{\lambda_2^2}{(\sigma_i + \lambda_2)^2} - \frac{\lambda_1^2}{(\sigma_i + \lambda_1)^2}\bigg) +
\frac{\sigma^2}{n}\sum_{i=1}^d \bigg(\frac{\sigma_i^2}{(\sigma_i + \lambda_2)^2} - \frac{\sigma_i^2}{(\sigma_i + \lambda_1)^2} \bigg)\\
&\leq& \frac{1}{n}\sum_{i=1}^d (U_i^T \by)^2\frac{\lambda_2^2}{(\sigma_i + \lambda_2)^2}\\
&\leq& \frac{1}{n}\sum_{i=1}^d (U_i^T \by)^2\frac{a^2 \sm^2}{\sm^2}\\
&\leq& \frac{a^2}{n}\|y\|^2
\end{eqnarray*}
\end{proof}
\textbf{Remark}: We can understand the division by $\|y\|^2/n$ as a way to normalize the above difference in generalization performance; in particular, $\|y\|^2/n$ is the test error obtained by the model which always guesses $0$.

Also, note that in the case where $\sigma = 0$, $\lambda_1 = 0$, $\lambda_2 = a \sigma_{min}$, $\sigma_{max} = c\sigma_{min}$, and $\sum_{i=1}^d (U_i^T \by)^2 = \|y\|^2$, we can also lower bound Equation~\eqref{eq1} by $\frac{a^2}{(c+a)^2}$.

\subsubsection{Theorem 2}

\begin{theorem}
	\label{thm2}
	Let $X \in \RR^{n\times d}$ be a data matrix with corresponding (linear) kernel matrix $K = XX^T$; let $X+C$ denote a $b$-bit quantization of $X$, with $\tK = (X+C)(X+C)^T$ the kernel matrix of the quantized data matrix. Here, $C$ denotes the quantization noise, with $\expect{}{C_{ij}} = 0$ and $\var{}{C_{ij}} \leq \delta_b^2/d \;\;\forall i,j$, where $b$ is the number of bits used per feature.
	Then for any $\Delta_1 \geq 0, \Delta_2 \geq \delta^2_b/\lambda$,
	\begin{eqnarray}
	\Prob\Big[(1 - \Delta_1) (K + \lambda I_n) \preceq \tK + \lambda I_n \preceq (1 + \Delta_2) (K + \lambda I_n)
	\Big] 
	\geq \\ 1 - 
	n \exp \bigg(\frac{-\Delta_1^2}{2dL^2 + (2L/3)\Delta_1}\bigg) -
	n \exp \bigg(\frac{-(\Delta_2-\delta_b^2/\lambda)^2}{2dL^2 + (2L/3)(\Delta_2-\delta_b^2/\lambda)}\bigg),
	\end{eqnarray}
	for $L \defeq 5 \cdot \frac{2^b \cdot \delta_b^2}{\lambda}\cdot \frac{n}{d}$.
\end{theorem}


%\noindent\textbf{Remark:} If we approximate $\delta_b^2 \approx 2^{-2b}$, and let $5n/d \eqdef c$, then $L \approx c /(2^b \lambda)$, and the RHS of this bound becomes
%\begin{eqnarray*}
%1 - 
%n \exp \bigg(\frac{-\Delta_1^2 /L}{2dL + (2/3)\Delta_1}\bigg) -
%n \exp \bigg(\frac{-(\Delta_2-\delta_b^2/\lambda)^2 /L}{2dL + (2/3)(\Delta_2-\delta_b^2/\lambda)}\bigg) \\
%= 1 - 
%n \exp \bigg(\frac{-\Delta_1^2 (2^b \lambda)}{2dc^2/(2^b \lambda) + (2c/3)\Delta_1}\bigg) -
%n \exp \bigg(\frac{-(\Delta_2-1/(2^{2b}\lambda))^2 (2^b \lambda)}{2dc^2/(2^b \lambda) + (2c/3)(\Delta_2-1/(2^{2b}\lambda))}\bigg)
%% OLD VERSION
%%= 1 - 2n \exp \Bigg(\frac{-\Big(\Delta-1/(2^{2b}\lambda)\Big)^2 \cdot 2^b \lambda}{2dc^2/(2^b\lambda) + (2c/3)\Big(\Delta-1/(2^{2b}\lambda)\Big)}\Bigg).
%\end{eqnarray*}
%As $2^b \lambda \rightarrow \infty$, and letting $\Delta_1=\Delta_2\eqdef \Delta$ for simplicity, the dominant terms become
%\begin{eqnarray*}
%1 - 2n \exp \Bigg(\frac{-\Delta^2 \cdot 2^b \lambda}{(2c/3)\Delta}\Bigg) = 1 - 2n \exp \Bigg(\frac{-\Delta \cdot 2^b \lambda}{(2c/3)}\Bigg),
%\end{eqnarray*}
%This makes it clear that as $2^b \lambda \rightarrow \infty$, this probability goes to 1, for any $\Delta > 0$.  \textbf{Note that this is NOT the case for the bound in the LP-RFF paper!}

\begin{proof}
We conjugate the desired inequality with $B \defeq (K + \lambda I_n)^{-1/2}$ (\ie, 
multiply by $B$ on the left and right), noting that semidefinite ordering is
preserved by conjugation:
\begin{align*}
&(1 - \Delta_1) (K + \lambda I_n) \preceq \tK + \lambda I_n \preceq (1 + \Delta_2) (K + \lambda I_n) \\
\iff\ &(1 - \Delta_1) I_n \preceq B (\tK + \lambda I_n) B \preceq (1 + \Delta_2) I_n \\
\iff\ &-\Delta_1 I_n \preceq B (\tK + \lambda I_n) B - I_n \preceq \Delta_2 I_n \\
\iff\ &-\Delta_1 I_n \preceq B (\tK + \lambda I_n - K - \lambda I_n) B \preceq \Delta_2 I_n \\
\iff\ &-\Delta_1 I_n \preceq B (\tK - K) B \preceq \Delta_2 I_n.
\end{align*}

Letting $D=\expect{}{CC^T}$, and recalling  $0 \preceq D \preceq \delta_b^2 I_n$, 
by Lemma~\ref{lem1} we have that 
$$-\Delta_1 I_n \preceq B (\tK - K - D) B \preceq
(\Delta_2 - \delta^2_b/\lambda) I_n 
\Longrightarrow -\Delta_1 I_n \preceq B (\tK - K) B \preceq
\Delta_2 I_n,$$
and thus
$$\Prob\Big[ -\Delta_1 I_n \preceq B (\tK - K) B \preceq
\Delta_2 I_n \Big] \geq  \Prob\Big[ -\Delta_1 I_n \preceq B (\tK - K - D) B \preceq
(\Delta_2 - \delta^2_b/\lambda) I_n \Big].$$
	
We thus proceed to bound the RHS of this inequality, using 
Lemma~\ref{lem:quantized_concentration_two_sided}.
This allows us to conclude that for any $\Delta_1 \geq 0$, $\Delta_2 \geq \delta_b^2/\lambda$,
\begin{align*}
&\Prob\bigg[- \Delta_1 I_n \preceq B\Big((X + C)(X + C)^T - (XX^T+D)\Big)B \preceq (\Delta_2 - \delta^2_b/\lambda) I_n\bigg] \\
\geq\ &1 - n \left[ \exp \left( \frac{-\Delta_1^2/2}{dL^2 +
	L\Delta_1/3)} \right) + \exp \left(\frac{-(\Delta_2 - \delta^2_b/\lambda)^2/2}{dL^2 + L(\Delta_2 - \delta^2_b/\lambda)/3)} \right)  \right],
\end{align*}
for $L = 5 \cdot \frac{2^b \cdot \delta_b^2}{\lambda}\cdot  \frac{n}{d}$.

Thus, combining all of the above results, we get that
\begin{eqnarray*}
&&\Prob\Big[(1 - \Delta_1) (K + \lambda I_n) \preceq \tK + \lambda I_n \preceq (1 + \Delta_2) (K + \lambda I_n)
\Big] \\
&=& \Prob\Big[ -\Delta_1 I_n \preceq B (\tK - K) B \preceq
\Delta_2 I_n \Big] \\
&\geq& \Prob\Big[ -\Delta_1 I_n \preceq B (\tK - K - D) B \preceq
(\Delta_2 - \delta^2_b/\lambda) I_n \Big]\\
&\geq& 1 - n \left[ \exp \left( \frac{-\Delta_1^2/2}{dL^2 +
	L\Delta_1/3)} \right) + \exp \left(\frac{-(\Delta_2 - \delta^2_b/\lambda)^2/2}{dL^2 + L(\Delta_2 - \delta^2_b/\lambda)/3)} \right)  \right]
\end{eqnarray*}
\end{proof}


\begin{corollary}
	If $\Delta_1 \geq \frac{\log(n/\rho)L}{3}\Big(1+\sqrt{1+\frac{18d}{\log(n/\rho)}}\Big) \approx \frac{5n}{2^b \lambda}\sqrt{\frac{2\log(n/\rho)}{d}}$,
	then $\Prob\big[(1 - \Delta_1) (K + \lambda I_n) \preceq \tK + \lambda I_n \big] \geq  1 - \rho$. 
	Similarly, if $\Delta_2 \geq \frac{\delta_b^2}{\lambda} +  \frac{\log(n/\rho)L}{3}\Big(1+\sqrt{1+\frac{18d}{\log(n/\rho)}}\Big) \approx \frac{1}{2^{2b}\lambda} + \frac{5n}{2^b \lambda}\sqrt{\frac{2\log(n/\rho)}{d}}$,
	then $\Prob\big[\tK + \lambda I_n \preceq (1 + \Delta_2) (K + \lambda I_n)\big] \geq  1 - \rho$. 
\end{corollary}

\begin{proof}
Letting $\Delta_2 \rightarrow \infty$ in Theorem~\ref{thm2}, we see that 
$\Prob\big[(1 - \Delta_1) (K + \lambda I_n) \preceq \tK + \lambda I_n \big] \geq 1 - n \exp \bigg(\frac{-\Delta_1^2}{2dL^2 + (2L/3)\Delta_1}\bigg)$.
Lower-bounding the RHS by $1-\rho$, and solving for $\Delta_1$, we get the following set of equivalent statements (assuming $\Delta_1 \geq 0$, and $a\defeq \log(n/\rho)$):

\begin{eqnarray*}
1 - n \exp \bigg(\frac{-\Delta_1^2}{2dL^2 + (2L/3)\Delta_1}\bigg) &\geq& 1-\rho \\
\Longleftrightarrow \rho &\geq& n \exp \bigg(\frac{-\Delta_1^2}{2dL^2 + (2L/3)\Delta_1}\bigg) \\
\Longleftrightarrow \log(n/\rho) &\leq& \frac{\Delta_1^2}{2dL^2 + (2L/3)\Delta_1} \\
\Longleftrightarrow 0 &\leq&  \Delta_1^2 - (2aL/3)\Delta_1 - 2adL^2 \\
\Longleftrightarrow \Delta_1 &\geq&  \frac{1}{2}\bigg(\frac{2aL}{3} + \sqrt{\Big(\frac{2aL}{3}\Big)^2 + 8adL^2}\bigg) \\
\Longleftrightarrow \Delta_1 &\geq& \frac{aL}{3}\bigg(1 + \sqrt{1 + \frac{18d}{a}}\bigg)
\end{eqnarray*}
Now, using $L = 5 \cdot \frac{2^b \cdot \delta_b^2}{\lambda}\cdot  \frac{n}{d} \approx \frac{5n/d}{2^b \lambda}$ (because $\delta_b^2 = (2^b-1)^{-2} \approx 2^{-2b}$), $1 + \sqrt{1 + \frac{18d}{a}} \approx \sqrt{\frac{18d}{a}}$ (assuming $18d \gg a$), we get
\begin{eqnarray*}
	\Delta_1 &\gtrsim& \frac{\log(n/\rho)}{3}\cdot \frac{5n/d}{2^b \lambda}\cdot \sqrt{\frac{18d}{\log(n/\rho)}} \\
	&=& \frac{5n}{2^b \lambda}\sqrt{\frac{2\log(n/\rho)}{d}}.
\end{eqnarray*}
This completes the first part of the proof.
To prove the second part, we repeat the exact same steps with $\widehat{\Delta_2} = \Delta_2 - \delta_b^2/\lambda$, this time letting $\Delta_1 \rightarrow \infty$ in Theorem~\ref{thm2}.

\end{proof}

\subsection{Lemmas}
\begin{lemma}
	\label{lem1}
	Letting $K=XX^T$, $\tK = (X+C)(X+C)^T$, $B = (K+\lambda I_n)^{-1/2}$, $D = \expect{}{CC^T}$, with $0 \preceq D \preceq \delta_b^2 I_n$, it follows that:
	\begin{align*}
-\Delta_1 I_n &\preceq B (\tK - K - D) B \preceq
(\Delta_2 - \delta^2_b/\lambda) I_n \\
\Longrightarrow -\Delta_1 I_n &\preceq B (\tK - K) B \preceq
\Delta_2 I_n
	\end{align*}
\end{lemma}
\begin{proof}
Simply add $BDB$ to all sides, and notice that $0 \preceq BDB \preceq (\delta_b^2/\lambda) I_n$ (follows because $\|BDB\| \leq \|B^2\| \|D\|$, $0 \preceq D \preceq \delta_b^2 I_n$ and $B^2 = (K+\lambda I_n)^{-1}$).
\begin{eqnarray*}
-\Delta_1 I_n + BDB &\preceq& B (\tK - K - D) B  + BDB \preceq
(\Delta_2 - \delta^2_b/\lambda) I_n  + BDB \\
\Longrightarrow -\Delta_1 I_n \preceq  -\Delta_1 I_n + BDB &\preceq& B (\tK - K) B \preceq
\Delta_2 I_n - (\delta_b^2/\lambda) I_n  + BDB \preceq \Delta_2 I_n
\end{eqnarray*}
\end{proof}


\begin{lemma}
	Let $K=XX^T$ be a (linear) kernel matrix for $X \in\RR^{n \times d}$, and $\tK = (X+C)(X+C)^T$ be a $b$-bit quantization of $K$ with expectation $K+D$.
	Letting $B\defeq (K+\lambda I_n)^{-1/2}$ and $L \defeq 5 \cdot \frac{2^b \cdot \delta_b^2}{\lambda}\cdot  \frac{n}{d}$, then for any $t_1, t_2 \geq 0$,
	\begin{align*}
	&\Prob\bigg[- t_1 I_n \preceq B\Big((X + C)(X + C)^T - (XX^T+D)\Big)B \preceq t_2 I_n\bigg] \\
	\geq\ &1 - n \left[ \exp \left( \frac{-t_1^2/2}{dL^2 +
		Lt_1/3)} \right) + \exp \left(\frac{-t_2^2/2}{dL^2 + Lt_2/3)} \right)  \right].
	\end{align*}
	\label{lem:quantized_concentration_two_sided}
\end{lemma}

\begin{proof}
	Using the notation from Theorem~\ref{thm:intdim-bernstein-herm},
	we consider $S_k = B\Big((x_k + c_k)(x_k + c_k)^T  - x_k x_k^T - D_k\Big)B$, for $B\defeq (K+\lambda I_n)^{-1/2}$, $D_k = \expect{}{c_k c_k^T}$, and $x_k$ and $c_k$ denoting the $k^{th}$ columns of $X$ and $C$ respectively.
	It is easy to see that $Z\defeq \sum_{k=1}^d S_k = B((X+C)(X+C)^T - XX^T - D)B = B(\tK - K - D)B$, and that $\expect{}{S_k} = 0$.
	Thus, to apply Theorem~\ref{thm:intdim-bernstein-herm}, we simply need to find upper bounds $L,v(Z)$ such that $\lambda_{max}(S_k) \leq L$ and  $\|\sum_k \expect{}{S_k^2}\| \leq v(Z)$.
	By Lemma~\ref{upper_bounds}, $L \defeq 5 \cdot \frac{2^b \cdot \delta_b^2}{\lambda}\cdot  \frac{n}{d}$ and $v(Z) = dL^2$.
	
	Applying Theorem~\ref{thm:intdim-bernstein-herm} with $Z$, for any $t_2 \geq 0$,
	we have
	\begin{equation*}
	\Prob\bigg[\lambda_{\max}(B\Big((X + C)(X + C)^T - (XX^T+D)\Big)B) \succeq t_2 I_n \bigg] \leq n
	\exp \left( \frac{-t_2^2/2}{dL^2 + Lt_2/3)} \right).
	\end{equation*}
	Similarly, applying Theorem~\ref{thm:intdim-bernstein-herm} with $-Z$ and
	using the fact that $\lambda_{\max}(-Z) = -\lambda_{\min}(Z)$, for any $t_1 \geq 0$,
	we have
	\begin{equation*}
	\Prob\bigg[\lambda_{\min}(B\Big((X + C)(X + C)^T - (XX^T+D)\Big)B) \preceq -t_1 I_n \bigg] \leq n
	\exp \left( \frac{-t_1^2/2}{dL^2 + Lt_1/3)} \right).
	\end{equation*}
	Combining the two bounds with the union bound yields the desired inequality.
\end{proof}


\begin{lemma}
	\label{upper_bounds}
	Let $S_k = B\Big((x_k + c_k)(x_k + c_k)^T  - x_k x_k^T - D_k\Big)B$, with $B=(K+\lambda I_n)^{-1/2}$ and $D_k=\expect{}{c_k c_k^T}$.  It follows that
	$$\|S_k\| \leq 5 \cdot \frac{2^b \cdot \delta_b^2}{\lambda}\cdot  \frac{n}{d} \eqdef L,$$
	and
	$$\Big\|\sum_{k=1}^d \expect{}{S_k^2}\Big\| \leq dL^2.$$
\end{lemma}
\begin{proof}
\begin{eqnarray*}
\|S_k\| &=& \|B\Big((x_k + c_k)(x_k + c_k)^T  - x_k x_k^T - D_k\Big)B\| \\
&=& \|B\Big(x_kc_k^T + c_kx_k^T + c_k c_k^T - D_k\Big)B\| \\
&\leq& \|B^2\|\Big(\|x_kc_k^T\| + \|c_kx_k^T\| + \|c_k c_k^T\| + \|D_k\|\Big) \\
&=& \|B^2\|\Big(2\|x_k\|\|c_k\| + \|c_k\|^2 + \|D_k\|\Big) 
\end{eqnarray*}
To bound $\|x_k\|$ and $\|c_k\|$ we use the facts that $x_k$ is a vector of length $n$, with each entry bounded in magnitude by $1/\sqrt{d}$, and that $c_k$ is a vector of length $n$ with each entry bounded by $\frac{2}{\sqrt{d}(2^b-1)}$.  Thus, $\|x_k\| \leq \sqrt{n/d}$, and $\|c_k\| \leq \sqrt{4n/(d(2^b-1)^2)} = \sqrt{4\delta_b^2n/d} = 2\delta_b\sqrt{n/d}$.  We will also use the fact that $\|B^2\| = \|(K+\lambda I_n)^{-1}\| \leq 1/\lambda$.  Lastly, we note that $0 \preceq D_k = \expect{}{c_k c_k^T} = diag(\expect{}{c_{k1}^2},\ldots,\expect{}{c_{kn}^2}) \preceq (\delta_b^2/d) I_n$, and thus $\|D_k\| \leq \delta_b^2/d$.

We now continue the above chain of inequalities:
\begin{eqnarray*}
\|S_k\| &\leq& (1/\lambda)\Big(2\sqrt{n/d}\cdot 2\delta_b\sqrt{n/d} + 4\delta_b^2(n/d) + \delta_b^2/d\Big)\\
&=& (1/\lambda)\Big(4\delta_b(n/d) + 4\delta_b^2(n/d) + \delta_b^2/d\Big) \\
&=& (1/\lambda)\Big(4\delta_b^2(2^b-1)(n/d) + (4n + 1)(\delta_b^2/d)\Big) \\
&\leq& (1/\lambda)\Big(5\delta_b^2(2^b-1)(n/d) + 5n(\delta_b^2/d)\Big) \\
&=& (1/\lambda)\Big(5n(\delta_b^2/d)\Big((2^b-1)+1\Big)\Big) \\
&=& 5 \cdot \frac{2^b \cdot \delta_b^2}{\lambda}\cdot  \frac{n}{d} \eqdef L
\end{eqnarray*}
This implies that $\|S_k^2\| \leq L^2$.  It is now easy to see that 
$\Big\|\sum_{k=1}^d \expect{}{S_k^2}\Big\| \leq \sum_{k=1}^d \big\|\expect{}{S_k^2}\big\| \leq dL^2 \eqdef v(Z)$, completing the proof of the lemma.


%Now it's time to bound $\E[S_i^2]$.  We will use
%\begin{eqnarray*}
%	\E[S_i^2] &=& \frac{1}{m^2}\Big(\E\big[(u_i u_i^T)^2\big] - \E\big[u_iu_i^T\big]^2\Big) \preceq \frac{1}{m^2} \E[(u_i u_i^T)^2]
%	= \frac{1}{m^2} \E[u_i u_i^T u_i u_i^T] \\
%	&=& \frac{1}{m^2} \E[\norm{u_i}^2 u_i
%	u_i^T] \preceq \frac{2n \norm{B}^2}{m^2} \E[u_i u_i^T].
%\end{eqnarray*}
%Thus
%\begin{equation*}
%\sum_{i=1}^{m} \E[S_i^2] \preceq \frac{2n \norm{B}^2}{m} \E[v_1 v_1^T] = \frac{2n
%	\norm{B}^2}{m} B(K + D) B^T \preceq \frac{2n \norm{B}^2}{m} B(K + \delta^2_b I_n)B^T
%= LM/m.
%\end{equation*}



\end{proof}


\begin{theorem}[Matrix Bernstein: Hermitian Case (Theorem 6.6.1 Tropp)] \label{thm:intdim-bernstein-herm}
	Consider a finite sequence $\{ S_k \}$ of random Hermitian matrices in $\RR^{n\times n}$, and assume that
	\begin{equation*}
	\E [S_k] = 0
	\quad\text{and}\quad
	\lambda_{\max}(S_k) \leq L
	\quad\text{for each index $k$.}
	\end{equation*}
	Introduce the random matrix
	\begin{equation*}
	Z = \sum\nolimits_k S_k.
	\end{equation*}
	Let $v(Z)$ be the matrix variance statistic of the sum:
	\begin{equation*}
	v(Z) = \|\E [Z^2]\| = \|\sum_k \E [S_k^2]\|.
	\end{equation*}
	Then, for $t \geq 0$,
	\begin{equation} \label{eqn:intdim-bernstein-tail}
	\Prob \left( \lambda_{\max}(Z) \geq t \right)
	\leq n \cdot \exp\left( \frac{-t^2/2}{v(Y) + Lt/3} \right).
	\end{equation}
	\label{thm:bernstein}
\end{theorem}
	
%\begin{lemma}
%	\label{lem2}
%	Letting $R = (X^T X + \lambda I_d)^{-1/2}$, $D = \expect{}{C^T C}$, with $0 \preceq D \preceq \frac{n/d}{(2^b-1)^2} I_d$, it follows that:
%	\begin{align*}
%	-\bigg(t - \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\bigg) I_d &\preceq R (\tX^T \tX - X^T X - D) R \preceq
%	\bigg(t - \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\bigg) I_d \\
%	\Longrightarrow -t I_d &\preceq R (\tX^T \tX - X^T X) R \preceq t I_d
%	\end{align*}
%\end{lemma}
%\begin{proof}
%	Simply add $RDR$ to all sides, and notice that $\|RDR\| \leq \|R^2\|\|D\| \leq  \frac{(n/d)\delta_b^2}{\sigma_d+\lambda}$. Thus
%	\begin{align*}
%-\bigg(t - \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\bigg) I_d + RDR &\preceq R (\tX^T \tX - X^T X - D) R + RDR \preceq
%\bigg(t - \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\bigg) I_d +RDR \\
%\Longrightarrow -t I_d \preceq -t I_d + \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}I_d + RDR &\preceq R (\tX^T \tX - X^T X) R \preceq
%t I_d - \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)} I_d + RDR \preceq t I_d
%\end{align*}
%\end{proof}
%\begin{lemma}
%	\label{upper_bounds2}
%	Let $S_k = R\Big((x_k + c_k)(x_k + c_k)^T  - x_k x_k^T - D_k\Big)R$, with $R=(X^T X+\lambda I_d)^{-1/2}$ and $D_k = \expect{}{c_kc_k^T}$. Importantly, unlike in Lemma~\ref{upper_bounds}, here $x_k$ and $c_k$ will be vectors in $\RR^d$ representing the $k^{th}$ word embedding and its corresponding quantization noise (\textbf{apologies for overloading notation!}).  
%	It follows that
%		$$\|S_k\| \leq \frac{9}{(2^b-1)(\sigma_d + \lambda)} \eqdef L,$$
%		and
%		$$\Big\|\sum_{k=1}^n \expect{}{S_k^2}\Big\| \leq nL^2.$$
%	\end{lemma}
%	\begin{proof}
%Exactly like in Lemma~\ref{upper_bounds} we can show that
%		\begin{eqnarray*}
%			\|S_k\| &\leq& \|R^2\|\Big(2\|x_k\|\|c_k\| + \|c_k\|^2 + \|D_k\|\Big) 
%		\end{eqnarray*}
%		To bound $\|x_k\|$ and $\|c_k\|$ we use the facts that $x_k$ is a vector of length $d$, with each entry bounded in magnitude by $1/\sqrt{d}$, and that $c_k$ is a vector of length $d$ with each entry bounded by $\frac{2}{\sqrt{d}(2^b-1)}$.  Thus, $\|x_k\| \leq 1$, and $\|c_k\| \leq 2/(2^b-1)$.  We will also use the fact that $\|R^2\| = \|(X^T X +\lambda I_d)^{-1}\| \leq \frac{1}{\sigma_d + \lambda}$. Lastly, we note that $0 \preceq D_k = \expect{}{c_k c_k^T} = diag(\expect{}{c_{k1}^2},\ldots,\expect{}{c_{kd}^2}) \preceq (\delta_b^2/d) I_d$, and thus $\|D_k\| \leq \delta_b^2/d = \frac{1}{d(2^b-1)^2}$.
%		
%		We now continue the above chain of inequalities:
%		\begin{eqnarray*}
%			\|S_k\| &\leq&\frac{1}{\sigma_d + \lambda}\Big(\frac{4}{2^b-1} + \frac{4}{(2^b-1)^2} + \frac{1}{d(2^b-1)^2}\Big)\\
%			&\leq&\frac{1}{\sigma_d + \lambda}\Big(\frac{8 + 1/d}{2^b-1}\Big)\\
%			&\leq&\frac{9}{(\sigma_d + \lambda)(2^b-1)} \eqdef L\\
%		\end{eqnarray*}
%		This implies that $\|S_k^2\| \leq L^2$.  It is now easy to see that 
%		$\Big\|\sum_{k=1}^n \expect{}{S_k^2}\Big\| \leq \sum_{k=1}^n \big\|\expect{}{S_k^2}\big\| \leq nL^2$, completing the proof of the lemma.
%	\end{proof}

%\subsection{Theorem 3}
%\begin{theorem}
%	Let $X\in \RR^{n\times d}$ be a data matrix with $|X_{ij}| \leq \frac{1}{\sqrt{d}}$, and $\tX\defeq X+C$ denote a random $b$-bit quantization of $X$ with $\expect{}{C_{ij}}=0$, $\expect{}{C_{ij}^2} \leq \frac{1}{(2^b-1)^2}$, and $|C_{ij}| \leq \frac{2}{\sqrt{d}(2^b-1)} \; \forall i,j$.  
%	Let $\sigma_d$ be the smallest eigenvalue of $X^T X$, and $\tsigma_d$ be the smallest eigenvalue of $\tX^T \tX$.
%	Let $y \in \RR^n$ be the corresponding vector of ``clean'' labels, where $y_i + \eps_i$ are the noisy observed labels; assume $\expect{}{\eps_i} = 0$, $\expect{}{\eps_i^2} = \sigma^2$, and $|\eps_i| \leq a \; \forall i$.  Let $\cR_{\lambda}(X)$ denote the expected generalization error when training with regularizer $\lambda$, data matrix $X$, and label noise $\sigma^2$.  Let $f(t) = \bigg(\frac{8n}{(2^b-1)(\tsigma_d + \lambda)} +\frac{t}{1-t}\cdot \frac{n}{\sigma_d+\lambda}\bigg)\cdot \big(\|y\| + a\sqrt{n}\big)$ and let $L = \frac{9}{(2^b-1)(\sigma_d + \lambda)}$.  It follows that for any $t\geq \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}$
%	
%	$$R_{\lambda}(\tX) - R_{\lambda}(X) \leq f(t)\Big(2\sqrt{R_{\lambda}(X)}+f(t)\Big)$$
%	with probability at least 
%	
%	$$1 - 2d \exp \left(\frac{-\Big(t-\frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\Big)^2}{2nL^2 + (2L/3)\Big(t-\frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\Big)} \right)$$
%\end{theorem}
%\noindent\textbf{Remark:} If we approximate $(2^b-1)^2 \approx 2^{2b}$, $2^b-1\approx 2^b$, $L \approx \frac{9}{2^b(\sigma_d + \lambda)}$, and the RHS of this bound becomes
%\begin{eqnarray*}
%	\approx 1 - 2d \exp \left(\frac{-\Big(t-\frac{n/d}{2^{2b}(\sigma_d + \lambda)}\Big)^2\frac{1}{L}}{2nL + (2/3)\Big(t-\frac{n/d}{2^{2b}(\sigma_d + \lambda)}\Big)} \right) &=& 1 - 2d \exp \left(\frac{-\Big(t-\frac{n/d}{2^{2b}(\sigma_d + \lambda)}\Big)^2\frac{2^b(\sigma_d + \lambda)}{9}}{\frac{18n}{2^b(\sigma_d + \lambda)} + (2/3)\Big(t-\frac{n/d}{2^{2b}(\sigma_d + \lambda)}\Big)} \right)
%\end{eqnarray*}
%As $2^b (\sigma_d +\lambda) \rightarrow \infty$, the dominant terms become
%\begin{eqnarray*}
%	1 - 2d \exp \left(\frac{-t^2 \cdot \frac{2^b(\sigma_d + \lambda)}{9}}{(2t/3)} \right) = 1 - 2d \exp \left(-  \frac{2^b(\sigma_d + \lambda)}{6}\cdot t \right).
%\end{eqnarray*}
%This makes it clear that as $2^b (\sigma_d + \lambda) \rightarrow \infty$, this probability goes to 1, for any $t > 0$.
%
%\avner{I need to update $f(t)$ to not depend on $\tsigma_d$!  Of course we can simply upper bound $f(t)$ by setting $\tsigma_d = 0$, but it would be nicer if we were able to replace $\tsigma_d + \lambda$ with $\sigma_d + \lambda$ instead.}
%
%\begin{proof}
%	The generalization error in fixed design kernel ridge regression, with a linear kernel function, is $\expect{\eps}{\|y-X(X^T X + \lambda I)^{-1} X^T (y+\eps)\|^2}$.  We will denote this by $R_{\lambda}(X)$.  We would like to bound the difference between $R_{\lambda}(X)$ and $R_{\lambda}(\tX)$, where $\tX = X+C$ is a quantized version of $X$.  Let $r_{\eps,\lambda}(X) = \|y-X(X^T X + \lambda I)^{-1} X^T (y+\eps)\|$; note that we will simply write $r(X)$ for simplicity, suppressing the subscripts. For brevity we will use $A \defeq X^T X + \lambda I$, and $\tA \defeq \tX^T \tX + \lambda I$.  Our approach will be to bound $R_{\lambda}(\tX) - R_{\lambda}(X)$ in terms of $r(X) - r(\tX)$, as follows:
%	\begin{eqnarray*}
%		R_{\lambda}(\tX) - R_{\lambda}(X) &=& \expect{\eps}{\|y-\tX\tA^{-1} \tX^T (y+\eps)\|^2-\|y-XA^{-1} X^T (y+\eps)\|^2 } \\
%		&=& \expect{\eps}{r(\tX)^2 - r(X)^2} \\
%		&=& \expect{\eps}{\Big(r(\tX) - r(X)\Big)\Big(r(\tX)+r(X)\Big)}
%	\end{eqnarray*}
%	Thus, if we upper bound the magnitude of $r(\tX) - r(X)$ and $r(\tX) + r(X)$, we will have an upper bound on $R_{\lambda}(\tX) - R_{\lambda}(X)$. The proof will follow the following steps:
%	\begin{itemize}
%		\item \textbf{Step 1}: We begin by bounding $r(\tX) - r(X)$ under the assumption that $(1-t)A \preceq \tA \preceq (1+t)A$.
%		\item \textbf{Step 2}: Given such a bound $T \geq r(\tX) - r(X)$, we can bound $r(\tX) + r(X) = 2r(x) + (r(\tX) - r(X)) \leq 2r(X) + T$.  This gives an upper bound
%		\begin{eqnarray*}
%			R_{\lambda}(\tX) - R_{\lambda}(X) &\leq& \expect{\eps}{T\Big(2r(X)+T\Big)}\\
%			&=& T\Big(2\expect{\eps}{r(X)}+T\Big) \quad \text{(Assuming $T$ doesn't depend on $\eps$.)}\\
%			&\leq& T\Big(2\sqrt{R_{\lambda}(X)}+T\Big) \quad \text{(By Jensen's ineqality.)}
%		\end{eqnarray*}
%		\item \textbf{Step 3}: We lower bound the probability of the event  $(1-t)A \preceq \tA \preceq (1+t)A$ occurring.
%	\end{itemize}
%	
%	\noindent We now begin the work of bounding $r(\tX) - r(X)$.
%	\begin{eqnarray*}
%		r(\tX) - r(X) &=& \|y-\tX\tA^{-1} \tX^T (y+\eps)\| - \|y-X A^{-1} X^T (y+\eps)\| \\
%		&\leq& \Big\|\Big(y-\tX\tA^{-1} \tX^T (y+\eps)\Big) - \Big(y - X A^{-1} X^T (y+\eps)\Big)\Big\| \\
%		&=& \|\tX\tA^{-1} \tX^T (y+\eps) - X A^{-1} X^T(y+\eps)\| \\
%		&\leq& \big\|\tX\tA^{-1} \tX^T - X A^{-1} X^T\big\|\cdot \|y + \eps\| \\
%		&=& \big\|(X+C)\tA^{-1} (X+C)^T - X A^{-1} X^T\big\|\cdot \|y + \eps\| \\
%		&=& \big\|C \tA^{-1} X^T + X \tA^{-1} C^T + C \tA^{-1} C^T + X \tA^{-1} X^T - X A^{-1} X^T\big\|\cdot \|y + \eps\| \\
%		&\leq& \bigg(\|C\| \|\tA^{-1}\|\Big(2\|X\| + \|C\|\Big) + \big\|X \tA^{-1} X^T - X A^{-1} X^T\big\|\bigg)\cdot \big(\|y\| + \|\eps\|\big) 
%	\end{eqnarray*}
%	We now proceed to bound the individual elements in this expression. We use the facts that $X \in R^{n \times d}$ with $|X_{ij}| \leq 1/\sqrt{d}$,
%	$C \in R^{n \times d}$ with $|C_{ij}|\leq \frac{2}{\sqrt{d}(2^b-1)}$, and $\eps \in \RR^n$ with $|\eps_i| \leq a$.  We let $\sigma_i$ and $\tsigma_i$ denote the $i^{th}$ largest eigenvalues of $X^T X$ and $\tX^T \tX$ respectively.
%	\begin{eqnarray*}
%		\|C\| &\leq& \|C\|_F \leq \sqrt{nd \cdot \frac{4}{d(2^b-1)^2}} = \frac{2\sqrt{n}}{2^b-1} \\
%		\|X\| &\leq& \|X\|_F \leq \sqrt{n}\\
%		\|\tA^{-1}\| &=& \|(\tX^T\tX + \lambda I)^{-1}\|  = \frac{1}{\tsigma_d + \lambda} \\
%		\|\eps\| &\leq& a\sqrt{n}
%	\end{eqnarray*}
%	The trickiest term to bound is $\big\|X \tA^{-1} X^T - X A^{-1} X^T\big\|$.  To do this, we assume $(1-t)A \preceq \tA \preceq (1+t)A$.
%	\begin{eqnarray*}
%		(1-t)A &\preceq& \tA \preceq (1+t)A\\
%		\Longleftrightarrow \frac{1}{1+t}A^{-1} &\preceq& \tA^{-1} \preceq \frac{1}{1-t}A^{-1} \\
%		\Longrightarrow \frac{1}{1+t} X A^{-1}X^T &\preceq& X \tA^{-1}X^T  \preceq \frac{1}{1-t}X A^{-1}X^T \\
%		\Longleftrightarrow \Big(\frac{1}{1+t} - 1\Big)X A^{-1}X^T &\preceq& X \tA^{-1}X^T - XA^{-1}X^T \preceq \Big(\frac{1}{1-t}-1\Big)X A^{-1}X^T \\
%		\Longleftrightarrow \frac{-t}{1+t}X A^{-1}X^T &\preceq& X \tA^{-1}X^T - XA^{-1}X^T \preceq \frac{t}{1-t}X A^{-1}X^T \\
%		\Longrightarrow \frac{-t}{1+t}\|XA^{-1}X^T\| I_n &\preceq&  X \tA^{-1}X^T - XA^{-1}X^T \preceq \frac{t}{1-t}\|X A^{-1}X^T\|I_n \\
%		\Longrightarrow  \|X \tA^{-1}X^T - XA^{-1}X^T\| &\leq&  \max\Big(\frac{t}{1+t},\frac{t}{1-t}\Big)\|X A^{-1}X^T\|\\
%		&\leq&  \frac{t}{1-t}\|X\|^2 \|A^{-1}\|\\
%		\Longrightarrow  \|X \tA^{-1}X^T - XA^{-1}X^T\| &\leq&  \frac{t}{1-t}\cdot \frac{n}{\sigma_d+\lambda}
%	\end{eqnarray*}
%	
%	Combining all the above, we see that if $(1-t)A \preceq \tA \preceq (1+t)A$, then
%	\begin{eqnarray*}
%		r(\tX) - r(X) &\leq& \bigg(\|C\| \|\tA^{-1}\|\Big(2\|X\| + \|C\|\Big) + \big\|X \tA^{-1} X^T - X A^{-1} X^T\big\|\bigg)\cdot \big(\|y\| + \|\eps\|\big) \\
%		&\leq& \bigg(\frac{2\sqrt{n}}{2^b-1}\cdot \frac{1}{\tsigma_d + \lambda}\Big(2\sqrt{n} +\frac{2\sqrt{n}}{2^b-1}\Big) +\frac{t}{1-t}\cdot \frac{n}{\sigma_d+\lambda}\bigg)\cdot \big(\|y\| + a\sqrt{n}\big) \\
%		&\leq& \bigg(\frac{2\sqrt{n}}{2^b-1}\cdot \frac{1}{\tsigma_d + \lambda}\cdot 4\sqrt{n} +\frac{t}{1-t}\cdot \frac{n}{\sigma_d+\lambda}\bigg)\cdot \big(\|y\| + a\sqrt{n}\big) \\
%		&=& \bigg(\frac{8n}{(2^b-1)(\tsigma_d + \lambda)} +\frac{t}{1-t}\cdot \frac{n}{\sigma_d+\lambda}\bigg)\cdot \big(\|y\| + a\sqrt{n}\big) 
%	\end{eqnarray*}
%	
%	This (basically) completes steps 1 and 2, though there is one final nuance:
%	\begin{itemize}
%		\item The upper bound includes the term $\tsigma_d$, which is random as it is the smallest eigenvalue of $\tX^T\tX$. There are two options for dealing with this: (1) simple use the fact that $\tsigma_d \geq 0$. (2) Use matrix concentration results to upper bound the probability of $\tsigma_d$ being below some value \textbf{TODO: DEAL WITH THIS}.
%	\end{itemize}
%	
%	Now, we proceed to lower bound the probability that $(1-t)A \preceq \tA \preceq (1+t)A$.  This proof will be similar to the proof of Theorem~\ref{thm2}. We will once again use the matrix Bernstein inequality. In a manner analogous to the proof of Theorem ~\ref{thm2}, we conjugate the desired inequality with $R \defeq (X^T X + \lambda I_d)^{-1/2} = A^{-1/2}$ noting that semidefinite ordering is preserved by conjugation:
%	\begin{align*}
%	&(1 - t) A \preceq \tA \preceq (1 + t) A \\
%	\iff\ &(1 - t) I_d \preceq R \tA R \preceq (1 + t) I_d \\
%	\iff\ &-t I_d \preceq R \tA R - I_d \preceq t I_d \\
%	\iff\ &-t I_d \preceq R (\tX^T \tX + \lambda I_d - X^T X - \lambda I_d) R \preceq t I_d \\
%	\iff\ &-t I_d \preceq R (\tX^T \tX - X^T X) R \preceq t I_d.
%	\end{align*}
%	
%	Letting $D=\expect{}{C^TC}$, by Lemma~\ref{lem2} we have that 
%	$$-\Big(t - \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\Big) I_d \preceq R (\tX^T \tX - X^T X - D) R \preceq
%	\Big(t - \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\Big) I_d 
%	\Longrightarrow -t I_d \preceq R (\tX^T \tX - X^T X) R \preceq
%	t I_d$$
%	and thus
%	$$\Prob\Big[ \|R (\tX^T \tX - X^T X) R\| \geq t\Big] \leq  \Prob\bigg[ \|R (\tX^T \tX - X^T X -D) R\| \geq t-\frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\bigg]  $$
%	
%	We thus proceed to bound the RHS of this inequality, using the matrix Bernstein inequality (Theorem 6.1.1 in Tropp).  We let $S_k = R\Big((x_k + c_k)(x_k + c_k)^T  - x_k x_k^T - D_k\Big)R$, with $R=(X^T X+\lambda I_d)^{-1/2}$ and $D_k = \expect{}{c_kc_k^T}$. Importantly, unlike in Lemma~\ref{upper_bounds}, here $x_k$ and $c_k$ will be vectors in $\RR^d$ representing the $k^{th}$ word embedding and its corresponding quantization noise (\textbf{apologies for overloading notation!}).  As we show in Lemma~\ref{upper_bounds2}
%	It follows that
%	$$\|S_k\| \leq \frac{9}{(2^b-1)(\sigma_d + \lambda)} \eqdef L,$$
%	and
%	$$\Big\|\sum_{k=1}^n \expect{}{S_k^2}\Big\| \leq nL^2.$$
%	
%	We note that $Z\defeq \sum_{k=1}^n S_k = R\Big(\tX^T \tX - X^T X - D\Big)R$.  We now use matrix Bernstein to conclude that $\forall t \geq \frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}$, 
%	\begin{eqnarray*}
%		\Prob\Big[ (1 - t) A \preceq \tA \preceq (1 + t) A\Big] &=& 
%		1- \Prob\Big[\big\|R (\tX^T \tX - X^T X) R\big\| \geq t\Big]\\ 
%		&\geq& 1- \Prob\bigg[ \big\|R (\tX^T \tX - X^T X -D) R\big\| \geq t-\frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\bigg] \\
%		&\geq& 1- 2d \exp \left(\frac{-\Big(t-\frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\Big)^2}{2nL^2 + (2L/3)\Big(t-\frac{n/d}{(2^b-1)^2(\sigma_d + \lambda)}\Big)} \right)
%	\end{eqnarray*}
%\end{proof}
