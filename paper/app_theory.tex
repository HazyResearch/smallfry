\subsection{NEW THEORY}
Assume $K = USU^T \in \RR^{n \times n}$ (rank $d$), $\tK = VRV^T\in \RR^{n \times n}$ (rank $k$).
Let $\{U_1,\ldots,U_n\}$ be the eigenvectors of $K$ in decreasing eigenvalue order, with the first $d$ having non-zero eigenvalue.
Similarly, let $\{V_1,\ldots,V_n\}$ be the eigenvectors of $\tK$ in decreasing eigenvalue order, with the first $k$ having non-zero eigenvalue.
Let $U = [U_1,\ldots U_d]$, $U_{\perp} = [U_{d+1},\ldots,U_n]$, $V = [V_1,\ldots V_k]$, $V_{\perp} = [V_{k+1},\ldots,V_n]$.

%\subsubsection{VERSION 1}
%\begin{eqnarray*}
%	R(K) &=& \frac{\lambda^2}{n}y^T (K+\lambda I)^{-2} y + \frac{\sigma^2}{n} \tr\bigg(K^2(K+\lambda I)^{-2}\bigg) \\
%	n\cdot R(K) &=& \sum_{i=1}^d \Big(\frac{\lambda}{\sigma_i + \lambda}\Big)^2(U_i^T y)^2 + \sum_{i=d+1}^n (U_i^T y)^2 + \sigma^2 \sum_{i=1}^d \Big(\frac{\sigma_i}{\sigma_i + \lambda}\Big)^2 \\
%\end{eqnarray*}
%If we assume that we are in the noiseless linear regression setting ($\sigma=0$, $\lambda=0$), instead of the noisy linear ridge regression setting, this simplifies:
%\begin{eqnarray*}
%	n\cdot R(K) &=& \sum_{i=d+1}^n (U_i^T y)^2 \\
%	&=& y^T U_{\perp} U_{\perp}^T y \\
%	&=& y^T (I - UU^T) y \\
%	&=& \|y\|^2 - y^T UU^T y \\
%	n\cdot R(\tK) &=& \|y\|^2 - y^T VV^T y \\
%	n(R(\tK) - R(K)) &=& \Big(\|y\|^2 - y^T VV^T y\Big) - \Big(\|y\|^2 - y^T UU^T y\Big)\\
%	&=& y^T (UU^T - VV^T) y \\
%	&\leq& \|y\|^2 \|UU^T - VV^T\|_2 \\
%	&\leq& \|y\|^2 \|UU^T - VV^T\|_F \\
%	&\leq& \|y\|^2 \sqrt{d + k - 2 \|U^T V\|_F^2} \quad \text{(by Lemma\ref{lemma1})}\\
%\end{eqnarray*}
%\textbf{PROBLEM}: (1) This bound is only meaningful when $\|UU^T - VV^T\|_2 < 1$, which can only happen if $k=d$. (2) The final Frobenius bound is only meaningful when  $\sqrt{d + k - 2 \|U^T V\|_F^2} < 1$, which only happens if $k=d$ and $\|U^T V\|_F^2 \approx d$.
%This is the same problem we were having with $\Delta_1$,$\Delta_2$ bounds.
%If $y \in nullspace(V^T) \cap span(U)$ (which is always non-empty if $k<d$), then learning with $\tK$ can perform arbitrarily poorly relative to learning with $K$.

%\subsubsection{VERSION 2}
%Here we consider a random $y \in \RR^n$ vector with identity covariance.
%\begin{eqnarray*}
%	\expect{y}{n(R(\tK) - R(K))} &=& \expect{y}{\Big(\|y\|^2 - y^T VV^T y\Big) - \Big(\|y\|^2 - y^T UU^T y\Big)}\\
%	&=& \expect{y}{y^T (UU^T - VV^T) y} \\
%	&=& \expect{y}{y^T UU^T y - y^TVV^T y} \\
%	&=&  \|U\|_F^2  -  \|V\|_F^2 \\
%	&=& d-k \\
%	&\leq& d  -  \|U^T V\|_F^2 \\
%	\|V\|_F^2 &=& \|[U,U_{\perp}]^T V\|_F^2 \quad \text{(because $[U,U_{\perp}]$ is an orthogonal projection).} \\
%	&=& \|U^T V\|_F^2 + \|U_{\perp}^T V\|_F^2 \\
%	&\geq& \|U^T V\|_F^2 \\
%%	OLD && \\
%%	&=& \|U\|_F^2  - \expect{y}{y^TVV^T y} \\
%%	&\leq& d - \|V^T U\|_F^2 \quad \text{by derivation below.} \\
%%	\expect{y}{n(R(K) - R(\tK))} &\leq& k - \|V^T U\|_F^2 \\
%%	\expect{y}{y^T VV^T y} &=& \expect{y}{\|V^T y\|_2^2} \\
%%	&=& \expect{y}{\|VV^T y\|_2^2} \\
%%	&\geq&  \expect{y}{\|VV^T UU^T y\|_2^2}  \quad\text{(projecting onto $U$ first makes $y$ magnitude smaller)}\\
%%	&=& \|VV^T UU^T\|_F^2 \quad \text{(by Lemma\ref{lemma2})}\\
%%	&=& \tr\Big(UU^TVV^TVV^TUU^T \Big) \\
%%	&=& \tr\Big((U^TV) V^TV (V^TU) U^TU \Big) \\
%%	&=& \tr \Big( (V^T U)^T (V^T U)\Big) \\
%%	&=& \|V^T U\|_F^2
%\end{eqnarray*}
%\textbf{PROBLEM}: I think this bound is pretty meaningless, since $\expect{y}{n(R(\tK) - R(K))}$ is always exactly equal to $d-k$ if $y$ is random vector with identity covariance.

%\subsubsection{VERSION 3}
%\begin{eqnarray*}
%	n(R(\tK) - R(K)) &=& \Big(\|y\|^2 - y^T VV^T y\Big) - \Big(\|y\|^2 - y^T UU^T y\Big)\\
%	&=& y^T (UU^T - VV^T) y \\
%	&=& y^T UU^T y - y^TVV^T y \\
%	&\leq& \text{add line here.} \\
%	y^T VV^T y &=& \|V^T y\|_2^2 \\
%	&=& \|VV^T y\|_2^2 \\
%	&\geq&  \|VV^T UU^T y\|_2^2  \quad\text{(projecting onto $U$ first makes $y$ magnitude smaller)}\\
%	&=&  \|(V^T U) (U^T y)\|_2^2 \\
%	&\geq& \sigma_{\min}(V^T U)^2 \|U^T y\|^2 \\
%	&=& \tr\Big(y^T UU^TVV^TVV^TUU^T y \Big) \\
%	&=& \tr\Big(UU^TVV^TVV^TUU^T yy^T \Big) \\
%	&=& \tr\Big(U (U^TV) (V^TU) U^T yy^T \Big) \\
%	&=& \tr\Big((U^TV) (V^TU) (U^T y)(U^T y)^T \Big) \\
%	&\geq& \text{NOT SURE HOW TO LOWER BOUND}
%	&=& \tr\Big((U^TV) V^TV (V^TU) U^TU \Big) \\
%	&=& \tr \Big( (V^T U)^T (V^T U)\Big) \\
%	&=& \|V^T U\|_F^2
%\end{eqnarray*}
%So the worst case analysis cares about the minimum singular value of $V^T U$. \todo{But this will be 0 if rank of $V$ is smaller than rank of $U$, right??}

%\subsubsection{VERSION 3}
%Tri says we can use the fact that the singular values of $UU^T-VV^T$ are the same as those of $U^{\perp} U^{\perp T} VV^T$ (but repeated twice).  And then use sin(theta) theorem to argue that $\|U^{\perp} U^{\perp T} VV^T\|$ is small

\subsubsection{Average case analysis (identity covariance matrix)}
We will assume $y$ is a random vector in the span of $U$.
Specifically, $y = \sum_{i=1}^d z_i U_i$, where the $z_i$ are i.i.d. zero mean random variable with variance 1.
\begin{eqnarray*}
	\expect{y}{\|U^T y\|_2^2} &=& \expect{y}{\sum_{j=1}^d \bigg( U_j^T \Big(\sum_{i=1}^d z_i U_i\Big)\bigg)^2} \\
	&=& \expect{y}{\sum_{j=1}^d z_j^2} \\
	&=& d \\
	\expect{y}{\|V^T y\|_2^2} &=& \expect{y}{\sum_{j=1}^k \bigg( V_j^T \Big(\sum_{i=1}^d z_i U_i\Big)\bigg)^2} \\
	&=& \expect{y}{\sum_{j=1}^k \bigg(\sum_{i=1}^d z_i V_j^TU_i\bigg)^2} \\
	&=& \expect{y}{\sum_{j=1}^k \sum_{i,l=1}^d (z_i V_j^TU_i) (z_l V_j^TU_l) }\\
	&=& \sum_{j=1}^k \sum_{i,l=1}^d \expect{y}{z_i z_l} (V_j^TU_i) (V_j^TU_l) \\
	&=& \sum_{j=1}^k \sum_{i=1}^d (V_j^TU_i)^2 \\
	&=& \|V^T U\|_F^2\\
	\expect{y}{n(R(\tK) - R(K))} &=& \expect{y}{\Big(\|y\|^2 - y^T VV^T y\Big) - \Big(\|y\|^2 - y^T UU^T y\Big)}\\
	&=& \expect{y}{\|U^T y\|^2 - \|V^Ty\|^2}\\
	&=& d -  \|V^T U\|_F^2\\
\end{eqnarray*}

\subsubsection{Average case analysis (diagonal covariance matrix)}
We will assume $y$ is a random vector in the span of $U$.
Specifically, $y = \sum_{i=1}^d z_i U_i$, where the $z_i$ are independent random variables satisfying $\expect{}{z_i} = 0$ and $\var{}{z_i} = \sigma_i^2$.
We let $\Sigma = \diag(\sigma_1,\ldots,\sigma_d)$.
\begin{eqnarray*}
	\expect{y}{\|U^T y\|_2^2} &=& \expect{y}{\sum_{j=1}^d \bigg( U_j^T \Big(\sum_{i=1}^d z_i U_i\Big)\bigg)^2} \\
	&=& \expect{y}{\sum_{j=1}^d z_j^2} \\
	&=& \sum_{i=1}^d \sigma_i^2 \\
	&=& \|\Sigma\|_F^2 \\
	\expect{y}{\|V^T y\|_2^2} &=& \expect{y}{\sum_{j=1}^k \bigg( V_j^T \Big(\sum_{i=1}^d z_i U_i\Big)\bigg)^2} \\
	&=& \expect{y}{\sum_{j=1}^k \bigg(\sum_{i=1}^d z_i V_j^TU_i\bigg)^2} \\
	&=& \expect{y}{\sum_{j=1}^k \sum_{i,l=1}^d (z_i V_j^TU_i) (z_l V_j^TU_l) }\\
	&=& \sum_{j=1}^k \sum_{i,l=1}^d \expect{y}{z_i z_l} (V_j^TU_i) (V_j^TU_l) \\
	&=& \sum_{j=1}^k \sum_{i=1}^d \sigma_i^2 (V_j^TU_i)^2 \\
	&=& \|V^T U \Sigma\|_F^2\\
	\expect{y}{n(R(\tK) - R(K))} &=& \expect{y}{\Big(\|y\|^2 - y^T VV^T y\Big) - \Big(\|y\|^2 - y^T UU^T y\Big)}\\
	&=& \expect{y}{\|U^T y\|^2 - \|V^Ty\|^2}\\
	&=& \|\Sigma\|_F^2 -  \|V^T U\Sigma\|_F^2\\
	&\geq& \|\Sigma\|_F^2 -  \|V^T U\|_F^2 \|\Sigma\|_2^2\\
%	&\geq& \|\Sigma\|_F^2(1 -  \|V^T U\|_F^2)\\
\end{eqnarray*}

\subsubsection{Davis-Kahan $\sin(\Theta)$ Theorem}
The Davis-Kahan $\sin(Theta)$ Theorem (adapted to bour setting) states the following:
\begin{theorem}{(Davis-Kahan $\sin(\Theta)$ Theorem (adapted))}
Let $K=U_0 S_0 U_0^T + U_1 S_1 U_1^T$ be the eigendecomposition of $K$ such that $U_0 \in \RR^{n \times d}$ are the first $d$ eigenvectors of $K=USU^T$, $S_0$ the first $d$ eigenvalues, $U_1,S_1$ the rest.
Similarly, let $\tK = V_0 R_0 V_0^T + V_1 R_1 V_1^T$ be the equivalent eigendecomposition for $\tk$, for $\tK = K + H$.
If the eigenvalues of $S_0$ are contained in the interval $(a_0,a_1)$, and the eigenvalues of $R_1$ are excluded from the interval $(a_0 - \delta,a_1 +\delta)$ for some $\delta>0$, then
\begin{eqnarray}
\|V_1^T U_0\| \leq \frac{\|V_1^T H U_0\|}{\delta}
\end{eqnarray}
for any unitarily invariant norm $\|\cdot \|$.
\end{theorem}

(This theorem reuses the notation above)
\begin{theorem}
If both $K$ and $\tK$ are rank $d$ matrices, then
\begin{eqnarray*}
\|V_0^T U_0\|^2 &\geq& d - \Bigg(\frac{2\sqrt{n}}{2^b-1} \cdot \frac{2\sqrt{\sigma_{\max}(K)} + \frac{2\sqrt{n}}{2^b-1} }{\sigma_{\min}(K)} \Bigg)^2
\end{eqnarray*}
\end{theorem}
\begin{proof}
We will apply this theorem to the setting $K = XX^T = USU^T$, $\tK = (X+C)(X+C)^T = VRV^T$,
for $X \in \RR^{n \times d}$, $X_{ij}\in [-\frac{1}{\sqrt{d}},\frac{1}{\sqrt{d}}]$,
where $\expect{}{C_{ij}} = 0$, $\expect{}{C_{ij}^2} \leq \frac{1}{d(2^b-1)^2} \defeq \delta_b^2/d$.
In this setting, $a_0 = \sigma_{\min}(K)$, $a_1 = \infty$, $\delta=\sigma_{\min}(K)$.
Note also the $H = \tK-K = (X+C)(X+C)^T - XX^T = XC^T + CX^T + CC^T$.

Using the $\sin(\Theta)$ Theorem, we can show the following:

\begin{eqnarray*}
\|V_1^T U_0\|_F
&\leq& \frac{\|V_1^T H U_0\|_F}{\sigma_{\min}(K)}\\
&=& \frac{\|V_1^T (XC^T + CX^T + CC^T) U_0\|_F}{\sigma_{\min}(K)}\\
&\leq& \frac{\|V_1^T\|_2 \|XC^T + CX^T + CC^T\|_F \|U_0\|_2}{\sigma_{\min}(K)} \quad \text{(using $\|AB\|_F \leq \|A\|_2 \|B\|_F$ twice.)}\\
&\leq& \frac{ \|XC^T + CX^T + CC^T\|_F}{\sigma_{\min}(K)} \quad \text{(using $\|V_1^T\|_2 = \|U_0\|_2 = 1$.)}\\
&\leq& \frac{\|XC^T\|_F + \|CX^T\|_F + \|CC^T\|_F}{\sigma_{\min}(K)} \\
&\leq& \frac{2\|X\|_2\|C\|_F + \|C\|_2 \|C\|_F}{\sigma_{\min}(K)} \quad \text{(using $\|AB\|_F \leq \|A\|_2 \|B\|_F$)}\\
&=& \frac{2\sigma_{\max}(X)\|C\|_F + \|C\|_2 \|C\|_F}{\sigma_{\min}(K)} \\
&\leq& \|C\|_F \cdot \frac{2\sqrt{\sigma_{\max}(K)} + \|C\|_F }{\sigma_{\min}(K)}  \quad \text{(loosened $\|C\|_2$ to  $\|C\|_F$.)}\\
&\leq& \frac{2\sqrt{n}}{2^b-1} \cdot \frac{2\sqrt{\sigma_{\max}(K)} + \frac{2\sqrt{n}}{2^b-1} }{\sigma_{\min}(K)} \quad \text{(using bound on $\|C\|_F$ shown below.)} \\
\end{eqnarray*}

Here we use the fact that $|C_{ij}| \leq \frac{2}{\sqrt{d}(2^b-1)}$ to bound $\|C\|_F$.
\begin{eqnarray*}
\|C\|_F &\leq& \sqrt{nd \cdot \frac{4}{d(2^b-1)^2}} \\
&=& \frac{2\sqrt{n}}{2^b-1} \\
\end{eqnarray*}
%\todo{Can we get a tighter bound? Perhaps by bounding $\|C\|_2$ with high probability, instead of bounding the $\|C\|_F$ in the numerator?}

To complete the theorem, we simply notice that:
\begin{eqnarray}
d &=& \|U_0\|_F^2 \\
&=&\|[V_0,V_1]^T U_0\|_F^2 \\
&=& \|V_0^T U_0\|_F^2 + \|V_1^T U_0\|_F^2\\
\Longrightarrow\quad  \|V_0^T U_0\|_F^2 &=& d - \|V_1^T U_0\|_F^2 \\
&\geq& d - \Bigg(\frac{2\sqrt{n}}{2^b-1} \cdot \frac{2\sqrt{\sigma_{\max}(K)} + \frac{2\sqrt{n}}{2^b-1} }{\sigma_{\min}(K)} \Bigg)^2 \\
&=& d - \frac{16n}{(2^b-1)^2} \Bigg( \frac{\sqrt{\sigma_{\max}(K)} + \frac{\sqrt{n}}{2^b-1} }{\sigma_{\min}(K)} \Bigg)^2
\end{eqnarray}
\end{proof}

\subsubsection{Davis-Kahan $\sin(\Theta)$ Theorem: VERSION 2}
We start as we did in the previous section: 

\begin{eqnarray*}
	\|V_1^T U_0\|_F
	&\leq& \frac{\|V_1^T H U_0\|_F}{\sigma_{\min}(K)}\\
	&=& \frac{\|V_1^T (XC^T + CX^T + CC^T) U_0\|_F}{\sigma_{\min}(K)}\\
	&\leq& \frac{\|V_1^T\|_2 \|XC^T + CX^T + CC^T\|_2 \|U_0\|_F}{\sigma_{\min}(K)} \quad \text{(using $\|AB\|_F \leq \|A\|_2 \|B\|_F$ twice.)}\\
	&=& \frac{\sqrt{d}\cdot \|XC^T + CX^T + CC^T\|_2}{\sigma_{\min}(K)} \quad \text{(using $\|V_1^T\|_2 = 1$, $\|U_0\|_F = \sqrt{d}$.)}\\
	&\leq& \frac{\sqrt{d}\Big(\|XC^T\|_2 + \|CX^T\|_2 + \|CC^T\|_2\Big)}{\sigma_{\min}(K)} \quad \text{(using $\|V_1^T\|_2 = 1$, $\|U_0\|_F = \sqrt{d}$.)}\\
\end{eqnarray*}
Now we will consider the expected value of both sides.  This requires computing the expected value of $\|XC^T\|_2$, $\|CX^T\|_2$, and $\|CC^T\|_2$.  Toward this end, we leverage the Matrix Bernstein Theorem (Theorem 6.1.1 in \citep{tropp15}).

We first note that $0 \preceq CC^T \preceq \delta_b^2 I$, and thus $\expect{}{\|CC^T\|_2} \leq \delta_b^2$.
We now proceed to bound $\expect{}{\|XC^T\|_2} = \expect{}{\|CX^T\|_2} \eqdef B$.

Using the notation from the Matrix Bernstein Theorem, we will let $S_k = x_k c_k^T$, where $x_k$ and $c_k$ are the $k^{th}$ columns of $X$ and $C$ respectively.
We note that $|x_{ki}| \leq \frac{1}{\sqrt{d}}$, $|c_{ki}| \leq \frac{2}{\sqrt{d}(2^b-1)}$, and $\expect{}{c_{ki}^2} \leq \frac{1}{d(2^b-1)^2} = \delta_b^2/d$, for $\delta_b^2 = \frac{1}{(2^b-1)^2}$.
It is easy to see that $\expect{}{S_k} = 0$ because $x_k$ is fixed and $c_k$ is random and zero mean.
We now upper bound $\|S_k\|_2$.
\begin{eqnarray*}
\|S_k\|_2 &=& \|x_k c_k^T\|_2 \\
&\leq&  \|x_k\|_2 \|c_k^T\|_2 \\
&\leq& \sqrt{n \cdot \frac{1}{d}} \cdot \sqrt{n \cdot \frac{4}{d(2^b-1)^2}}\\
&=& \frac{2n}{d(2^b-1)}\\
&\eqdef& L\\
\end{eqnarray*}

Now, I must upper bound $\max\Big(\|\sum_{k=1}^d \expect{}{S_k S_k^T}\|, \|\sum_{k=1}^d \expect{}{S_k^T S_k}\|\Big)$.

We do these one at a time:
\begin{eqnarray*}
\Big\|\sum_{k=1}^d \expect{}{S_k S_k^T}\Big\| &=& \Big\|\sum_{k=1}^d \expect{}{x_k c_k^T c_k x_k^T }\Big\|\\
&=& \Big\|\sum_{k=1}^d x_k \expect{}{c_k^T c_k} x_k^T \Big\| \\
&\leq& \frac{n\delta_b^2}{d}\Big\|\sum_{k=1}^d x_k x_k^T \Big\| \\
&=&\frac{n\delta_b^2}{d}\Big\|XX^T \Big\| \\
&=&\frac{n\delta_b^2}{d}\|X\|_2^2 \\
\end{eqnarray*}
\begin{eqnarray*}
	\Big\|\sum_{k=1}^d \expect{}{S_k^T S_k}\Big\| &=& \Big\|\sum_{k=1}^d \expect{}{c_k x_k^T x_k c_k^T}\Big\|\\
	&=& \Big\|\sum_{k=1}^d \|x_k\|^2 \expect{}{c_k c_k^T}\Big\| \\
	&\leq& \Big\|\sum_{k=1}^d \|x_k\|^2 \frac{\delta_b^2}{d}I\Big\| \\
	&\leq& \frac{\delta_b^2}{d} \sum_{k=1}^d \|x_k\|^2 \\
	&=& \frac{\delta_b^2}{d} \|X\|_F^2 \\
\end{eqnarray*}


\subsubsection{Davis-Kahan $\sin(\Theta)$ Theorem: VERSION 3}
We begin in the middle of the first derivation (see above).
We will let $a\in [0,1]$ be the scalar such that $\sigma_{\min}(X) = a \sqrt{\frac{n}{d}}$ (equivalently, $\sigma_{\min}(K) = a^2 \frac{n}{d}$).
\begin{eqnarray*}
	\|V_1^T U_0\|_F	&\leq& \frac{ \|XC^T + CX^T + CC^T\|_F}{\sigma_{\min}(K)} \\
	\Longrightarrow \frac{1}{d}	\|V_1^T U_0\|_F^2 &\leq& \frac{ \|XC^T + CX^T + CC^T\|_F^2}{d \cdot \sigma_{\min}(K)^2} \\
	\Longleftrightarrow 1-\frac{1}{d}\|V_0^T U_0\|_F^2 &\leq&  \frac{ \|XC^T + CX^T + CC^T\|_F^2}{d \cdot \sigma_{\min}(K)^2} \\
	\Longleftrightarrow 1-\eigover(X,\tX) &\leq&  \frac{ \|XC^T + CX^T + CC^T\|_F^2}{d \cdot \sigma_{\min}(K)^2} \\
	\Longrightarrow \expect{}{1-\eigover(X,\tX)} &\leq&  \expect{}{\frac{ \|XC^T + CX^T + CC^T\|_F^2}{d \cdot \sigma_{\min}(K)^2}} \\
	&=&  \frac{ \expect{}{\|XC^T + CX^T + CC^T\|_F^2}}{d \cdot \sigma_{\min}(K)^2}\\
	&\leq& \frac{\frac{20n^2\delta_b^2}{d}}{d \cdot \sigma_{\min}(K)^2} \quad \text{(by Lemma~\ref{lem:expect_h}).} \\
	&=& \frac{\frac{20n^2\delta_b^2}{d}}{d a^4 (n^2/d^2)} \\
	&=& \frac{20 \delta_b^2}{a^4}
\end{eqnarray*}

\begin{corollary}
$\expect{}{1-\eigover(X,\tX)} \leq \eps$ if $b \geq \log_2\bigg(\frac{\sqrt{20}}{a^2\sqrt{\eps}}+1\bigg)$.
\end{corollary}
\begin{proof}
	We set $\frac{20 \delta_b^2}{a^4} = \eps$ and solve for $b$, using $\delta_b^2 = \frac{1}{(2^b-1)^2}$.
\begin{eqnarray*}
	\frac{20 \delta_b^2}{a^4} &=& \eps \\
	\frac{20}{a^4(2^b-1)^2} &=& \eps \\
	\frac{20}{a^4\eps} &=& (2^b-1)^2 \\
	\frac{\sqrt{20}}{a^2\sqrt{\eps}} &=& 2^b-1 \\
	\log_2\bigg(\frac{\sqrt{20}}{a^2\sqrt{\eps}} + 1\bigg) &=& b \\
\end{eqnarray*}
\end{proof}

We now present and prove Lemma~\ref{lem:expect_h}.

\begin{lemma}
Letting $\tX = X+C$ denote the uniformly quantized embedding matrix, it follows that:
\begin{eqnarray}
\expect{}{\|XC^T + CX^T + CC^T\|_F^2} &\leq& \frac{20n^2\delta_b^2}{d}
\end{eqnarray}
\label{lem:expect_h}
\end{lemma}
\begin{proof}
We will let $H \defeq XC^T + CX^T + CC^T$.
To bound $\expect{}{\|H\|_F^2} = \sum_{i,j=1}^n \expect{}{H_{ij}^2}$, we will consider two cases: $H_{ij}$ for $i\neq j$ and $H_{ij}$ for $i=j$.
We will let $x_i, c_i \in \RR^d$ denote the $i^{th}$ rows of $X$ and $C$ respectively.\\
\begin{enumerate}
\item \textbf{Case 1: $i\neq j$}\\
\begin{eqnarray*}
	\expect{}{H_{ij}^2} &=& \expect{}{(x_i^T c_j + c_i^T x_j + c_i^T c_j)^2} \\
	&=& \expect{}{(x_i^T c_j)^2 + (c_i^T x_j)^2 + (c_i^T c_j)^2} \\
	&=& \expect{}{\Big(\sum_{k=1}^d x_{ik} c_{jk}\Big)^2} + \expect{}{\Big(\sum_{k=1}^d c_{ik} x_{jk}\Big)^2} + \expect{}{\Big(\sum_{k=1}^d c_{ik} c_{jk}\Big)^2} \\
	&=& \expect{}{\sum_{k=1}^d x_{ik}^2 c_{jk}^2} + \expect{}{\sum_{k=1}^d c_{ik}^2 x_{jk}^2} + \expect{}{\sum_{k=1}^d c_{ik}^2 c_{jk}^2}\\
	&=& \sum_{k=1}^d x_{ik}^2 \expect{}{c_{jk}^2} + \sum_{k=1}^d \expect{}{c_{ik}^2} x_{jk}^2 + \sum_{k=1}^d \expect{}{c_{ik}^2} \expect{}{c_{jk}^2}\\
	&\leq& \frac{\delta_b^2}{d}\cdot \sum_{k=1}^d x_{ik}^2 + \frac{\delta_b^2}{d}\cdot \sum_{k=1}^d x_{jk}^2 + \sum_{k=1}^d \Big(\frac{\delta_b^2}{d}\Big)^2 \\
	&\leq& \frac{\delta_b^2}{d}\cdot \|x_i\|^2 + \frac{\delta_b^2}{d}\cdot \|x_j\|^2 + \sum_{k=1}^d \Big(\frac{\delta_b^2}{d}\Big)^2 \\
	&\leq& \frac{2\delta_b^2 + \delta_b^4}{d} \quad \text{(using $\|x_i\|^2 \leq 1$.)} \\
	&\leq& \frac{3\delta_b^2}{d} \quad \text{(using $\delta_b \leq 1$.)} \\
\end{eqnarray*}
\item \textbf{Case 2: $i = j$}\\
\begin{eqnarray*}
	\expect{}{H_{ii}^2} &=& \expect{}{(x_i^T c_i + c_i^T x_i + c_i^T c_i)^2} \\
	&=& \expect{}{\Big(2 \sum_{k=1}^d x_{ik} c_{ik}  + \sum_{l=1}^d c_{il}^2 \Big)^2} \\
	&=& \expect{}{4 \Big(\sum_{k=1}^d x_{ik} c_{ik}\Big)^2  + 4 \Big(\sum_{k=1}^d x_{ik} c_{ik}\Big)\cdot \Big(\sum_{l=1}^d c_{il}^2 \Big) + \Big(\sum_{l=1}^d c_{il}^2 \Big)^2} \\
	&=& \expect{}{4 \sum_{k=1}^d x_{ik}^2 c_{ik}^2  + 4 \sum_{k,l=1}^d x_{ik} c_{ik} c_{il}^2  + \sum_{k,l=1}^d c_{il}^2 c_{ik}^2 } \\
	&=& 4 \sum_{k=1}^d x_{ik}^2 \expect{}{c_{ik}^2}  + 4 \sum_{k=1}^d x_{ik} \expect{}{c_{ik}^3}  + \sum_{k,l=1}^d \expect{}{c_{il}^2 c_{ik}^2} \\
	&\leq& 4\cdot \frac{\delta_b^2}{d}\cdot \sum_{k=1}^d x_{ik}^2  + 4 \sum_{k=1}^d \frac{1}{\sqrt{d}} \bigg(\frac{2}{\sqrt{d}(2^b-1)}\bigg)^3  + \sum_{k,l=1}^d \bigg(\frac{2}{\sqrt{d}(2^b-1)}\bigg)^4 \\
	&=& 4\cdot \frac{\delta_b^2}{d}\cdot \|x_i\|^2  + 4 d\cdot \frac{8}{d^2(2^b-1)^3}  + d^2 \cdot \frac{16}{d^2(2^b-1)^4} \\
	&\leq&  \frac{4\delta_b^2}{d}  + \frac{32}{d(2^b-1)^3}  + \frac{16}{(2^b-1)^4} \\
	&=& \frac{4\delta_b^2 + 32\delta_b^3}{d} + 16 \delta_b^4 \\
	&\leq& \frac{36\delta_b^2}{d} + 16 \delta_b^4  \quad \text{(using $\delta_b \leq 1$.)} \\
%	&=& \expect{}{(2 x_i^T c_i  + \|c_i\|^2)^2} \\
%	&=& \expect{}{2(x_i^T c_j)^2 + (c_i^T x_j)^2 + (c_i^T c_j)^2} \\
%	&=& \expect{}{\Big(\sum_{k=1}^d x_{ik} c_{jk}\Big)^2} + \expect{}{\Big(\sum_{k=1}^d c_{ik} x_{jk}\Big)^2} + \expect{}{\Big(\sum_{k=1}^d c_{ik} c_{jk}\Big)^2} \\
%	&=& \expect{}{\sum_{k=1}^d x_{ik}^2 c_{jk}^2} + \expect{}{\sum_{k=1}^d c_{ik}^2 x_{jk}^2} + \expect{}{\sum_{k=1}^d c_{ik}^2 c_{jk}^2}\\
%	&=& \sum_{k=1}^d x_{ik}^2 \expect{}{c_{jk}^2} + \sum_{k=1}^d \expect{}{c_{ik}^2} x_{jk}^2 + \sum_{k=1}^d \expect{}{c_{ik}^2} \expect{}{c_{jk}^2}\\
%	&\leq& \frac{\delta_b^2}{d}\cdot \sum_{k=1}^d x_{ik}^2 + \frac{\delta_b^2}{d}\cdot \sum_{k=1}^d x_{jk}^2 + \sum_{k=1}^d \Big(\frac{\delta_b^2}{d}\Big)^2 \\
%	&\leq& \frac{\delta_b^2}{d}\cdot \|x_i\|^2 + \frac{\delta_b^2}{d}\cdot \|x_j\|^2 + d\cdot \Big(\frac{\delta_b^2}{d}\Big)^2 \\
\end{eqnarray*}
\end{enumerate}
Now we can combine the above results:
\begin{eqnarray*}
	\sum_{i,j=1}^n \expect{}{H_{ij}^2} &\leq& \sum_{i \neq j} \Bigg(\frac{3\delta_b^2}{d} \Bigg) 
	+ \sum_{i=1}^n \Bigg(\frac{36\delta_b^2}{d} + 16 \delta_b^4\Bigg) \\
	&=& n(n-1)\Bigg(\frac{3\delta_b^2}{d} \Bigg) 
	+ n\Bigg(\frac{36\delta_b^2}{d} + 16 \delta_b^4\Bigg) \\
	&=& \frac{3n^2\delta_b^2 - 3n \delta_b^2 + 36 n \delta_b^2}{d} + 16n \delta_b^4 \\
	&=& \frac{3n^2\delta_b^2 + 33 n \delta_b^2}{d} + 16n \delta_b^4 \\
	&\leq& \frac{4n^2\delta_b^2}{d} + 16n \delta_b^4  \quad \text{(assuming $n\geq 33$.)}\\
	&\leq& \frac{4n^2\delta_b^2}{d} + \frac{16n^2\delta_b^2}{d}  \quad \text{(assuming $n\geq d$.)}\\
	&=& \frac{20n^2\delta_b^2}{d}\\
\end{eqnarray*}
\end{proof}

%We note that $|x_{ki}| \leq \frac{1}{\sqrt{d}}$, $|c_{ki}| \leq \frac{2}{\sqrt{d}(2^b-1)}$, and $\expect{}{c_{ki}^2} \leq \frac{1}{d(2^b-1)^2} = \delta_b^2/d$, for $\delta_b^2 = \frac{1}{(2^b-1)^2}$.


%\begin{lemma}
%\label{lemma1}$
%\begin{eqnarray*}
%	\|UU^T - VV^T \|_F^2 &=& \tr\Big( (UU^T - VV^T)^T (UU^T - VV^T)  \Big)\\
%	&=& \tr(UU^TUU^T) + \tr(VV^T VV^T) - \tr(VV^T UU^T) - \tr(UU^T VV^T) \\
%	&=& d + k - 2 \tr((U^T V)^T (U^T V)) \\
%	&=& d + k - 2 \|U^T V\|_F^2
%\end{eqnarray*}
%\end{lemma}
%
%\begin{lemma}
%\label{lemma2}
%Let $A \in \RR^{n\times d}$ be an arbitrary matrix. If a random vector $x \in \RR^d$ is drawn from a distribution with identiy covariance matrix, then $\|A\|_F^2 = \expect{x}{\|Ax\|_2^2}$.
%\end{lemma}
%\begin{proof}
%\begin{eqnarray*}
%\expect{x}{\|Ax\|_2^2} &=& \expect{x}{(Ax)^T (Ax)} \\
%&=& \expect{x}{x^T A^T A x} \\
%&=& \expect{x}{\tr \Big(x^T A^T A x\Big)} \\
%&=& \expect{x}{\tr \Big(A^T A x x^T\Big)} \\
%&=& \tr \Big(A^T A \cdot \expect{x}{x x^T}\Big) \\
%&=& \tr \Big(A^T A\Big) \\
%&=& \|A\|_F^2 \\
%\end{eqnarray*}
%\end{proof}

%Let $\{a_i\}_{i=1}^r$ be an orthonormal basis of $U \backslash V$, $\{b_j\}_{i=1}^s$ be an orthonormal basis of $V / U$, and $\{c_k\}_{i=1}^t$ be an orthonormal basis of $U \cap V$.
%\begin{eqnarray*}
%	\|UU^T - VV^T \|_F^2 &=& \sum_{i=1}^r \|(UU^T - VV^T)a_i\|^2 + \sum_{j=1}^s \|(UU^T - VV^T)b_j\|^2 + \sum_{k=1}^t \|(UU^T - VV^T)c_k\|^2 \\
%	&=& r + s
%\end{eqnarray*}
