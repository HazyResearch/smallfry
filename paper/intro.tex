In recent years, \textit{word embeddings} \citep{word2vec13,glove14,fasttext18} have brought large improvements to a wide range of applications in natural language processing (NLP) \citep{collins16,drqa17}.
However, these word embeddings can occupy a very large amount of memory, making it expensive to deploy them in data centers, and impractical to use them in memory-constrained environments like smart phones.
While there have been numerous successful methods proposed recently for compressing embeddings \citep{sparse16,andrews16,dccl17,kway18}, these methods are generally quite computationally expensive and training them requires tuning many hyperparameters \todo{more concise?}. 
%, costing valuable computation and developer time to use.

To gain insight into how to design simpler and faster compression methods, we first study existing compression methods to better understand what properties of the compressed embeddings lead to strong downstream performance.
Surprisingly, we find that existing metrics for evaluating the quality of the compressed embeddings fail to explain their downstream performance.
For example, we observe that the reconstruction error of the embeddings, the Pairwise Inner Product (PIP) loss \citep{yin18}, and spectral measures of approximation between the pairwise inner product matrices of the embeddings \citep{avron17,lprff18} all correlate poorly with the downstream performance of the compressed embeddings.
%\todo{Should we give any intuition for why these methods fail?}
These observations suggest that we need a new metric which better captures what makes a compressed embedding perform well, and which we can then use to guide the development of new compression methods.

Toward this end, we introduce the \textit{eigenspace overlap metric};
we show that this metric correlates strongly with downstream performance across a variety of tasks, and prove generalization bounds in terms of this metric in the context of linear regression.
\todo{Defend linear regression?}
This metric measures the degree of overlap between the subspaces spanned by the eigenvectors of the pairwise inner product matrices of the compressed and uncompressed embeddings.
Intuitively, this is a natural metric because a linear regressor is only able to capture the part of the label vector contained within the span of its eigenvectors.
A consequence of this metric is that for a compressed embedding matrix to have high eigenspace overlap, it is necessary for its rank to be similar to that of the uncompressed embedding.

In an effort to design a simple and fast compression method which preserves the rank of the uncompressed embedding, we propose a compression method using uniform quantization, and show it competes with state-of-the-art embeddings on a variety of tasks.
This compression method has two parts: It first determines the optimal threshold at which to clip the extreme values in the word embedding matrix, and then uniformly quantizes the embeddings within the clipped interval.
This method has no hyperparameters, and can compress large embeddings in under a minute on a single CPU core.
Empirically, we observe that our compression method performs within \todo{XX\%}-\todo{YY\%} relative performance to the state-of-the-art deep composition code learning (DCCL) approach~\citep{dccl17}, at 32x compression ratio across four task types and two embedding types.
Theoretically, we analyze the impact of uniform quantization on the eigenspace overlap metric using matrix perturbation theory, showing that high eigenspace overlap can be attained at low-precision.
This analysis helps explain the strong performance of our method at low-precision.

The rest of this paper is organized as follows:
In Section~\ref{sec:relwork} we review related work, including existing methods for compressing word embeddings.
In Section~\ref{sec:uniform} we present our uniform quantization method for compressing embeddings.
We discuss our large-scale experiments in Section~\ref{sec:experiments}, and the theoretical analysis for our method in Section~\ref{sec:theory}.
We conclude in Section~\ref{sec:conclusion}.


% Though our generalization bounds are specific to linear regression, we believe this is an important first step toward understanding compression's impact on generalization, and hope this work inspires further analysis.
%\todo{Add sentence with more empirical details?}
%We observe that across tasks, our compression method can attain 32x compression within \todo{XX\%}-\todo{YY\%} performance relative to those attained by the state-of-the-art deep composition code learning (DCCL) approach~\citep{dccl17}.



%\todo{The rest of this paper is organized as follows...}
%1) Our goal is to compress embeddings to get good generalization performance under a memory budget.  We want something simple.
%2) To inform our design of a new simple compression method, we study other methods, and surprisingly the standard metrics fail, suggesting the need for a more refined metric.
%3) We introduce bla metric, and show it predicts empirical performance well. We also prove generalization bounds, connecting this metric directly to generalization performance. A consequence of this metric is that in order to attain large overlap, it is necessary for approximation to be high-rank.
%4) Motivated by the above connection between rank and generalization performance, we propose simple compression approach based on uniform quantization.
%>> no hyperparameters, fast training.
%>> Performs on par with the complex embeddings.
%>> We can prove its overlap is large.
