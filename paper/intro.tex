In recent years, \textit{word embeddings} \citep{word2vec13,glove14,fasttext18} have brought large improvements to a wide range of applications in natural language processing (NLP) \citep{collins16,drqa17}.
However, these word embeddings can occupy a very large amount of memory, making it expensive to deploy them in data centers, and impractical to use them in memory-constrained environments like smart phones.
While there have been numerous successful methods proposed recently for compressing embeddings \citep{sparse16,andrews16,dccl17,kway18}, these methods are generally quite computationally expensive and training them requires tuning many hyperparameters.%, costing valuable computation and developer time to use.

To gain insight into how to design simpler and faster compression methods, we first study existing compression methods to better understand what properties of the compressed embeddings lead to strong downstream performance.
Surprisingly, we find that existing metrics for evaluating the quality of the compressed embeddings fail to explain their downstream performance.
For example, we observe that the reconstruction error of the embeddings, the Pairwise Inner Product (PIP) loss \citep{yin18}, and spectral measures of approximation between the pairwise inner product matrices of the embeddings \citep{avron17,lprff18} all correlate poorly with the downstream performance of the compressed embeddings.
\todo{Should we give any intuition for why these methods fail?}
These observations suggest that we need a new metric which better captures what makes a compressed embedding perform well, and which we can then use to guide the development of new compression methods.

Toward this end, we introduce the \textit{eigenspace overlap metric};
we show that this metric correlates strongly with downstream performance across a variety of tasks, and prove generalization bounds in terms of this metric in the context of linear regression.
This metric measures the degree of overlap between the subspaces spanned by the eigenvectors of the pairwise inner product matrices of the compressed and uncompressed embeddings.
Intuitively, this is a natural metric because a linear regressor is only able to capture the part of the label vector contained within the span of its eigenvectors.
A consequence of this metric is that for a compressed embedding matrix to have high eigenspace overlap, it is necessary for its rank to be similar to that of the uncompressed embedding.

In an effort to design a simple compression method which preserves the rank of the uncompressed embedding, we propose a compression method using uniform quantization.
This compression method has two parts: It first determines the optimal threshold at which to clip the extreme values in the word embedding matrix, and then uniformly quantizes the embeddings within the clipped interval.
This method has no hyperparameters, and can compress large embeddings in under a minute on a single CPU core.
Additionally, we show that it can attain similar empirical performance to the state-of-the-art compression methods across a variety of tasks and embedding types.
\todo{Add sentence with more empirical details?}
%We observe that across tasks, our compression method can attain 32x compression within \todo{XX\%}-\todo{YY\%} performance relative to those attained by the state-of-the-art deep composition code learning (DCCL) approach~\citep{dccl17}.
Theoretically, we analyze the impact of uniform quantization on the eigenspace overlap metric, using matrix perturbation analysis to show that high eigenspace overlap can be attained at low-precision.

\todo{The rest of this paper is organized as follows...}
%1) Our goal is to compress embeddings to get good generalization performance under a memory budget.  We want something simple.
%2) To inform our design of a new simple compression method, we study other methods, and surprisingly the standard metrics fail, suggesting the need for a more refined metric.
%3) We introduce bla metric, and show it predicts empirical performance well. We also prove generalization bounds, connecting this metric directly to generalization performance. A consequence of this metric is that in order to attain large overlap, it is necessary for approximation to be high-rank.
%4) Motivated by the above connection between rank and generalization performance, we propose simple compression approach based on uniform quantization.
%>> no hyperparameters, fast training.
%>> Performs on par with the complex embeddings.
%>> We can prove its overlap is large.
