%Compressing word embeddings is critical to their deployment on memory-constrained devices.
We study the principles governing the downstream performance of compressed word embeddings.
We empirically demonstrate that existing metrics for evaluating compression performance \todo{align poorly} with downstream performance, and propose a new metric---the \textit{eigenspace overlap metric}---which \todo{aligns much better}.
%This metric measures the degree of overlap between the subspaces spanned by the eigenvectors of the nonzero eigenvalues of the Gram matrices of the compressed and uncompressed embeddings.
To explain this metric's influence on downstream performance, we prove bounds on the generalization performance of compressed embeddings in terms of this metric in the linear regression setting.
We then use this metric to better understand the empirical success of a simple compression based on uniform quantization, by proving explicit bounds on its eigenspace overlap.

%In this paper, we present a deeper understanding on the generalization performance of compressed word embeddings in downstream tasks.
%We begin by empirically comparing different embedding compression methods. Surprisingly, we observe simple uniform quantization can compete with state-of-the-art methods, and can match the generalization performance of uncompressed embeddings at high compression rate.
%To explain this performance comparison, 
%we propose a new compression quality metric \textit{eigenspace overlap}, which empirically correlates strongly with downstream task performance while conventional metrics fail to so. Built on this metric, we analyze the generalization bound for compressed embedding in downstream tasks, and use it to explain why uniformly quantized embeddings can attain strong generalization performance. Our analysis reveals that uniformly quantized embeddings can achieve strong eigenspace overlap with respect to the uncompressed counterparts with high probability. Thus it can match the performance of uncompressed embedding with high compression rate.


%We explicitly relate this metric with the downstream performance of embeddings by 
%
%We show that this metric is closely connected to downstream performance by proving bounds on the generalization performance of compressed embeddings in the linear regression setting in terms of this metric.
%
%
%
%To explain the downstream performance of compressed embeddings using this metric,
%
%To explain the impact of eigenspace overlap on downstream performance,
%
%We directly connect this metric with the downstream performance
%
%show that we can bound the generalization performance of compressed embeddings in the linear regression setting in terms of this metric.

%We then show that we can use this metric to better understand the performance of specific compression methods by proving explicit bounds on the eigenspace overlap of a simple compression method based on uniform quantization.




%Compressing word embeddings is critical to their deployment on memory-constrained devices such as mobile phones.
%State-of-the-art embedding compression methods attains strong empirical performance by learning compressed representations using deep architectures.
%In this paper, we work on a simple and fast embedding compression method based on uniform quantization, which can match the empirical performance of state-of-the-art compression methods across different compression rates and tasks.
%Theoretically, we analyze how the precision and dimensionality of the quantized word embeddings jointly impact their generalization performance in the context of linear ridge regression.
%Our analysis reveals that one can attain better performance under memory constraints by using lower-precision embeddings whose dimension approaches but does not exceed the optimal embedding dimension.

%First, we perform an empirical analysis of the downstream performance of several compression methods. 
%Surprisingly, we find that simple baselines, including one based on uniform quantization, can compete with the state-of-the-art neural compression algorithms, and that their success cannot be explained with existing metrics.


%We then use this metric to provide explicit bounds on the generalization performance of embeddings compressed with uniform quantization.

%To explain these empirical results, we propose a new metric---the eigenspace overlap metric---which we show better explains the downstream performance of embeddings.
%
%We investigate how to design simple and fast compression methods for word embeddings with strong downstream performance.
%To better reveal the impact of compression on downstream performance, we first propose a new metric---the \textit{eigenspace overlap metric}---for evaluating the quality of a compressed embedding.
%We prove generalization bounds in terms of this metric, and demonstrate that it aligns much better with the embedding's downstream performance than existing metrics.
%For a compressed embedding to have high eigenspace overlap, its rank must be similar to that of the uncompressed embedding.
%This motivates our design of a simple and fast compression method which preserves the embedding rank by uniformly quantizing the entries of the original embedding matrix.
%Theoretically, we show that uniform quantization has a small impact on the eigenspace overlap when \todo{$<$include condition here$>$}.
%Empirically, we demonstrate that our compression method is orders of magnitude faster than existing methods, and can match their downstream performance \todo{$<$include numbers here$>$}.


%Compressing word embeddings is critical to their deployment on memory-constrained devices such as mobile phones.
%Although numerous sophisticated compression techniques have been proposed, they are generally quite computationally expensive, and their performance is poorly understood.
%To better explain the performance of existing techniques, we propose a new eigenspace overlap metric which measure the 


% OLD VERSION
%Compressing word embeddings is critical to their deployment on memory-constrained devices such as mobile phones.
%State-of-the-art embedding compression methods attain strong empirical performance by learning compressed representations using deep architectures.
%In this paper, we work on a simple and fast embedding compression method based on uniform quantization that matches the empirical performance of state-of-the-art compression methods across different compression rates and tasks.
%To shed light on the settings in which this compression method is guaranteed to perform well, we analyze the impact of compression on generalization performance.
%This analysis reveals that if the number of bits per embedding entry is chosen appropriately, strong generalization performance is guaranteed.

%
%To shed light on the settings in which this compression method is guaranteed to perform well, we analyze the
%
%To lever
%
%To shed light on the settings in which this compression method is guaranteed to perform well, we present generalization bounds leveraging a notion of spectral approximation between matrices.
%
%This analysis that 
%
%To shed light on settings where uniformly quantized embeddings attain strong performance in downstream tasks, we theoretically analyze how quantization precision impact generalization performance in the context of linear ridge regression.
%Our analysis reveals that when quantization error is small relative to regularization, uniformly quantized embeddings can achieve similar performance as the uncompressed embeddings in downstream tasks.

%This method first determines a threshold value to clip extreme values in the embedding matrix using simple golden section search; it then uniformly quantizes the clipped embedding into fixed-point representation. 
%Without computationally expensive training, the uniform quantization approach can empirically match the performance of state-of-the-art compression methods across different compression rates.
%approach and solution
%Contribution 1: empirically match 
%Contribution 2: 
