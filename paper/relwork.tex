Compressing word embeddings has recently become an active area of research in NLP.
One important line of work uses sparse dictionary learning in order to compress word embedding matrices while maintaining strong empirical performance.
In this framework, words are represented as sparse combinations of vectors in a learned dictionary.
One approach to learning these sparse representations is to use non-negative sparse matrix factorization \citep{murphy12,sparse16}.
Recently, deep compositional code learning (DCCL) \cite{dccl17} and K-way D-dimensional discrete code learning \cite{kway18} independently proposed using deep architectures in order to learn a set of dictionaries, where each word is represented as a sum of one vector from each dictionary.
This approach to compressing word embeddings attains high compression rates while matching the state-of-the-art performance on tasks such as machine translation and sentiment analysis.
An alternative compression approach proposed by \citet{andrews16} is to run k-means clustering on the entries of the embedding matrix, and then represent each entry by the index of the closest centroid.
In this paper, we compare the performance of these existing compression methods with a simple and fast method based on uniform quantization.
We show this simple approach can match the empirical performance of the state-of-the-art approaches, and does not require expensive training or hyperparameter tuning.

Our analysis on how quantization impacts generalization performance builds on recent work on kernel ridge regression \citep{avron17, lprff18}.
In those papers, the authors study the impact of approximating the kernel matrix on the performance of the trained model.
Similarly, in this work we study the impact of approximating the embedding matrix with uniform quantization, and analyze the impact this approximation has on generalization performance. \todo{if we are going to talk about different methods in section 3, we then should move related work to the end of the paper.}

%\begin{itemize}
%	\item Existing compression methods: DCCL paper \citep{dccl17}, K-way code \citep{chen2018learning}, k-means \citep{andrews16}, and sparse representations \citep{sparse16}
%	\item Our paper \citep{lprff18} and Avron's paper \citep{avron17}
%	\item Recently there are works with good empirical performances: Sparsity and quantization. Quantization: k means method. Sparsity: based on variants of dictionary learning: dictionary based learning approaches (including Murphy)--- 1) murphy/chen16 uses NNSE , k way approaches, As another sparsity approach, Andrew also tested with per-dimensional sparsity using auto encoder. K means quantization . These method improves iterative training process, in this paper we use no training
%	\item Our generalization builds on matrix spectral approximation techniques and matrix concentration bounds. We use Bernstein to understand the guarantees on the generalizatin performance of blabla.
%\end{itemize}