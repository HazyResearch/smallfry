Compressing word embeddings has recently become an active area of research in NLP.
One important line of work uses sparse dictionary learning in order to compress word embedding matrices while maintaining strong empirical performance.
In this framework, words are represented as sparse combinations of vectors in a learned dictionary.
One approach to learning these sparse representations is to use non-negative sparse matrix factorization \citep{murphy12,sparse16}.
Recently, deep compositional code learning (DCCL) \cite{dccl17} and K-way D-dimensional discrete code learning \cite{kway18} independently proposed using deep architectures in order to learn a set of dictionaries, where each word is represented as a sum of one vector from each dictionary.
This approach to compressing word embeddings attains high compression rates while matching the state-of-the-art performance on tasks such as machine translation and sentiment analysis.
An alternative compression approach proposed by \citet{andrews16} is to run k-means clustering on the entries of the embedding matrix, and then represent each entry by the index of the closest centroid.
In this paper, we compare the performance of these compression methods including a simple and fast method based on uniform quantization. Based on these observations, we propose a new compression quality metric to gain deeper understanding on the downstream task performance of compressed word embeddings.

The generation of many word embedding types \cite{levy14, levy15} can be cast as implicit matrix factorization based on word co-occurrence statistics. Recently, there has been numerous progresses on understanding the connection between approximated matrix decomposition and the performance of machine learning models. For example, \citet{udell2017big} analyze data matrix generated from a class of latent variable models. They show low rank factorization can keep the maximum per entry approximation error sufficiently low. In the context of kernel methods, \citet{avron17, lprff18} proposed spectral approximation error metrics for different kernel approximation methods. They build on these metrics to explicitly understand the generalization of kernel ridge regression models.
Similarly, in this work we study the impact of approximating the Gram of a embedding matrix across different compression method, and analyze the impact this approximation has on generalization performance. 
%\todo{if we are going to talk about different methods in section 3, we then should move related work to the end of the paper.}

%\begin{itemize}
%	\item Existing compression methods: DCCL paper \citep{dccl17}, K-way code \citep{chen2018learning}, k-means \citep{andrews16}, and sparse representations \citep{sparse16}
%	\item Our paper \citep{lprff18} and Avron's paper \citep{avron17}
%	\item Recently there are works with good empirical performances: Sparsity and quantization. Quantization: k means method. Sparsity: based on variants of dictionary learning: dictionary based learning approaches (including Murphy)--- 1) murphy/chen16 uses NNSE , k way approaches, As another sparsity approach, Andrew also tested with per-dimensional sparsity using auto encoder. K means quantization . These method improves iterative training process, in this paper we use no training
%	\item Our generalization builds on matrix spectral approximation techniques and matrix concentration bounds. We use Bernstein to understand the guarantees on the generalizatin performance of blabla.
%\end{itemize}