In this section, we analyze the impact of uniform quantization on the generalization performance of linear regression models trained on top of the word embeddings.
We present two results:
First, we show that if the singular values of the embedding matrix decay slowly, then training the regression model with a large regularization parameter $\lambda$ cannot perform much worse than training with a smaller one.
Second, we show that when the matrix is quantized to $b$ bits, when $2^b \lambda$ is large, we attain a good generalization bounds for the quantized features with high probability.
Combining this theoretical result with our empirical observation that the singular values of embedding matrices decay slowly for various embedding types provides a principled explanation for why the quantized embeddings perform well on various tasks.

We begin, however, by reviewing generalization bounds for fixed design linear ridge regression.

\subsection{Background: Generalized Bounds for Fixed Design Linear Ridge Regression}
In fixed design linear regression, one is given a dataset $\{(x_i,y_i)\}_{i=1}^n$, for $x_i \in \RR^d$ and $y_i = \by_i + \eps_i \in \RR$, where the $\eps_i$ are independent random perturbations of the ``true labels'' $\by_i$ satisfying $\expect{}{\eps_i} = 0$ and $\var{}{\eps_i} = \sigma^2 < \infty$.
The goal is to design a training algorithm which takes as input the noisy dataset $\{(x_i,y_i)\}_{i=1}^n$ and outputs a model $f(x) = w^T x$ for which $\expect{}{\frac{1}{n}\sum_i (f(x_i) -\by_i)} \eqdef \cR(f)$ is small.
Linear ridge regression selects $w^* = \argmin_w \sum_i (w^T x_i - y_i)^2 + \lambda\|w\|_2^2$.
Letting $X \in \RR^{n\times d}$ be the matrix whose rows are $x_i$, $y \defeq (y_1,...,y_n) \in \RR^d$, and $I_d$ be the $d$-dimensional identity matrix, the minimizer of this optimization problem is $w^* = (\lambda I_d + X^T X)^{-1}X^Ty$.
It is easy to show \citep{alaoui15} that the expected generalization error for the model $f_X(x) = w^{*T}x$ is
\begin{eqnarray*}
\cR(f_X) &=& \frac{\lambda^2}{n} \by^T(XX^T + \lambda I_n)^{-2}\by \; +\\ && \frac{\sigma^2}{n}\tr\Big((XX^T)^2(XX^T + \lambda I_n)^{-2}\Big).
\end{eqnarray*}

The question we ask in this work is: when we replace $X$ by an approximation $\tX$, how close will $\cR(f_{\tX})$ be to $\cR(X)$?
To answer this question, we intuitively require two things: a notion of distance between $X$ and $\tX$, and an upper bound for $\cR(f_{\tX})$ in terms of $\cR{X}$ and the distance between $X$ and $\tX$.
For both of these components, we leverage the recent work of \citep{lprff18}.
In that work, they define the following notion of distance between two matrices:

\begin{definition}{\citep{lprff18}}
	\label{def:specdist}
	For $\Delta_1, \Delta_2 \geq 0$, a symmetric matrix $A$ is a \emph{$(\Delta_1, \Delta_2)$-spectral approximation} of another symmetric matrix $B$ if $(1-\Delta_1)B \preceq A \preceq (1+\Delta_2)B$. 
\end{definition}

By applying this definition to $\tX\tX^T + \lambda I_n$ and $XX^T + \lambda I_n$, the authors prove the following generalization bound:
\begin{proposition}{(Adapted from \citep{lprff18})}
	Let $K \defeq XX^T$ and $\tK \defeq \tX\tX^T$, and suppose $\tK + \lambda I_n$ is $(\Delta_1, \Delta_2)$-spectral approximation of $K+\lambda I_n$, for $\Delta_1 \in [0,1)$, $\Delta_2 \geq 0$.
	Let $d$ denote the rank of $\tX$, and let $f_{X}$ and $f_{\tX}$ be the ridge regression estimators learned using these matrices, with regularizing constant $\lambda \geq 0$ and label noise variance $\sigma^2 < \infty$. Then
	\begin{equation}
	\cR(f_{\tX}) \leq \frac{1}{1-\Delta_1} \hcR(f_X) +  \frac{\Delta_2}{1+\Delta_2}\frac{d}{n}\sigma^2,
	\label{eq:risk_bound}
	\end{equation}
	where 
	\begin{eqnarray*}
	\hcR(f_X) &\defeq& \frac{\lambda}{n} \by^T(K+\lambda I)^{-1}\by + \frac{\sigma^2}{n}\tr\Big(K(K+\lambda I)^{-1}\Big) \\
	&\geq& \cR(f_X).
%	\label{eq:avron_rhat}
	\end{eqnarray*}
	\label{prop:genbound}
\end{proposition}

\subsection{Theoretical Results}
We now show that our compression algorithm with high probability produces embedding matrices $\tX$ such that $\tX\tX^T + \lambda I_n$ is a $(\Delta_1,\Delta_2)$-spectral approximation of $XX^T+\lambda I_n$.
Combining this with the Proposition~\ref{prop:genbound} yields a generalization bound for the compressed embeddings.
A consequence of our bound is that when $\lambda$ is large, lower precision can be used for the embeddings without affecting $\Delta_1$ and $\Delta_2$.
We show that in the case of word embeddings with slowly-decaying singular values, a large regularization parameter $\lambda$ (and thus, a low-precision $b$) can be used without significantly affecting generalization performance.
Our empirical observation that embedding matrices have slowly decaying singular values thus helps explain why our compression algorithm is able to attain strong generalization performance at very low levels of precision.


\begin{theorem}
	\label{thm:main}
	Let $X \in \RR^{n\times d}$ be a data matrix with corresponding (linear) kernel matrix $K = XX^T$; let $X+C$ denote a $b$-bit quantization of $X$, with $\tK = (X+C)(X+C)^T$ the kernel matrix of the quantized data matrix. Here, $C$ denotes the quantization noise, with $\expect{}{C_{ij}} = 0$ and $\var{}{C_{ij}} \leq \delta_b^2/d \;\;\forall i,j$, where $b$ is the number of bits used per feature.
	Then for any $\Delta_1 \geq 0, \Delta_2 \geq \delta^2_b/\lambda$,
	\begin{eqnarray*}
	&&\hspace{-0.37in}\Prob\Big[(1- \Delta_1) (K + \lambda I_n) \preceq \tK + \lambda I_n \preceq (1 + \Delta_2) (K + \lambda I_n)
	\Big] 
	\\ &\geq& 1 - 
	n \exp \bigg(\frac{-\Delta_1^2}{2dL^2 + (2L/3)\Delta_1}\bigg) \\
	&&- n \exp \bigg(\frac{-(\Delta_2-\delta_b^2/\lambda)^2}{2dL^2 + (2L/3)(\Delta_2-\delta_b^2/\lambda)}\bigg),
	\end{eqnarray*}
	for $L \defeq 5 \cdot \frac{2^b \cdot \delta_b^2}{\lambda}\cdot \frac{n}{d}$.
\end{theorem}
\begin{corollary}
	\label{cor:main}
	If $\Delta_1 \geq \frac{\log(n/\rho)L}{3}\Big(1+\sqrt{1+\frac{18d}{\log(n/\rho)}}\Big) \approx \frac{5n}{2^b \lambda}\sqrt{\frac{2\log(n/\rho)}{d}}$,
	then $\Prob\big[(1 - \Delta_1) (K + \lambda I_n) \preceq \tK + \lambda I_n \big] \geq  1 - \rho$. 
	Similarly, if $\Delta_2 \geq \frac{\delta_b^2}{\lambda} +  \frac{\log(n/\rho)L}{3}\Big(1+\sqrt{1+\frac{18d}{\log(n/\rho)}}\Big) \approx \frac{1}{2^{2b}\lambda} + \frac{5n}{2^b \lambda}\sqrt{\frac{2\log(n/\rho)}{d}}$,
	then $\Prob\big[\tK + \lambda I_n \preceq (1 + \Delta_2) (K + \lambda I_n)\big] \geq  1 - \rho$. 
\end{corollary}

\begin{theorem}
	\label{thm:large_lambda}
	Let $X$ be a data matrix, and $\by$ be the corresponding vector of labels. Let $\sm$ be the smallest eigenvalue of $X^T X$, and let $\lambda_1, \lambda_2$ be two scalars such that $0 \leq \lambda_1 \leq \lambda_2 \leq a\cdot \sm$, for some $a \in [0,1]$. Letting $\cR_{\lambda}(K)$ denote the expected loss when training with regularizer $\lambda$, Gram matrix $K = XX^T$, and label noise $\sigma^2$, we get that:
	\begin{equation}
	\frac{R_{\lambda_2}(XX^T) - R_{\lambda_1}(XX^T)}{\|y\|^2/n} \leq a^2
	\label{eq1}
	\end{equation}
\end{theorem}
