%\subsection{Eigenspace Overlap}
%\label{subsec:eigen_overlap}
%	\begin{itemize}
%		\item Introduce the eigenspace overlap metric.
%%		\item Present the generalization bound.
%	\end{itemize}
%	
%\subsection{Generalization of Compressed Embeddings}
%\label{subsec:revisit}
%	\begin{itemize}
%		\item Empirically demonstrate the correlation with respect to generalization performance
%		\item Discuss why it explains better than Deltas (if have not discuss the relation to Deltas, we should discuss here also).
%	\end{itemize}

% Version Feb 19: using average similarity and analogy score
\begin{table*}
	\caption{The Spearman rank correlation coefficient $\rho$ between compression quality metrics and downstream performance. The larger the absolute value of the coefficient, the stronger the correlation is.
	Within each entry in the table, the correlations are presented in terms of `GloVe (Wiki'14) $\rho$ \; $|$ \; GloVe (Wiki'17) $\rho$ \; $|$ \; fastText $\rho$. In the table we demonstrate the \textbf{absolute value} of $\rho$ and higher absolute values indicate stronger rank correlation.
	}
	\small
	\centering
	\begin{tabular}{c | c | c | c | c | c | c | c | c | c | c | c | c}
		\toprule
		& \multicolumn{3}{|c|}{QA} & \multicolumn{3}{|c|}{Sentiment} & \multicolumn{3}{|c|}{Analogy} & \multicolumn{3}{|c}{Similarity} \\
		\midrule
%		Embed. reconst. error &  $-0.61/-0.22/-0.84$  &  $-0.40/-0.25/-0.69$  &  $\mathbf{-0.98/-1.00}/-0.88$  &  $-0.19/0.77/-0.36$  \\ 
%		PIP loss &  $-0.49/-0.32/-0.74$  &  $-0.29/-0.27/-0.61$  &  $-0.82/-0.67/-0.82$  &  $0.05/-0.07/-0.23$  \\  
%		$1/(1-\Delta_1)$ &  $-0.62/-0.83/-0.77$  &  $-0.42/-0.79/-0.56$  &  $-0.90/-0.98/-0.90$  &  $-0.22/-0.69/-0.31$  \\  
%		$\Delta_2$ &  $-0.46/-0.87/0.18$  &  $-0.39/-0.87/0.30$  &  $-0.70/-0.82/-0.02$  &  $-0.43/-0.72/-0.15$  \\  
%		$1 - \mathcal{E}$ & $\mathbf{-0.81/-0.97/-0.90}$  &  $\mathbf{-0.72/-0.96/-0.73}$  &  $-0.79/-0.91/\mathbf{-0.96}$  &  $\mathbf{-0.62/-0.88/-0.67}$  \\  
% latest p/n value version
%		Embed. reconst. error &  $-0.59/-0.56/-0.84$  &  $-0.37/-0.46/-0.69$  &  $\mathbf{-0.98/-0.95/-0.88}$  &  $-0.37/-0.39/-0.36$  \\ 
%		PIP loss &  $-0.49/-0.26/-0.34$  &  $-0.29/0.08/-0.21$  &  $-0.82/-0.38/-0.82$  &  $0.05/0.56/-0.21$  \\  
%		$1/(1-\Delta_1)$ &  $-0.62/-0.35/-0.80$  &  $-0.42/-0.42/-0.69$  &  $-0.90/-0.83/-0.79$  &  $-0.22/-0.13/-0.59$  \\  
%		$\Delta_2$ &  $-0.46/-0.40/0.48$  &  $-0.39/-0.27/0.57$  &  $-0.70/-0.42/-0.18$  &  $-0.43/-0.63/0.24$  \\  
%		$1 - \mathcal{E}$ & $\mathbf{-0.81/-0.71/-0.91}$  &  $\mathbf{-0.72/-0.66/-0.83}$  &  $-0.79/-0.77/-0.76$  &  $\mathbf{-0.62/-0.65/-0.84}$  \\ 
% absolute value version 
%		Embed. reconst. error &  $0.59$&$0.56$&$0.84$  &  $0.37$&$0.46$&$0.69$  &  $\mathbf{0.98}$&$\mathbf{0.95}$&$\mathbf{0.88}$  &  $0.37$&$0.39$&$0.36$  \\ 
		Embed. reconst. error (Frob.) &  $0.82$&$0.72$&$0.86$  &  $0.72$&$0.68$&$0.72$  &  $0.79$&$0.79$&$0.90$  &  $0.62$&$0.58$&$0.78$  \\
		PIP loss &  $0.49$&$0.26$&$0.34$  &  $0.29$&$0.08$&$0.21$  &  $0.82$&$0.38$&$\mathbf{0.82}$  &  $0.05$&$0.56$&$0.21$  \\  
		$1/(1-\Delta_1)$ &  $0.62$&$0.35$&$0.80$  &  $0.42$&$0.42$&$0.69$  &  $\mathbf{0.90}$&$\mathbf{0.83}$&$0.79$  &  $0.22$&$0.13$&$0.59$  \\  
		$\Delta_2$ &  $0.46$&$0.40$&$0.48$  &  $0.39$&$0.27$&$0.57$  &  $0.70$&$0.42$&$0.18$  &  $0.43$&$0.63$&$0.24$  \\  
		$1 - \mathcal{E}$ & $\mathbf{0.81}$&$\mathbf{0.71}$&$\mathbf{0.91}$  &  $\mathbf{0.72}$&$\mathbf{0.66}$&$\mathbf{0.83}$  &  $0.79$&$0.77$&$0.76$  &  $\mathbf{0.62}$&$\mathbf{0.65}$&$\mathbf{0.84}$  \\  
		%%	\midrule
		%Embed. reconst. error & & & & \\
		%%	\midrule
		%PIP loss & & & & \\
		%%	\midrule
		%$1/(1-\Delta_1)$ & & & & \\
		%%	\midrule
		%$\Delta_2$ & & & & \\
		%%	\midrule
		%$\mathcal{E}$ & & & & \\
		\bottomrule
	\end{tabular}
	\label{tab:sp_rank}
\end{table*}

% Version Feb 19: using average similarity and analogy score
\begin{table*}
	\caption{The Spearman rank correlation coefficient $\rho$ between compression quality metrics and downstream performance. The larger the absolute value of the coefficient, the stronger the correlation is.
	Within each entry in the table, the correlations are presented in terms of `GloVe (Wiki'14) $\rho$ \; $|$ \; GloVe (Wiki'17) $\rho$ \; $|$ \; fastText $\rho$. In the table we demonstrate the \textbf{absolute value} of $\rho$ and higher absolute values indicate stronger rank correlation. \textbf{We take google-mul and bruni-men for analogy and similarity.}
	}
	\small
	\centering
	\begin{tabular}{c | c | c | c | c | c | c | c | c | c | c | c | c}
		\toprule
		& \multicolumn{3}{|c|}{QA} & \multicolumn{3}{|c|}{Sentiment} & \multicolumn{3}{|c|}{Analogy} & \multicolumn{3}{|c}{Similarity} \\
		\midrule
%		Embed. reconst. error &  $-0.61/-0.22/-0.84$  &  $-0.40/-0.25/-0.69$  &  $\mathbf{-0.98/-1.00}/-0.88$  &  $-0.19/0.77/-0.36$  \\ 
%		PIP loss &  $-0.49/-0.32/-0.74$  &  $-0.29/-0.27/-0.61$  &  $-0.82/-0.67/-0.82$  &  $0.05/-0.07/-0.23$  \\  
%		$1/(1-\Delta_1)$ &  $-0.62/-0.83/-0.77$  &  $-0.42/-0.79/-0.56$  &  $-0.90/-0.98/-0.90$  &  $-0.22/-0.69/-0.31$  \\  
%		$\Delta_2$ &  $-0.46/-0.87/0.18$  &  $-0.39/-0.87/0.30$  &  $-0.70/-0.82/-0.02$  &  $-0.43/-0.72/-0.15$  \\  
%		$1 - \mathcal{E}$ & $\mathbf{-0.81/-0.97/-0.90}$  &  $\mathbf{-0.72/-0.96/-0.73}$  &  $-0.79/-0.91/\mathbf{-0.96}$  &  $\mathbf{-0.62/-0.88/-0.67}$  \\  
% latest p/n value version
%		Embed. reconst. error &  $-0.59/-0.56/-0.84$  &  $-0.37/-0.46/-0.69$  &  $\mathbf{-0.98/-0.95/-0.88}$  &  $-0.37/-0.39/-0.36$  \\ 
%		PIP loss &  $-0.49/-0.26/-0.34$  &  $-0.29/0.08/-0.21$  &  $-0.82/-0.38/-0.82$  &  $0.05/0.56/-0.21$  \\  
%		$1/(1-\Delta_1)$ &  $-0.62/-0.35/-0.80$  &  $-0.42/-0.42/-0.69$  &  $-0.90/-0.83/-0.79$  &  $-0.22/-0.13/-0.59$  \\  
%		$\Delta_2$ &  $-0.46/-0.40/0.48$  &  $-0.39/-0.27/0.57$  &  $-0.70/-0.42/-0.18$  &  $-0.43/-0.63/0.24$  \\  
%		$1 - \mathcal{E}$ & $\mathbf{-0.81/-0.71/-0.91}$  &  $\mathbf{-0.72/-0.66/-0.83}$  &  $-0.79/-0.77/-0.76$  &  $\mathbf{-0.62/-0.65/-0.84}$  \\ 
% absolute value version 
%		Embed. reconst. error &  $0.59$&$0.56$&$0.84$  &  $0.37$&$0.46$&$0.69$  &  $\mathbf{0.97}$&$\mathbf{0.90}$&$\mathbf{0.88}$  &  $0.24$&$0.26$&$0.64$  \\ 
		Embed. reconst. error (Frob.) &  $0.82$&$0.72$&$0.86$  &  $0.72$&$0.68$&$0.72$  &  $0.80$&$0.83$&$0.90$ &  $0.41$&$0.41$&$0.03$  \\
		PIP loss &  $0.49$&$0.26$&$0.34$  &  $0.29$&$0.08$&$0.21$  &  $0.82$&$0.18$&$\mathbf{0.82}$  &  $0.29$&$\mathbf{0.69}$&$\mathbf{0.58}$  \\  
		$1/(1-\Delta_1)$ &  $0.62$&$0.35$&$0.80$  &  $0.42$&$0.42$&$0.69$  &  $\mathbf{0.91}$&$0.76$&$0.79$  &  $0.07$&$0.09$&$0.03$  \\  
		$\Delta_2$ &  $0.46$&$0.40$&$0.48$  &  $0.39$&$0.27$&$0.57$  &  $0.69$&$0.37$&$0.18$  &  $0.27$&$0.60$&$0.37$  \\  
		$1 - \mathcal{E}$ & $\mathbf{0.81}$&$\mathbf{0.71}$&$\mathbf{0.91}$  &  $\mathbf{0.72}$&$\mathbf{0.66}$&$\mathbf{0.83}$  &  $0.80$&$\mathbf{0.81}$&$0.76$  &  $\mathbf{0.41}$&$0.51$&$0.24$  \\  
		%%	\midrule
		%Embed. reconst. error & & & & \\
		%%	\midrule
		%PIP loss & & & & \\
		%%	\midrule
		%$1/(1-\Delta_1)$ & & & & \\
		%%	\midrule
		%$\Delta_2$ & & & & \\
		%%	\midrule
		%$\mathcal{E}$ & & & & \\
		\bottomrule
	\end{tabular}
	\label{tab:sp_rank}
\end{table*}

%\begin{figure*}
%	\footnotesize
%	\begin{tabular}{@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}}
%		\includegraphics[width=.245\linewidth]{figures/glove400k_qa_best-f1_vs_subspace-eig-overlap_linx.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_qa_best-f1_vs_subspace-eig-overlap_linx.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove400k_sentiment_trec_test-acc_vs_subspace-eig-overlap_linx.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_sentiment_trec_test-acc_vs_subspace-eig-overlap_linx.pdf}	\\
%		(a) GloVe (Wiki'14), QA & \;\;\;\;(b) GloVe (Wiki'17), QA  & \;\;\;\;\;\;(c) GloVe (Wiki'14), sentiment & \;\;\;\;\;(d) GloVe (Wiki'14)
%	\end{tabular}
%	\caption{Our proposed compression quality metric eigenspace overlap correlates strongly with the downstream task performance of compressed embeddings.  On both the question answering and sentiment analysis task, we demonstrate that eigenspace overlap consistently achieves strong correlation 1) across different compression methods (shown in (a), (c)) and 2) across different precision for the uniform quantization methods (shown in (b), (d)). In other words, embeddings compressed by different methods using different configurations can performs similarly in downstream tasks, when they have similar eigenspace overlap with the same reference uncompressed embedding.}
%	\label{fig:good_correlation}
%\end{figure*}

%\begin{figure*}
%	\footnotesize
%	\begin{tabular}{@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}}
%		\includegraphics[width=.245\linewidth]{figures/glove400k_qa_best-f1_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_qa_best-f1_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove400k_sentiment_trec_test-acc_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_sentiment_trec_test-acc_vs_subspace-dist-normalized_linx_stoc.pdf}	\\
%		(a) GloVe (Wiki'14), QA & \;\;\;\;(b) GloVe (Wiki'17), QA  & \;\;\;\;\;\;(c) GloVe (Wiki'14), sentiment & \;\;\;\;\;(d) GloVe (Wiki'14)
%	\end{tabular}
%	\caption{\textbf{Stochastic.} Our proposed compression quality metric eigenspace overlap correlates strongly with the downstream task performance of compressed embeddings.  On both the question answering and sentiment analysis task, we demonstrate that eigenspace overlap consistently achieves strong correlation 1) across different compression methods (shown in (a), (c)) and 2) across different precision for the uniform quantization methods (shown in (b), (d)). In other words, embeddings compressed by different methods using different configurations can performs similarly in downstream tasks, when they have similar eigenspace overlap with the same reference uncompressed embedding.}
%	\label{fig:good_correlation_stoc}
%\end{figure*}

\begin{figure*}
	\footnotesize
	\begin{tabular}{@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}}
		\includegraphics[width=.245\linewidth]{figures/glove400k_qa_best-f1_vs_subspace-dist-normalized_linx_det.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_qa_best-f1_vs_subspace-dist-normalized_linx_det.pdf} &
		\includegraphics[width=.245\linewidth]{figures/fasttext1m_qa_best-f1_vs_subspace-dist-normalized_linx_det.pdf} &
		\includegraphics[width=.245\linewidth]{figures/glove400k_sentiment_sst_test-acc_vs_subspace-dist-normalized_linx_det.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_sentiment_sst_test-acc_vs_subspace-dist-normalized_linx_det.pdf}	\\
		\includegraphics[width=.245\linewidth]{figures/fasttext1m_sentiment_sst_test-acc_vs_subspace-dist-normalized_linx_det.pdf}	\\
		(a) GloVe (Wiki'14), QA & \;\;\;\;(b) fastText, QA  & \;\;\;\;\;\;(c) GloVe (Wiki'14), sentiment & \;\;\;\;\;(d) fastText, sentiment 
	\end{tabular}
	\caption{Our proposed compression quality metric eigenspace overlap correlates strongly with the downstream task performance of compressed embeddings.  On both the question answering and sentiment analysis task, we demonstrate that eigenspace overlap consistently achieves strong correlation 1) across different compression methods (shown in (a), (c)) and 2) across different precision for the uniform quantization methods (shown in (b), (d)). In other words, embeddings compressed by different methods using different configurations can performs similarly in downstream tasks, when they have similar eigenspace overlap with the same reference uncompressed embedding.}
	\label{fig:good_correlation_det}
\end{figure*}


%\begin{figure*}
%	\footnotesize
%	\begin{tabular}{@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}}
%		\includegraphics[width=.245\linewidth]{figures/glove400k_translation_BLEU4_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_BLEU4_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_BLEU4_vs_subspace-dist-normalized_linx_stoc.pdf}	\\
%		\includegraphics[width=.245\linewidth]{figures/glove400k_translation_min_val_loss_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_min_val_loss_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_min_val_loss_vs_subspace-dist-normalized_linx_stoc.pdf}	\\
%		\includegraphics[width=.245\linewidth]{figures/glove400k_translation_min_val_ppl_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_min_val_ppl_vs_subspace-dist-normalized_linx_stoc.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_min_val_ppl_vs_subspace-dist-normalized_linx_stoc.pdf}	\\
%		(a) GloVe (Wiki'14), translation & \;\;\;\;(b) GloVe (Wiki'17), translation  & \;\;\;\;\;\;(c) FastText, translation 	\end{tabular}
%	\caption{\textbf{Stochastic translation only. First Row, BLUE4; Second Row, val. loss; Third Row, val. perp} }
%	\label{fig:good_correlation_stoc_trans_only}
%\end{figure*}

%\begin{figure*}
%	\footnotesize
%	\begin{tabular}{@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}c@{\hskip -0.0in}}
%		\includegraphics[width=.245\linewidth]{figures/glove400k_translation_BLEU4_vs_subspace-dist-normalized_linx_det.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_BLEU4_vs_subspace-dist-normalized_linx_det.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_BLEU4_vs_subspace-dist-normalized_linx_det.pdf}	\\
%		\includegraphics[width=.245\linewidth]{figures/glove400k_translation_min_val_loss_vs_subspace-dist-normalized_linx_det.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_min_val_loss_vs_subspace-dist-normalized_linx_det.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_min_val_loss_vs_subspace-dist-normalized_linx_det.pdf}	\\
%		\includegraphics[width=.245\linewidth]{figures/glove400k_translation_min_val_ppl_vs_subspace-dist-normalized_linx_det.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_min_val_ppl_vs_subspace-dist-normalized_linx_det.pdf} &
%		\includegraphics[width=.245\linewidth]{figures/glove-wiki400k-am_translation_min_val_ppl_vs_subspace-dist-normalized_linx_det.pdf}	\\
%		(a) GloVe (Wiki'14), translation & \;\;\;\;(b) GloVe (Wiki'17), translation  & \;\;\;\;\;\;(c) FastText, translation 	\end{tabular}
%	\caption{\textbf{Deterministic translation only. First Row, BLUE4; Second Row, val. loss; Third Row, val. perp} }
%	\label{fig:good_correlation_stoc_trans_only}
%\end{figure*}

In this section, we introduce the \textit{eigenspace overlap metric} to measure the quality of a compressed embedding relative to the uncompressed embedding.
We prove average-case generalization bounds for this metric in the context of linear regression, and show that this metric indeed aligns very well with the downstream performance of the compressed embeddings.

\subsection{Eigenspace Overlap and Generalization}
\label{subsec:eigen_overlap}
\begin{definition}
Given two embedding matrices $X \in \RR^{n \times d}$, $\tX \in \RR^{n \times k}$, whose Gram matrices have eigendecompositions $XX^T = USU^T$, $\tX\tX^T = VRV^T$ for $U \in \RR^{n\times d}$, $V\in \RR^{n \times k}$, we define the eigenspace overlap metric $\eigover(X,\tX) \defeq  \frac{1}{\max(d,k)}\|U^T V\|_F^2$.
\end{definition}

This metric measures the degree to which the span of the eigenvectors with nonzero eigenvalue of $\tX\tX^T$ agrees with that of $XX^T$.
In particular, assuming $k\leq d$, it measures the ratio between the squared Frobenius norm of $U$ before and after being projected onto $V$.
As an example, if the span $\Span(V)$ of the columns of $V$ is a subspace of $\Span(U)$, then $\eigover(X,\tX) = \frac{1}{d}\|U^T V\|_F^2 = \frac{1}{d}\|UU^T V\|_F^2 = \frac{1}{d}\|V\|_F^2 = \frac{k}{d}$.
In contrast, if $\Span(V)$ is orthogonal to $\Span(U)$, then $\eigover(X,\tX) = 0$.

We now show that this metric is closely related to the generalization performance of the linear regression model trained using the compressed embeddings $\tX$ in place of $X$.
For simplicity, we will consider here the noiseless fixed design regression setting;
In this setting, the training loss is equal to the generalization loss.
Letting $y\in\RR^n$ denote the vector of labels and using the closed form solution for the optimal parameters $w^* = (X^T X)^{-1}X^Ty$, we can derive that the generalization performance is equal to $\cR(X) = \|Xw^* - y\|^2 = \|y\|^2 - \|U^T y\|^2$.

To expose the influence of the eigenspace overlap on the generalization performance, it will be necessary for us to considering an average-case analysis.
This is necessary because in worst-case analysis, if there exists a single direction in $\Span(U)$ orthogonal to $\Span(V)$ (which always occurs when $\dim(V) < \dim(U)$) the label vector $y$ can simply be equal to this direction.
In this case $\cR(X) = \|y\|^2 - \|U^T y\|^2 = 0$, while $\cR(\tX) = \|y\|^2 - \|V^T y\|^2 = \|y\|^2$.
Thus, the setting we will consider instead is one where $y$ is a random vector in $\Span(U)$.
We consider the setting $y \in \Span(U)$ (which implies $\cR(X) = 0$) for simplicity because we are most interested in the situation where we know the uncompressed embedding matrix $X$ performs well, and we would like to understand how well $\tX$ will do.
We now present our average-case result (see Appendix~\ref{app:theory} for proof):

\begin{proposition}
If $y = Uz$ for a random vector $z \in \RR^d$ with zero mean and identity covariance, then
\begin{eqnarray}
\expect{y}{\cR(\tX) - \cR(X)} &=& d\cdot(1 - \eigover(X,\tX))
\end{eqnarray}
\end{proposition}

This proposition reveals that a larger eigenspace overlap value results in better generalization performance for the compressed embedding.

\paragraph{Discussion}
Here, we provide a more detailed discussion of two important aspects of the eigenspace overlap metric:
(1) It only depends on the left singular vectors of the embedding matrices;
(2) It is much more robust than the previously proposed metrics to perturbations of the embedding matrix which are unlikely to significantly change the generalization performance (in the average-case sense).

To better understand why the eigenspace overlap only depends on the left singular vectors of the embedding matrices, we first observe in the above equation for $\cR(X)$ that the generalization performance of a linear model trained on $X$ only depends on the left singular vectors of $X$ (assuming $X$ is full-rank).
To see why this is the case, consider the predicted labels $y \defeq Xw$, for some linear model $w \in \RR^d$ over $X$.
If we replace $X$ with $\tX \defeq U \tSigma \tW^T$, and we replace $w$ with $\tw \defeq \tW \tSigma^{-1} \Sigma W^T w$, then $\tX \tw = U\tSigma \tW^T \tW \tSigma^{-1} \Sigma W^T w = Xw$.
Thus, a linear model over $\tX$ can exactly replicate the output of any linear model over $X$, assuming the left singular vectors of $X$ and $\tX$ are equal.
This observation provides intuition for why it is reasonable for the eigenspace overlap metric to only consider the left singular vectors of the embedding matrices.

To demonstrate the robustness of the eigenspace overlap to perturbations of the embedding matrix unlikely to have a large effect on generalization performance, we consider the following simple perturbation:
if $X = \sum_{i=1}^{d} \sigma_i U_i W_i^T$ is the singular value decomposition of $X$, we consider setting it's largest eigenvalue to 0, resulting in the perturbed matrix $\tX \defeq \sum_{i=2}^{d} \sigma_i U_i W_i^T$.
Assuming a linear model $y = Xw = \sum_i \alpha_i U_i$, $\tX$ would have a generalization error of $\alpha_1^2$.
If we assume that $\alpha_1^2 \ll \sum_{i=1}^{d-1}\alpha_i^2$ (as is the case in our average-case analysis), then $\tX$ would perform similarly to $X$.

In Table~\ref{table:perturb}, we show the impact of the above perturbation on the various metrics we have discussed for comparing embedding matrices.
At a high-level, we observe that this perturbation can have a dramatic effect of the previously proposed metrics, while having minimal effect on the eigenspace overlap metric.
For example, setting the largest eigenvalue to 0 can result in a $\Delta_1$ value very close to 1 if $\lambda \ll \sigma_1$.
This makes sense, because $\Delta_1$ can be used to attain a worst-case generalization bound for the perturbed embeddings \citep{lprff18}, and there exist cases where setting the largest singular value to 0 can significantly harm the generalization performance of the embeddings (\eg, if $\alpha_1^2 \approx \|\alpha\|^2$).
Thus, while the $\Delta_1$ metric is an important metric for understanding the worst-case performance of the compressed embeddings, it is generally an overly pessimistic metric.
Our metric, on the other hand, is generally unable to provide worst-case guarantees, but aligns nicely with the expected performance of the compressed embeddings in the average-case setting.

Another perturbation which can affect the other metrics much more than it affects the eigenspace overlap metric is right-multiplication by an invertible matrix.
While this perturbation can have unbounded effects on the other metrics, it has no effect on the eigenspace overlap metric, and no effect on generalization performance.
%Note that the eigenspace overlap metric is additionally invariant to right multiplication of the embedding matrix by any invertible matrix, while none of the other metrics are.

\begin{table}
	\caption{In this table, we consider the effect of perturbing an embedding matrix $X$ by setting its largest singular value to 0 on the various compression quality metrics discussed above.
%	In the last row, we plot the generalization error $\cR(\tX) = \|\tX \tw - y\|^2$ which results from learning a linear model on top of $\tX_1$ or $\tX_2$ in place of $X$, where we assume the labels $y = \sum_{i=1}^d \alpha_i U_i$.}
	As we can see, setting the largest singular value of $X$ to 0 can have a disproportionately large effect on the relative reconstruction error, relative PIP loss, and $\Delta_1$ metrics (values can approach 1), while having a modest effect on the eigenspace overlap metric (value of $1/d$ always).
%	In the last row, we plot the generalization error $\cR(\tX) = \|\tX \tw - y\|^2$ which results from learning a linear model on top of the perturbed embeddings in place of $X$, where we assume the labels $y = \sum_{i=1}^d \alpha_i U_i$.
%	If we assume that $\alpha_1^2 \ll \sum_{i=1}^{d-1}\alpha_i^2$ (as in our average-case analysis), then the perturbed embeddings would perform similarly to the unperturbed embeddings.
}
	\centering
	\begin{tabular}{ c | c }
		Compression quality metric & Metric after perturbation \\ \midrule
		Rel. reconstruction error &  $\sigma_1/\sqrt{\sum_{i=1}^d \sigma_i^2}$ \\
		Rel. PIP loss & $\sigma_1^2 / \sqrt{\sum_{i=1}^d \sigma_i^4}$ \\
		$\Delta_1$ & $\sigma_1^2 / \Big(\sigma_1^2 + \lambda\Big)$ \\
		$\Delta_2$ & 0 \\
		$1-\eigover(X,\tX)$ & $\frac{1}{d}$\\ %\hline
%		$\cR(\tX)$ & $\alpha_1^2$\\
	\end{tabular}
	\label{table:perturb}
\end{table}

%\begin{table}
%	\caption{In this table, we consider two perturbations $\tX_1$ and $\tX_2$ of $X$, and compute the value of the various metrics of compression quality between these perturbations and $X$. Specifically, we consider $\tX_1 = aX$ for some $a \geq 0$, and $\tX_2 = \sum_{i=1}^{d-1} \sigma_i U_i W_i^T$, where $X = \sum_{i=1}^d \sigma_i U_i W_i^T$.  We let $\sigma_{\min}$ and $\sigma_{\max}$ denotes the smallest and largest singular values of $X$, respectively. In the last row, we plot the generalization error $\cR(\tX) = \|\tX \tw - y\|^2$ which results from learning a linear model on top of $\tX_1$ or $\tX_2$ in place of $X$, where we assume the labels $y = \sum_{i=1}^d \alpha_i U_i$.}
%	\centering
%	\begin{tabular}{ c | c | c }
%		& $\tX_1$ & $\tX_2$ \\ \hline
%		Rel. reconstruction error & $|a-1| \cdot \|X\|_F$ & $\sigma_{\min}$ \\
%		Rel. PIP loss & $|a^2 - 1| \cdot \|XX^T\|_F$ & $\sigma_{\min}^2$ \\
%		$\Delta_1$ & $\max\Big(0,\frac{\sigma_{\max}^2 (1-a^2) }{\sigma_{\max}^2 + \lambda}\Big)$ & $\frac{\sigma_{\min}^2}{\sigma_{\min}^2 + \lambda}$ \\
%		$\Delta_2$ & $\max\Big(0,\frac{\sigma_{\max}^2 (a^2-1) }{\sigma_{\max}^2 + \lambda}\Big)$ & 0 \\
%		$1-\eigover(X,\tX)$ & $0$ & $\frac{1}{d}$\\ \hline
%		$\cR(\tX)$ & $0$ & $\alpha_d^2$\\
%	\end{tabular}
%\label{table:perturb}
%\end{table}

\subsection{Revisiting the Performance of Compressed Embeddings}
\label{subsec:revisit}
We now demonstrate that unlike the metrics we discussed in Section~\ref{sec:prelim}, this metric empirically aligns very well with the performance of compressed embeddings on downstream tasks.
In Figure~\ref{fig:good_correlation} we show scatter plots of downstream performance vs.\ eigenspace overlap, and see that in general higher eigenspace overlap corresponds to better performance.
Thus, even though our analysis is for a linear regression setting, we can see that this metric predicts performance well on a variety of downstream tasks which use neural networks for training.
%\todo{Discuss performance relative to $\Delta_1$,$\Delta_2$?}
