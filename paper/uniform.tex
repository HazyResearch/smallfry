%\subsection{Method Details}
%\label{subsec:method}
%	\begin{itemize}
%		\item Give the method descriptions
%		\item The pseudo-code.
%	\end{itemize}
	
\subsection{Theoretical Results}
%\label{subsec:gen_perf}
	\begin{itemize}
		\item Theorem on quantization achieve strong subspace overlap with high probability
		\item Corollary on the bits to achieve a specific subspace overlap value
	\end{itemize}
	
\subsection{Theory validation}
	\begin{itemize}
		\item Impact of quantization on overlap 
			\begin{itemize}
 				\item Exp 1: overlap vs precision for different dimensionality. Expectation: overlap increases with higher precision.
				\item Exp2: overlap vs dimensionality for different precision. Expectation: overlap increases with dimensionality. This explains that under fix memory budget, using lower bits quantization can be beneficial
			\end{itemize}
		\item The impact of clipping on eigen-subspace overlap
			\begin{itemize}
				\item Simulation based experiments on subspace overlap as a function of different clipping threshold and precision. 
				\item The way we introduce this in: in our main theorem, we assume the dynamic range is $O(1/\sqrt{d})$ as a consequence of the automatic clipping. We want to show here this is the case in practice and then discuss the specific way clipping influence eigenspace overlap.
			\end{itemize}
		\item Loop-back discussion on the large scale empirical experiments in Section~\ref{subsec:hard_explain}
	\end{itemize}

%In this section, we present our word embedding compression algorithm.
%At a high-level, our algorithm clips the embedding matrix into the range $[-r,r]$, and then uniformly quantizes the entries of the clipped matrix;
%we choose the value of $r$ which minimizes the reconstruction error of the quantized matrix.
%Because our algorithm uses uniform quantization, we first review uniform quantization, and then describe in more detail how our algorithm uses it to compress word embeddings.
%
%A \textit{$b$-bit uniform quantization} $Q_{b,r}(x)$ of a real number $x \in [-r,r]$ is computed as follows:
%First, the interval $[-r,r]$ is divided into $2^b - 1$ sub-intervals of equal size.
%Then, $x$ is rounded to either the top or bottom of the sub-interval $[\ulx,\olx]$ containing $x$, where $\ulx = r + j\frac{2r}{2^b-1}$ and $\olx = r + (j+1)\frac{2r}{2^b-1}$, for $j\in\{0,1,\ldots,2^b-2\}$.
%Given this rounded value, one can simply store the $b$-bit integer $j$ or $j+1$ in place of the real-valued $x$, depending on whether $x$ was rounded to $\ulx$ and $\olx$ respectively.
%In this work, we will consider a deterministic rounding scheme which rounds $x$ to the nearest value, and a stochastic rounding scheme which rounds $x$ up or down in such a way that the expected value is equal to $x$.\footnote{
%	This stochastic scheme rounds $x$ to $\ulx$ with probability $\frac{\olx-x}{\olx-\ulx}$ and to $\olx$ with probability $\frac{x-\ulx}{\olx-\ulx}$.
%}
%In particular, our analysis will focus on the stochastic rounding scheme, while our experiments will include results with both schemes.
%%Note that we can upper bound the variance of these rounding schemes using the fact that a bounded random variable in an interval of length $c$ has variance at most $c^2/4$;
%%using this fact, we can see that the variances of these rounding schemes are at most $\frac{1}{4} \cdot \Big(\frac{2r}{(2^b-1)}\Big)^2 = \frac{4r^2}{(2^b-1)^2}$.
%
%
%We are now ready to present our compression algorithm, which we express in pseudo-code in Algorithm~\ref{alg:smallfry}.
%The input to the algorithm is an embedding matrix $X \in \RR^{n\times d}$, where $n$ is the size of the vocabulary, and $d$ is the dimension of the embeddings.
%We define the function $\clip_r(x) = \max(\min(x,r),-r)$ for any non-negative $r$; when matrices are passed in as inputs to this function, it clips the entries in an element-wise fashion.
%The first step in our algorithm is to find the value of $r \in [0,\max(|X|)]$ which minimizes the reconstruction error of the quantized embeddings after $X$ is clipped to $[-r,r]$.
%More formally, we let $r^* \defeq \argmin_{r \in [0,\max(|X|)} \|Q_{b,r}(\clip_r(X))-X\|_F$.
%We then use this value $r^*$ to clip $X$, and then quantize the clipped embeddings to $b$ bits per entry.
%
%In our experiments, we find $r^*$ to within a specified tolerance $\eps > 0$ using the golden-section search algorithm \citep{golden53}.
%To avoid stochasticity impacting the search process, we always use deterministic rounding in the search for $r^*$, even if we use stochastic quantization in the final quantization.
%%this choice also allows us to more cleanly compare deterministic vs.\ stochastic rounding, since they will always use the same value of $r^*$.
%
%\begin{algorithm}[tb]
%   \caption{Our embedding compression algorithm}
%   \label{alg:smallfry}
%\begin{algorithmic}[1]
%	\STATE {\bfseries Input:}  Embedding matrix $X \in \RR^{n \times d}$, quantization function $Q_{b,r}$, clipping function $\clip_r\colon\RR\rightarrow[-r,r]$.
%	\STATE {\bfseries Output:} Quantized embeddings $\hat{X}$.
%	\STATE $r^* \defeq \argmin_{r \in [0,\max(|X|)} \|Q_{b,r}(\clip_r(X))-X\|_F$.
%	%Search for $r^* \in [0,\max(|X|)]$ minimizing $\|Q_{b,r}(\clip_r(X))-X\|_F$.
%	\STATE {\bfseries Return:} $Q_{b,r^*}(\clip_{r^*}(X))$.
%\end{algorithmic}
%\end{algorithm}
%
