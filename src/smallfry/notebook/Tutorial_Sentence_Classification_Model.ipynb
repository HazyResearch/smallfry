{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from third_party.sentence_classification.train_classifier import train_sentiment\n",
    "from third_party.sentence_classification.train_classifier import eval_model\n",
    "from third_party.sentence_classification import dataloader\n",
    "from third_party.sentence_classification import modules\n",
    "from quant_embedding import QuantEmbedding\n",
    "from quant_embedding import quantize_embed\n",
    "import argparse\n",
    "import logging\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train a model and save check points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.__version__ >= '0.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--dataset', 'trec', '--path', '../third_party/sentence_classification/data/', '--embedding', '../glove.6B.300d.txt', '--cnn', '--max_epoch', '100', '--model_seed', '1', '--data_seed', '1', '--lr', '0.001', '--save_mdl', './model_ckpt', '--dropout', 0.0]\n",
      "INFO:root:Embedding hash: b78f53fb56ec1ce9edc367d2e6186ba4\n",
      "INFO:root:Machine: dawn8.stanford.edu\n",
      "INFO:root:CMD: python /lfs/1/zjian/anaconda2/envs/pytorch_1.0/lib/python3.6/site-packages/ipykernel_launcher.py -f /run/user/16494/jupyter/kernel-2d2fd2c3-1ff8-4514-ad67-c3a7b174f2c9.json\n",
      "INFO:root:========================================\n",
      "INFO:root:Command Line Args\n",
      "INFO:root:========================================\n",
      "INFO:root:cnn            : 1         \n",
      "INFO:root:lstm           : 0         \n",
      "INFO:root:la             : 0         \n",
      "INFO:root:no_normalize   : 0         \n",
      "INFO:root:dataset        : trec      \n",
      "INFO:root:path           : ../third_party/sentence_classification/data/\n",
      "INFO:root:embedding      : ../glove.6B.300d.txt\n",
      "INFO:root:batch_size     : 32        \n",
      "INFO:root:max_epoch      : 100       \n",
      "INFO:root:d              : 128       \n",
      "INFO:root:dropout        : 0.0       \n",
      "INFO:root:depth          : 2         \n",
      "INFO:root:lr             : 0.001     \n",
      "INFO:root:lr_decay       : 0.0       \n",
      "INFO:root:model_seed     : 1         \n",
      "INFO:root:data_seed      : 1         \n",
      "INFO:root:save_mdl       : ./model_ckpt\n",
      "INFO:root:load_mdl       : None      \n",
      "INFO:root:out            : None      \n",
      "INFO:root:snapshot       : 0         \n",
      "INFO:root:cycles         : None      \n",
      "INFO:root:embedding_list : None      \n",
      "INFO:root:tag            : None      \n",
      "INFO:root:no_cudnn       : 0         \n",
      "INFO:root:----------------------------------------\n",
      "INFO:root:Using single embedding file.\n",
      "INFO:root:Beginning to load embeddings\n",
      "INFO:root:Finished loading embeddings\n",
      "INFO:root:400000 pre-trained word embeddings loaded.\n",
      "WARNING:root:n_d (128) != word vector size (300). Use 300 for embeddings.\n",
      "INFO:root:Number of vectors: 403717, Number of loaded vectors: 400000, Number of oov 3717\n",
      "INFO:root:embedding shape: torch.Size([403717, 300])\n",
      "INFO:root:154 batches, avg len: 21.0\n",
      "INFO:root:18 batches, avg len: 20.8\n",
      "INFO:root:16 batches, avg len: 14.4\n",
      "INFO:root:Epoch=0 iter=154 lr=0.001000 train_loss=1.078427 valid_err=0.267399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../third_party/sentence_classification/train_classifier.py:81: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  x, y = Variable(x, volatile=True), Variable(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch=1 iter=308 lr=0.001000 train_loss=0.378204 valid_err=0.194139\n",
      "INFO:root:Epoch=2 iter=462 lr=0.001000 train_loss=0.131971 valid_err=0.164835\n",
      "INFO:root:Epoch=3 iter=616 lr=0.001000 train_loss=0.061415 valid_err=0.144689\n",
      "INFO:root:Epoch=4 iter=770 lr=0.001000 train_loss=0.037204 valid_err=0.137363\n",
      "INFO:root:Epoch=5 iter=924 lr=0.001000 train_loss=0.025701 valid_err=0.133700\n",
      "INFO:root:Epoch=6 iter=1078 lr=0.001000 train_loss=0.019831 valid_err=0.126374\n",
      "INFO:root:Epoch=7 iter=1232 lr=0.001000 train_loss=0.014343 valid_err=0.128205\n",
      "INFO:root:Epoch=8 iter=1386 lr=0.001000 train_loss=0.009624 valid_err=0.124542\n",
      "INFO:root:Epoch=9 iter=1540 lr=0.001000 train_loss=0.006781 valid_err=0.126374\n",
      "INFO:root:Epoch=10 iter=1694 lr=0.001000 train_loss=0.004996 valid_err=0.117216\n",
      "INFO:root:Epoch=11 iter=1848 lr=0.001000 train_loss=0.003826 valid_err=0.122711\n",
      "INFO:root:Epoch=12 iter=2002 lr=0.001000 train_loss=0.002954 valid_err=0.120879\n",
      "INFO:root:Epoch=13 iter=2156 lr=0.001000 train_loss=0.002103 valid_err=0.119048\n",
      "INFO:root:Epoch=14 iter=2310 lr=0.001000 train_loss=0.001408 valid_err=0.122711\n",
      "INFO:root:Epoch=15 iter=2464 lr=0.001000 train_loss=0.001109 valid_err=0.126374\n",
      "INFO:root:Epoch=16 iter=2618 lr=0.001000 train_loss=0.001041 valid_err=0.122711\n",
      "INFO:root:Epoch=17 iter=2772 lr=0.001000 train_loss=0.001007 valid_err=0.122711\n",
      "INFO:root:Epoch=18 iter=2926 lr=0.001000 train_loss=0.000943 valid_err=0.124542\n",
      "INFO:root:Epoch=19 iter=3080 lr=0.001000 train_loss=0.000833 valid_err=0.128205\n",
      "INFO:root:Epoch=20 iter=3234 lr=0.001000 train_loss=0.000712 valid_err=0.128205\n",
      "INFO:root:Epoch=21 iter=3388 lr=0.001000 train_loss=0.000615 valid_err=0.128205\n",
      "INFO:root:Epoch=22 iter=3542 lr=0.001000 train_loss=0.000531 valid_err=0.126374\n",
      "INFO:root:Epoch=23 iter=3696 lr=0.001000 train_loss=0.000463 valid_err=0.126374\n",
      "INFO:root:Epoch=24 iter=3850 lr=0.001000 train_loss=0.000402 valid_err=0.124542\n",
      "INFO:root:Epoch=25 iter=4004 lr=0.001000 train_loss=0.000354 valid_err=0.120879\n",
      "INFO:root:Epoch=26 iter=4158 lr=0.001000 train_loss=0.000312 valid_err=0.119048\n",
      "INFO:root:Epoch=27 iter=4312 lr=0.001000 train_loss=0.000276 valid_err=0.115385\n",
      "INFO:root:Epoch=28 iter=4466 lr=0.001000 train_loss=0.000246 valid_err=0.115385\n",
      "INFO:root:Epoch=29 iter=4620 lr=0.001000 train_loss=0.000219 valid_err=0.117216\n",
      "INFO:root:Epoch=30 iter=4774 lr=0.001000 train_loss=0.000196 valid_err=0.115385\n",
      "INFO:root:Epoch=31 iter=4928 lr=0.001000 train_loss=0.000176 valid_err=0.115385\n",
      "INFO:root:Epoch=32 iter=5082 lr=0.001000 train_loss=0.000159 valid_err=0.115385\n",
      "INFO:root:Epoch=33 iter=5236 lr=0.001000 train_loss=0.000144 valid_err=0.113553\n",
      "INFO:root:Epoch=34 iter=5390 lr=0.001000 train_loss=0.000130 valid_err=0.113553\n",
      "INFO:root:Epoch=35 iter=5544 lr=0.001000 train_loss=0.000118 valid_err=0.113553\n",
      "INFO:root:Epoch=36 iter=5698 lr=0.001000 train_loss=0.000107 valid_err=0.113553\n",
      "INFO:root:Epoch=37 iter=5852 lr=0.001000 train_loss=0.000098 valid_err=0.113553\n",
      "INFO:root:Epoch=38 iter=6006 lr=0.001000 train_loss=0.000089 valid_err=0.113553\n",
      "INFO:root:Epoch=39 iter=6160 lr=0.001000 train_loss=0.000081 valid_err=0.115385\n",
      "INFO:root:Epoch=40 iter=6314 lr=0.001000 train_loss=0.000074 valid_err=0.115385\n",
      "INFO:root:Epoch=41 iter=6468 lr=0.001000 train_loss=0.000068 valid_err=0.117216\n",
      "INFO:root:Epoch=42 iter=6622 lr=0.001000 train_loss=0.000062 valid_err=0.117216\n",
      "INFO:root:Epoch=43 iter=6776 lr=0.001000 train_loss=0.000057 valid_err=0.115385\n",
      "INFO:root:Epoch=44 iter=6930 lr=0.001000 train_loss=0.000052 valid_err=0.115385\n",
      "INFO:root:Epoch=45 iter=7084 lr=0.001000 train_loss=0.000047 valid_err=0.115385\n",
      "INFO:root:Epoch=46 iter=7238 lr=0.001000 train_loss=0.000044 valid_err=0.115385\n",
      "INFO:root:Epoch=47 iter=7392 lr=0.001000 train_loss=0.000040 valid_err=0.115385\n",
      "INFO:root:Epoch=48 iter=7546 lr=0.001000 train_loss=0.000037 valid_err=0.115385\n",
      "INFO:root:Epoch=49 iter=7700 lr=0.001000 train_loss=0.000034 valid_err=0.115385\n",
      "INFO:root:Epoch=50 iter=7854 lr=0.001000 train_loss=0.000031 valid_err=0.113553\n",
      "INFO:root:Epoch=51 iter=8008 lr=0.001000 train_loss=0.000028 valid_err=0.113553\n",
      "INFO:root:Epoch=52 iter=8162 lr=0.001000 train_loss=0.000026 valid_err=0.111722\n",
      "INFO:root:Epoch=53 iter=8316 lr=0.001000 train_loss=0.000024 valid_err=0.111722\n",
      "INFO:root:Epoch=54 iter=8470 lr=0.001000 train_loss=0.000022 valid_err=0.111722\n",
      "INFO:root:Epoch=55 iter=8624 lr=0.001000 train_loss=0.000020 valid_err=0.111722\n",
      "INFO:root:Epoch=56 iter=8778 lr=0.001000 train_loss=0.000019 valid_err=0.111722\n",
      "INFO:root:Epoch=57 iter=8932 lr=0.001000 train_loss=0.000017 valid_err=0.109890\n",
      "INFO:root:Epoch=58 iter=9086 lr=0.001000 train_loss=0.000016 valid_err=0.109890\n",
      "INFO:root:Epoch=59 iter=9240 lr=0.001000 train_loss=0.000014 valid_err=0.109890\n",
      "INFO:root:Epoch=60 iter=9394 lr=0.001000 train_loss=0.000013 valid_err=0.109890\n",
      "INFO:root:Epoch=61 iter=9548 lr=0.001000 train_loss=0.000012 valid_err=0.109890\n",
      "INFO:root:Epoch=62 iter=9702 lr=0.001000 train_loss=0.000011 valid_err=0.109890\n",
      "INFO:root:Epoch=63 iter=9856 lr=0.001000 train_loss=0.000010 valid_err=0.109890\n",
      "INFO:root:Epoch=64 iter=10010 lr=0.001000 train_loss=0.000009 valid_err=0.109890\n",
      "INFO:root:Epoch=65 iter=10164 lr=0.001000 train_loss=0.000009 valid_err=0.109890\n",
      "INFO:root:Epoch=66 iter=10318 lr=0.001000 train_loss=0.000008 valid_err=0.109890\n",
      "INFO:root:Epoch=67 iter=10472 lr=0.001000 train_loss=0.000007 valid_err=0.109890\n",
      "INFO:root:Epoch=68 iter=10626 lr=0.001000 train_loss=0.000007 valid_err=0.109890\n",
      "INFO:root:Epoch=69 iter=10780 lr=0.001000 train_loss=0.000006 valid_err=0.109890\n",
      "INFO:root:Epoch=70 iter=10934 lr=0.001000 train_loss=0.000006 valid_err=0.109890\n",
      "INFO:root:Epoch=71 iter=11088 lr=0.001000 train_loss=0.000005 valid_err=0.111722\n",
      "INFO:root:Epoch=72 iter=11242 lr=0.001000 train_loss=0.000005 valid_err=0.111722\n",
      "INFO:root:Epoch=73 iter=11396 lr=0.001000 train_loss=0.000004 valid_err=0.111722\n",
      "INFO:root:Epoch=74 iter=11550 lr=0.001000 train_loss=0.000004 valid_err=0.111722\n",
      "INFO:root:Epoch=75 iter=11704 lr=0.001000 train_loss=0.000004 valid_err=0.111722\n",
      "INFO:root:Epoch=76 iter=11858 lr=0.001000 train_loss=0.000003 valid_err=0.111722\n",
      "INFO:root:Epoch=77 iter=12012 lr=0.001000 train_loss=0.000003 valid_err=0.111722\n",
      "INFO:root:Epoch=78 iter=12166 lr=0.001000 train_loss=0.000003 valid_err=0.111722\n",
      "INFO:root:Epoch=79 iter=12320 lr=0.001000 train_loss=0.000003 valid_err=0.111722\n",
      "INFO:root:Epoch=80 iter=12474 lr=0.001000 train_loss=0.000002 valid_err=0.111722\n",
      "INFO:root:Epoch=81 iter=12628 lr=0.001000 train_loss=0.000002 valid_err=0.111722\n",
      "INFO:root:Epoch=82 iter=12782 lr=0.001000 train_loss=0.000002 valid_err=0.111722\n",
      "INFO:root:Epoch=83 iter=12936 lr=0.001000 train_loss=0.000002 valid_err=0.109890\n",
      "INFO:root:Epoch=84 iter=13090 lr=0.001000 train_loss=0.000002 valid_err=0.109890\n",
      "INFO:root:Epoch=85 iter=13244 lr=0.001000 train_loss=0.000002 valid_err=0.109890\n",
      "INFO:root:Epoch=86 iter=13398 lr=0.001000 train_loss=0.000002 valid_err=0.109890\n",
      "INFO:root:Epoch=87 iter=13552 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=88 iter=13706 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=89 iter=13860 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=90 iter=14014 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=91 iter=14168 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=92 iter=14322 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=93 iter=14476 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=94 iter=14630 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=95 iter=14784 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=96 iter=14938 lr=0.001000 train_loss=0.000001 valid_err=0.109890\n",
      "INFO:root:Epoch=97 iter=15092 lr=0.001000 train_loss=0.000000 valid_err=0.109890\n",
      "INFO:root:Epoch=98 iter=15246 lr=0.001000 train_loss=0.000000 valid_err=0.109890\n",
      "INFO:root:Epoch=99 iter=15400 lr=0.001000 train_loss=0.000000 valid_err=0.109890\n",
      "INFO:root:========================================\n",
      "INFO:root:best_valid: 0.109890\n",
      "INFO:root:test_err: 0.078000\n",
      "INFO:root:========================================\n",
      "0.10989010989010994 0.07799999999999996\n"
     ]
    }
   ],
   "source": [
    "cmdlines = [\"--dataset\", 'trec', \n",
    "            \"--path\", \"../third_party/sentence_classification/data/\", \n",
    "            \"--embedding\", \"../glove.6B.300d.txt\", \n",
    "            \"--cnn\", \n",
    "            \"--max_epoch\", str(100), \n",
    "            \"--model_seed\", str(1), \n",
    "            \"--data_seed\", str(1),\n",
    "            \"--lr\", str(0.001),\n",
    "            \"--save_mdl\", \"./model_ckpt\",\n",
    "            \"--dropout\", 0.0]\n",
    "err_valid, err_test = train_sentiment(cmdlines)\n",
    "print(err_valid, err_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_args(cmdline_args):\n",
    "    argparser = argparse.ArgumentParser(sys.argv[0], conflict_handler='resolve')\n",
    "    argparser.add_argument(\"--cnn\", action='store_true', help=\"whether to use cnn\")\n",
    "    argparser.add_argument(\"--dataset\", type=str, default=\"mr\", help=\"which dataset\")\n",
    "    argparser.add_argument(\"--path\", type=str, required=True, help=\"path to corpus directory\")\n",
    "    argparser.add_argument(\"--no_normalize\", action='store_true', help=\"Do not normalize embeddings\")\n",
    "    argparser.add_argument(\"--embedding\", type=str, help=\"word vectors\")\n",
    "    argparser.add_argument(\"--batch_size\", \"--batch\", type=int, default=32)\n",
    "    argparser.add_argument(\"--max_epoch\", type=int, default=100)\n",
    "    argparser.add_argument(\"--d\", type=int, default=128)\n",
    "    argparser.add_argument(\"--dropout\", type=float, default=0.5)\n",
    "    argparser.add_argument(\"--depth\", type=int, default=2)\n",
    "    argparser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "    argparser.add_argument(\"--model_seed\", type=int, default=1234)\n",
    "    argparser.add_argument(\"--data_seed\", type=int, default=1234)\n",
    "    argparser.add_argument(\"--save_mdl\", type=str, default=None, help=\"Save model to this file.\")\n",
    "    argparser.add_argument(\"--load_mdl\", type=str, default=None, help=\"Load model from this file.\")\n",
    "    argparser.add_argument(\"--out\", type=str, help=\"Path to output directory.\")\n",
    "    print(cmdline_args)\n",
    "    args = argparser.parse_args(cmdline_args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--dataset', 'trec', '--path', '../third_party/sentence_classification/data/', '--embedding', '../glove.6B.300d.txt', '--cnn', '--max_epoch', '25', '--model_seed', '1', '--data_seed', '1', '--lr', '0.001', '--load_mdl', './model_ckpt', '--dropout', 0.0]\n"
     ]
    }
   ],
   "source": [
    "cmdlines = [\"--dataset\", 'trec', \n",
    "            \"--path\", \"../third_party/sentence_classification/data/\", \n",
    "            \"--embedding\", \"../glove.6B.300d.txt\", \n",
    "            \"--cnn\", \n",
    "            \"--max_epoch\", str(25), \n",
    "            \"--model_seed\", str(1), \n",
    "            \"--data_seed\", str(1),\n",
    "            \"--lr\", str(0.001),\n",
    "            \"--load_mdl\", \"./model_ckpt\",\n",
    "            \"--dropout\", 0.0]\n",
    "args = parse_args(cmdlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Beginning to load embeddings\n",
      "INFO:root:Finished loading embeddings\n",
      "INFO:root:400000 pre-trained word embeddings loaded.\n",
      "WARNING:root:n_d (128) != word vector size (300). Use 300 for embeddings.\n",
      "INFO:root:Number of vectors: 403717, Number of loaded vectors: 400000, Number of oov 3717\n",
      "INFO:root:embedding shape: torch.Size([403717, 300])\n",
      "INFO:root:154 batches, avg len: 21.0\n",
      "INFO:root:18 batches, avg len: 20.8\n",
      "INFO:root:16 batches, avg len: 14.4\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y, test_x, test_y = \\\n",
    "    dataloader.read_split_dataset(args.path, args.dataset)\n",
    "data = train_x + valid_x + test_x\n",
    "    \n",
    "emb_layer = modules.EmbeddingLayer(\n",
    "    args.d, data,\n",
    "    embs = dataloader.load_embedding(args.embedding),\n",
    "    normalize=not args.no_normalize\n",
    ")\n",
    "    \n",
    "train_x, train_y = dataloader.create_batches(\n",
    "    train_x, train_y,\n",
    "    args.batch_size,\n",
    "    emb_layer.word2id,\n",
    "    sort = args.dataset == 'sst'\n",
    ")\n",
    "valid_x, valid_y = dataloader.create_batches(\n",
    "    valid_x, valid_y,\n",
    "    args.batch_size,\n",
    "    emb_layer.word2id,\n",
    "    sort = args.dataset == 'sst'\n",
    ")\n",
    "test_x, test_y = dataloader.create_batches(\n",
    "    test_x, test_y,\n",
    "    args.batch_size,\n",
    "    emb_layer.word2id,\n",
    "    sort = args.dataset == 'sst'\n",
    ")\n",
    "model = torch.load(args.load_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: use quantized model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Replace embedding layers with quantized embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Replaced embedding in EmbeddingLayer\n"
     ]
    }
   ],
   "source": [
    "# compress the embedding layer can take upto 1 min\n",
    "model_comp = quantize_embed(deepcopy(model), nbit=2).cuda()\n",
    "model_uncomp = deepcopy(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model error 0.09399999999999997\n",
      "Original model error 0.07799999999999996\n"
     ]
    }
   ],
   "source": [
    "err_comp = eval_model(model_comp, test_x, test_y)\n",
    "err_uncomp = eval_model(model_uncomp, test_x, test_y)\n",
    "print(\"Quantized model error\", err_comp)\n",
    "print(\"Original model error\", err_uncomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: use quantized model for training with fixed pretrained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace embedding layers with quantized embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Replaced embedding in EmbeddingLayer\n"
     ]
    }
   ],
   "source": [
    "model_comp = quantize_embed(deepcopy(model), nbit=2).cuda()\n",
    "model_uncomp = deepcopy(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform training on layers other than the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_grad = lambda x: x.requires_grad\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(need_grad, model_comp.parameters()),\n",
    "    lr = args.lr\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(train_x, train_y):\n",
    "    model_comp.zero_grad()\n",
    "    x, y = torch.autograd.Variable(x), torch.autograd.Variable(y)\n",
    "    output = model_comp(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
